"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[513],{4793:(i,e,n)=>{n.r(e),n.d(e,{assets:()=>o,contentTitle:()=>l,default:()=>d,frontMatter:()=>t,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"daily/20251027-20251102","title":"20251027-20251102","description":"2025-10-27","source":"@site/docs/daily/20251027-20251102.md","sourceDirName":"daily","slug":"/daily/20251027-20251102","permalink":"/Recommend-System-Note/daily/20251027-20251102","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1762876111000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Daily","permalink":"/Recommend-System-Note/category/daily"},"next":{"title":"20251103-20251109","permalink":"/Recommend-System-Note/daily/20251103-20251109"}}');var s=n(4848),a=n(8453);const t={},l="20251027-20251102",o={},c=[{value:"2025-10-27",id:"2025-10-27",level:2},{value:"2025-10-28",id:"2025-10-28",level:2},{value:"2025-10-29",id:"2025-10-29",level:2},{value:"2025-10-30",id:"2025-10-30",level:2},{value:"2025-10-31",id:"2025-10-31",level:2},{value:"2025-11-01",id:"2025-11-01",level:2},{value:"2025-11-02",id:"2025-11-02",level:2}];function h(i){const e={a:"a",annotation:"annotation",h1:"h1",h2:"h2",header:"header",li:"li",math:"math",mi:"mi",mn:"mn",mrow:"mrow",msub:"msub",msup:"msup",mtext:"mtext",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,a.R)(),...i.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"20251027-20251102",children:"20251027-20251102"})}),"\n",(0,s.jsx)(e.h2,{id:"2025-10-27",children:"2025-10-27"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] Learning to Schedule: A Supervised Learning Framework for Network-Aware\nScheduling of Data-Intensive Workloads"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [scheduling], [network-aware scheduling, supervised learning, data-intensive workloads, Kubernetes, job completion time prediction]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Sankalpa Timilsina, Susmit Shannigrahi"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Tennessee Technological University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.21419v1",children:"http://arxiv.org/pdf/2510.21419v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a network-aware job scheduler using supervised learning to predict job completion times based on real-time cluster telemetry. The system employs a prediction-and-ranking mechanism that evaluates nodes and selects optimal placements for data-intensive workloads. Evaluation on a geo-distributed Kubernetes cluster showed 34-54% higher accuracy in node selection compared to the default Kubernetes scheduler."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] From SLA to vendor-neutral metrics: An intelligent knowledge-based\napproach for multi-cloud SLA-based broker"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [cloud computing, multi-cloud, SLA management, vendor-neutral metrics, intelligent knowledge-based system, auto-scaling]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," V\xedctor Ramp\xe9rez, Javier Soriano, David Lizcano, Shadi Aljawarneh, Juan A. Lara"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Universidad Polit\xe9cnica de Madrid (UPM), Madrid Open University (UDIMA)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.21173v1",children:"http://arxiv.org/pdf/2510.21173v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes an intelligent knowledge-based system that automatically translates high-level SLAs into vendor-neutral metrics for multi-cloud environments. The approach enables cross-provider metric measurement and provides consumer feedback through an intelligent tutoring system. Validation with IaaS and PaaS use cases demonstrates the system allows transparent multi-cloud exploitation across various application domains."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] xMem: A CPU-Based Approach for Accurate Estimation of GPU Memory in Deep\nLearning Training Workloads"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [GPU memory estimation, dynamic analysis, resource management, scheduling]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Jiabo Shi, Dimitrios Pezaros, Yehia Elkhatib"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Glasgow"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.21048v1",children:"http://arxiv.org/pdf/2510.21048v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," xMem proposes a CPU-based dynamic analysis framework to accurately estimate peak GPU memory requirements for deep learning training workloads without consuming GPU resources. The method achieves 91% reduction in median relative error and 75% reduction in OOM probability compared to existing solutions. This enables better GPU sharing and scheduling in cluster environments while significantly improving memory conservation potential."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] Lincoln AI Computing Survey (LAICS) and Trends"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models training, Other models inference], [AI accelerators, performance analysis, power consumption, market segmentation, computing architectures]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Albert Reuther, Peter Michaleas, Michael Jones, Vijay Gadepally, Jeremy Kepner"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," MIT Lincoln Laboratory"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.20931v1",children:"http://arxiv.org/pdf/2510.20931v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper updates the Lincoln AI Computing Survey by collecting performance and power consumption data of commercial AI accelerators, plotting them on scatter graphs, and analyzing market trends. It introduces a new categorization of computing architectures and examines how GenAI models have shifted computational demands toward matrix-vector operations and high memory bandwidth. The survey highlights ongoing innovations in AI hardware across various deployment scales from embedded systems to data centers."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] ParaRNN: Unlocking Parallel Training of Nonlinear RNNs for Large\nLanguage Models"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM training], [parallel training, nonlinear RNNs, sequence modeling, Newton's iterations, parallel reductions]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Federico Danieli, Pau Rodriguez, Miguel Sarabia, Xavier Suau, Luca Zappella"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Apple"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.21450v1",children:"http://arxiv.org/pdf/2510.21450v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," ParaRNN enables parallel training of nonlinear RNNs by formulating recurrence relationships as a system of equations and solving them using Newton's iterations with parallel reductions. This approach achieves up to 665x speedup over sequential methods and allows training 7B parameter RNNs with performance comparable to Transformers and Mamba2. The framework is released as open-source to facilitate scalable nonlinear RNN research."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] REVE: A Foundation Model for EEG -- Adapting to Any Setup with\nLarge-Scale Pretraining on 25,000 Subjects"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models training], [EEG foundation model, 4D positional encoding, masked autoencoding, brain-computer interfaces, clinical neuroscience]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yassine El Ouahidi, Jonathan Lys, Philipp Th\xf6lke, Nicolas Farrugia, Bastien Pasdeloup, Vincent Gripon, Karim Jerbi, Giulia Lioi"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," IMT Atlantique, Universit\xe9 de Montr\xe9al, Mila, UNIQUE"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.21585v1",children:"http://arxiv.org/pdf/2510.21585v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," REVE introduces a novel 4D positional encoding scheme and uses masked autoencoding pretraining on 60,000 hours of EEG data from 25,000 subjects. The model achieves state-of-the-art performance across 10 EEG tasks including motor imagery and seizure detection. It demonstrates strong generalization with minimal fine-tuning and enables standardized EEG research through released code and weights."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-10-28",children:"2025-10-28"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"cs.DC total: 13"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251028] Simopt-Power: Leveraging Simulation Metadata for Low-Power Design Synthesis"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [FPGA power optimization], [simulation metadata, Shannon Decomposition, activity profiling, truth table duplication, netlist transformation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Eashan Wadhwa, Shanker Shreejith"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Trinity College Dublin"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.21745",children:"https://arxiv.org/pdf/2510.21745"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents Simopt-Power, a framework that uses simulation metadata to identify high-toggle paths in FPGA designs and applies Shannon Decomposition to insert duplicate logic and relocate critical nets. The method achieves approximately 9% reduction in switching-induced power while adding only modest resource overhead. Results demonstrate that coupling simulation insights with targeted optimizations provides an effective approach for dynamic power reduction in FPGA design flows."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251028] Separation of Unconscious Robots with Obstructed Visibility"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed robotics], [opaque robots, semicircle separation, look-compute-move cycle, semi-synchronous scheduler]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Prajyot Pyati, Navjot Kaur, Saswata Jana, Adri Bhattacharya, Partha Sarathi Mandal"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Indian Institute of Technology"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.22434",children:"https://arxiv.org/pdf/2510.22434"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents a collision-free algorithm for unconscious robots with obstructed visibility that separates robots into concentric semicircles. The algorithm operates under a semi-synchronous scheduler and achieves separation in O(n) epochs. The robots coordinate without knowing the total number of robots but agree on one coordinate axis."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251028] When Agents are Powerful: Black Hole Search in Time-Varying Graphs"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed algorithms], [global communication, 1-hop visibility, deterministic algorithms, mobile agents]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Tanvir Kaur, Ashish Saxena"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Indian Institute of Technology Ropar"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.22309",children:"https://arxiv.org/pdf/2510.22309"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper enhances Black Hole Search in dynamic graphs by equipping agents with global communication and 1-hop visibility capabilities. These improvements allow for more efficient solutions compared to previous face-to-face communication approaches. The enhanced agent capabilities reduce the number of agents required to successfully identify the black hole while ensuring at least one agent survives."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251028] A Feature Engineering Approach for Business Impact-Oriented Failure Detection in Distributed Instant Payment Systems"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [fault-tolerance], [feature engineering, anomaly detection, ISO 20022 message processing, processing time analysis, explainable AI]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Lorenzo Porcelli"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Bank of Italy, University of Salerno"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.21710",children:"https://arxiv.org/pdf/2510.21710"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces a feature engineering approach that computes processing times between consecutive ISO 20022 message exchanges to create system state representations for anomaly detection. The method enables early failure detection and incident classification in distributed instant payment systems. Experimental evaluation on TARGET Instant Payment Settlement demonstrates effectiveness in detecting diverse anomaly patterns while providing interpretable explanations for business impact assessment."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251028] Heaven & Hell II: Scale Laws and Robustness in One-Step Heaven-Hell Consensus"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [ai], [network consensus], [Heaven-Hell dynamics, conservation-law perspective, tie-breaking policies, pointwise bounds, asynchronous updates, Coq proofs]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Nnamdi Daniel Aghanya, Romain Leemans"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Cranfield University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.21950",children:"https://arxiv.org/pdf/2510.21950"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper develops scale laws and operational refinements for Heaven-Hell consensus dynamics, using a conservation-law perspective to derive tighter bounds and robustness guarantees. The research establishes conditions for robust convergence under various scenarios including tie-breaking policies, asynchronous updates, and multiple hubs. All proofs are mechanized in Coq and experiments validate the tightness of the bounds across diverse network types."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251028] Rethinking Inference Placement for Deep Learning across Edge and Cloud Platforms: A Multi-Objective Optimization Perspective and Future Directions"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [model offloading, model compression, model distillation, transmission compression, internal classifiers]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Zongshun Zhang, Ibrahim Matta"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Boston University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.22909",children:"https://arxiv.org/pdf/2510.22909"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper surveys methods for optimizing deep learning inference placement across edge and cloud platforms using techniques like model partitioning, compression, and architecture adaptations. It analyzes how these approaches balance multiple objectives including latency, privacy, and monetary cost. The research concludes that effective inference placement requires multi-objective optimization across the computational continuum from devices to cloud."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251028] CodeAD: Synthesize Code of Rules for Log-based Anomaly Detection with LLMs"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [hierarchical clustering, anchor-grounded sampling, agentic workflow, rule synthesis, contrastive log windows]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Junjie Huang, Minghua He, Jinyang Liu, Yintong Huo, Domenico Bianculli, Michael R. Lyu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," The Chinese University of Hong Kong, Peking University, Singapore Management University, University of Luxembourg"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.22986",children:"https://arxiv.org/pdf/2510.22986"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," CodeAD automatically synthesizes lightweight Python rule functions for log-based anomaly detection using LLMs through hierarchical clustering and iterative agentic workflows. The framework achieves 3.6% higher F1 score than state-of-the-art methods while processing data 4x faster at minimal cost, providing an interpretable and efficient solution for real-time log analysis."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251028] Power to the Clients: Federated Learning in a Dictatorship Setting"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [fault-tolerance], [federated learning, byzantine clients, dictator clients, model convergence, attack strategies]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Mohammadsajad Alipour, Mohammad Mohammadi Amiri"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Rensselaer Polytechnic Institute"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.22149",children:"https://arxiv.org/pdf/2510.22149"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces dictator clients, a class of malicious participants in federated learning that can erase other clients' contributions while preserving their own. The authors propose attack strategies and analyze their effects on model convergence in various scenarios including collaboration and betrayal between multiple dictator clients. Their theoretical analysis and empirical evaluations demonstrate how these clients can significantly compromise the global model's integrity."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251028] Sentinel: Dynamic Knowledge Distillation for Personalized Federated Intrusion Detection in Heterogeneous IoT Networks"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [personalized federated learning, knowledge distillation, dual-model architecture, class-balanced loss, gradient aggregation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Gurpreet Singh, Keshav Sood, P. Rajalakshmi, Yong Xiang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Deakin University, Indian Institute of Technology Hyderabad"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.23019",children:"https://arxiv.org/pdf/2510.23019"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes Sentinel, a personalized federated intrusion detection system that uses a dual-model architecture with teacher-student knowledge distillation to handle data heterogeneity in IoT networks. It incorporates bidirectional distillation, feature alignment, and balanced loss functions to improve performance. Experiments show Sentinel outperforms existing methods while maintaining communication efficiency."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251028] AirFed: Federated Graph-Enhanced Multi-Agent Reinforcement Learning for Multi-UAV Cooperative Mobile Edge Computing"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [federated learning, graph attention networks, multi-agent reinforcement learning, trajectory planning, task offloading, resource allocation, gradient quantization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Zhiyu Wang, Suman Raj, Rajkumar Buyya"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," The University of Melbourne, Indian Institute of Science"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.23053",children:"https://arxiv.org/pdf/2510.23053"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes AirFed, a federated graph-enhanced multi-agent reinforcement learning framework that uses dual-layer Graph Attention Networks and a dual-Actor single-Critic architecture for UAV coordination in mobile edge computing. The system achieves significant improvements including 42.9% cost reduction, over 99% deadline satisfaction, and 54.5% lower communication overhead compared to state-of-the-art methods, demonstrating strong scalability for large-scale UAV deployments."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251028] AutoStreamPipe: LLM Assisted Automatic Generation of Data Stream Processing Pipelines"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [Hypergraph of Thoughts, multi-agent reasoning, resilient execution strategies, advanced query analysis]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Abolfazl Younesi, Zahra Najafabadi Samani, Thomas Fahringer"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Innsbruck"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.23408",children:"https://arxiv.org/pdf/2510.23408"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," AutoStreamPipe uses Large Language Models with Hypergraph of Thoughts to automatically generate data stream processing pipelines from high-level user intents. The framework bridges the semantic gap between user requirements and platform-specific implementations through structured multi-agent reasoning. Experimental results show it significantly reduces development time by 6.3\xd7 and error rates by 5.19\xd7 compared to traditional LLM code-generation methods."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251028] Bayes-Split-Edge: Bayesian Optimization for Constrained Collaborative Inference in Wireless Edge Systems"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [Bayesian optimization, split learning, collaborative inference, wireless edge computing, constrained optimization, neural network splitting]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Fatemeh Zahra Safaeipour, Jacob Chakareski, Morteza Hashemi"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Kansas, New Jersey Institute of Technology"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.23503",children:"https://arxiv.org/pdf/2510.23503"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes Bayes-Split-Edge, a Bayesian optimization framework that jointly optimizes transmission power and neural network split points for collaborative inference in wireless edge systems. The method uses a novel hybrid acquisition function to balance inference utility with energy and delay constraints. Results show it achieves 2.4\xd7 evaluation cost reduction compared to standard Bayesian optimization while matching exhaustive search performance under tight constraints."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsxs)(e.strong,{children:["[arXiv251028] The First Star-by-star ",(0,s.jsxs)(e.span,{className:"katex",children:[(0,s.jsx)(e.span,{className:"katex-mathml",children:(0,s.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,s.jsxs)(e.semantics,{children:[(0,s.jsx)(e.mrow,{children:(0,s.jsx)(e.mi,{children:"N"})}),(0,s.jsx)(e.annotation,{encoding:"application/x-tex",children:"N"})]})})}),(0,s.jsx)(e.span,{className:"katex-html","aria-hidden":"true",children:(0,s.jsxs)(e.span,{className:"base",children:[(0,s.jsx)(e.span,{className:"strut",style:{height:"0.6833em"}}),(0,s.jsx)(e.span,{className:"mord mathnormal",style:{marginRight:"0.10903em"},children:"N"})]})})]}),"-body/Hydrodynamics Simulation of Our Galaxy Coupling with a Surrogate Model"]})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [N-body simulation, smoothed-particle hydrodynamics, surrogate model, deep learning, Fugaku supercomputer]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Keiya Hirashima, Michiko S. Fujii, Takayuki R. Saitoh, Naoto Harada, Kentaro Nomura, Kohji Yoshikawa, Yutaka Hirai, Tetsuro Asano, Kana Moriwaki, Masaki Iwasawa, Takashi Okamoto, Junichiro Makino"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," RIKEN, University of Tokyo, Kobe University, Preferred Networks, University of Tsukuba, Tohoku University of Community Service and Science, Universitat de Barcelona, National Institute of Technology, Hokkaido University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.23330",children:"https://arxiv.org/pdf/2510.23330"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents a novel integration scheme combining N-body/hydrodynamics simulations with machine learning surrogate models to bypass computational bottlenecks caused by supernova explosions. The method achieved 300 billion particles using 148,900 nodes, breaking the billion-particle barrier and enabling the first star-by-star galaxy simulation. This represents a major advancement in computational astrophysics by resolving individual stars in Milky Way simulations."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 52'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Transitive RL: Value Learning via Divide and Conquer ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22512",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Predictive Coding Enhances Meta-RL To Achieve Interpretable Bayes-Optimal Belief Representation Under Partial Observability ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22039",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Hazard-Responsive Digital Twin for Climate-Driven Urban Resilience and Equity ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22941",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Policies over Poses: Reinforcement Learning based Distributed Pose-Graph Optimization for Multi-Robot SLAM ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22740",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] CityRiSE: Reasoning Urban Socio-Economic Status in Vision-Language Models via Reinforcement Learning ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22282",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Beyond Reasoning Gains: Mitigating General Capabilities Forgetting in Large Reasoning Models ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.21978",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Taxonomy and Trends in Reinforcement Learning for Robotics and Control Systems: A Structured Review ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.21758",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Advantage Shaping as Surrogate Reward Maximization: Unifying Pass@K Policy Gradients ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.23049",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] PACR: Progressively Ascending Confidence Reward for LLM Reasoning ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22255",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Is Temporal Difference Learning the Gold Standard for Stitching in RL? ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.21995",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Incentivizing Agentic Reasoning in LLM Judges via Tool-Integrated Reinforcement Learning ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.23038",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] FlowCritic: Bridging Value Estimation with Flow Matching in Reinforcement Learning ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22686",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Offline Preference Optimization via Maximum Marginal Likelihood Estimation ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22881",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Agent-GSPO: Communication-Efficient Multi-Agent Systems via Group Sequence Policy Optimization ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22477",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Dopamine-driven synaptic credit assignment in neural networks ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22178",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Activating Visual Context and Commonsense Reasoning through Masked Prediction in VLMs ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.21807",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Towards Stable and Effective Reinforcement Learning for Mixture-of-Experts ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.23027",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] UCB-type Algorithm for Budget-Constrained Expert Learning ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22654",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Online Optimization for Offline Safe Reinforcement Learning ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22027",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] GAPO: Group Adaptive Policy Optimization for Real-World Code Edit ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.21830",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Guardian: Decoupling Exploration from Safety in Reinforcement Learning ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22859",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] STAR-RIS-assisted Collaborative Beamforming for Low-altitude Wireless Networks ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22108",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Curriculum-Based Iterative Self-Play for Scalable Multi-Drone Racing ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22570",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Solving Continuous Mean Field Games: Deep Reinforcement Learning for Non-Stationary Dynamics ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22158",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] BLIP-FusePPO: A Vision-Language Deep Reinforcement Learning Framework for Lane Keeping in Autonomous Vehicles ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22370",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] VEHME: A Vision-Language Model For Evaluating Handwritten Mathematics Expressions ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22798",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Softmax is ",(0,s.jsxs)(e.span,{className:"katex",children:[(0,s.jsx)(e.span,{className:"katex-mathml",children:(0,s.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,s.jsxs)(e.semantics,{children:[(0,s.jsxs)(e.mrow,{children:[(0,s.jsx)(e.mn,{children:"1"}),(0,s.jsx)(e.mi,{mathvariant:"normal",children:"/"}),(0,s.jsx)(e.mn,{children:"2"})]}),(0,s.jsx)(e.annotation,{encoding:"application/x-tex",children:"1/2"})]})})}),(0,s.jsx)(e.span,{className:"katex-html","aria-hidden":"true",children:(0,s.jsxs)(e.span,{className:"base",children:[(0,s.jsx)(e.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,s.jsx)(e.span,{className:"mord",children:"1/2"})]})})]}),"-Lipschitz: A tight bound across all ",(0,s.jsxs)(e.span,{className:"katex",children:[(0,s.jsx)(e.span,{className:"katex-mathml",children:(0,s.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,s.jsxs)(e.semantics,{children:[(0,s.jsx)(e.mrow,{children:(0,s.jsxs)(e.msub,{children:[(0,s.jsx)(e.mi,{mathvariant:"normal",children:"\u2113"}),(0,s.jsx)(e.mi,{children:"p"})]})}),(0,s.jsx)(e.annotation,{encoding:"application/x-tex",children:"\\ell_p"})]})})}),(0,s.jsx)(e.span,{className:"katex-html","aria-hidden":"true",children:(0,s.jsxs)(e.span,{className:"base",children:[(0,s.jsx)(e.span,{className:"strut",style:{height:"0.9805em",verticalAlign:"-0.2861em"}}),(0,s.jsxs)(e.span,{className:"mord",children:[(0,s.jsx)(e.span,{className:"mord",children:"\u2113"}),(0,s.jsx)(e.span,{className:"msupsub",children:(0,s.jsxs)(e.span,{className:"vlist-t vlist-t2",children:[(0,s.jsxs)(e.span,{className:"vlist-r",children:[(0,s.jsx)(e.span,{className:"vlist",style:{height:"0.1514em"},children:(0,s.jsxs)(e.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,s.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,s.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,s.jsx)(e.span,{className:"mord mathnormal mtight",children:"p"})})]})}),(0,s.jsx)(e.span,{className:"vlist-s",children:"\u200b"})]}),(0,s.jsx)(e.span,{className:"vlist-r",children:(0,s.jsx)(e.span,{className:"vlist",style:{height:"0.2861em"},children:(0,s.jsx)(e.span,{})})})]})})]})]})})]})," norms ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.23012",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] SPIRAL: Self-Play Incremental Racing Algorithm for Learning in Multi-Drone Competitions ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22568",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Agentic Reinforcement Learning for Real-World Code Repair ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22075",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Computational Hardness of Reinforcement Learning with Partial ",(0,s.jsxs)(e.span,{className:"katex",children:[(0,s.jsx)(e.span,{className:"katex-mathml",children:(0,s.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,s.jsxs)(e.semantics,{children:[(0,s.jsx)(e.mrow,{children:(0,s.jsxs)(e.msup,{children:[(0,s.jsx)(e.mi,{children:"q"}),(0,s.jsx)(e.mi,{children:"\u03c0"})]})}),(0,s.jsx)(e.annotation,{encoding:"application/x-tex",children:"q^\u03c0"})]})})}),(0,s.jsx)(e.span,{className:"katex-html","aria-hidden":"true",children:(0,s.jsxs)(e.span,{className:"base",children:[(0,s.jsx)(e.span,{className:"strut",style:{height:"0.8588em",verticalAlign:"-0.1944em"}}),(0,s.jsxs)(e.span,{className:"mord",children:[(0,s.jsx)(e.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"q"}),(0,s.jsx)(e.span,{className:"msupsub",children:(0,s.jsx)(e.span,{className:"vlist-t",children:(0,s.jsx)(e.span,{className:"vlist-r",children:(0,s.jsx)(e.span,{className:"vlist",style:{height:"0.6644em"},children:(0,s.jsxs)(e.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,s.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,s.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,s.jsx)(e.span,{className:"mord mathnormal mtight",style:{marginRight:"0.03588em"},children:"\u03c0"})})]})})})})})]})]})})]}),"-Realizability ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.21888",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Toward Agents That Reason About Their Computation ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22833",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable Reasoning ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22543",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Do You Trust the Process?: Modeling Institutional Trust for Community Adoption of Reinforcement Learning Policies ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22017",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] HRM-Agent: Training a recurrent reasoning model in dynamic environments using reinforcement learning ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22832",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via Regulated Clipping ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22319",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] SynCast: Synergizing Contradictions in Precipitation Nowcasting via Diffusion Sequential Preference Optimization ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.21847",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Multi-Agent Conditional Diffusion Model with Mean Field Communication as Wireless Resource Allocation Planner ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22969",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Think before Recommendation: Autonomous Reasoning-enhanced Recommender ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.23077",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Adapting Interleaved Encoders with PPO for Language-Guided Reinforcement Learning in BabyAI ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.23148",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Guiding Skill Discovery with Foundation Models ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.23167",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] TARC: Time-Adaptive Robotic Control ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.23176",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Human-Like Goalkeeping in a Realistic Football Simulation: a Sample-Efficient Reinforcement Learning Approach ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.23216",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] CNOT Minimal Circuit Synthesis: A Reinforcement Learning Approach ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.23304",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] The Best of N Worlds: Aligning Reinforcement Learning with Best-of-N Sampling via max@k Optimisation ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.23393",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Causal Deep Q Network ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.23424",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] An Information-Theoretic Analysis of Out-of-Distribution Generalization in Meta-Learning with Applications to Meta-RL ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.23448",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Learning to Reason Efficiently with Discounted Reinforcement Learning ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.23486",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Sequential Multi-Agent Dynamic Algorithm Configuration ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.23535",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Multi-Agent Evolve: LLM Self-Improve through Co-evolution ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.23595",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Right Place, Right Time: Market Simulation-based RL for Execution Optimisation ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22206",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Reinforcement learning-guided optimization of critical current in high-temperature superconductors ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22424",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] PASS-Enhanced MEC: Joint Optimization of Task Offloading and Uplink PASS Beamforming ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22948",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "accelerat" total: 23'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] A phase-aware AI car-following model for electric vehicles with adaptive cruise control: Development and validation using real-world data ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.21735",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Impact and Implications of Generative AI for Enterprise Architects in Agile Environments: A Systematic Literature Review ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22003",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Step2Motion: Locomotion Reconstruction from Pressure Sensing Insoles ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22712",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Accelerating Materials Design via LLM-Guided Evolutionary Search ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22503",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] PACR: Progressively Ascending Confidence Reward for LLM Reasoning ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22255",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] AI-Enhanced Operator Assistance for UNICOS Applications ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.21717",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Dopamine-driven synaptic credit assignment in neural networks ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22178",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] OpenEM: Large-scale multi-structural 3D datasets for electromagnetic methods ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.21859",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Encoder-Decoder Diffusion Language Models for Efficient Training and Inference ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22852",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] QuArch: A Benchmark for Evaluating LLM Reasoning in Computer Architecture ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22087",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Residual-guided AI-CFD hybrid method enables stable and scalable simulations: from 2D benchmarks to 3D applications ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.21804",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Graph-Coarsening Approach for the Capacitated Vehicle Routing Problem with Time Windows ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22329",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Approximate Gradient Coding for Distributed Learning with Heterogeneous Stragglers ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22539",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] TernaryCLIP: Efficiently Compressing Vision-Language Models with Ternary Weights and Distilled Knowledge ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.21879",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Lost in Tokenization: Context as the Key to Unlocking Biomolecular Understanding in Scientific LLMs ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.23127",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Guiding Skill Discovery with Foundation Models ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.23167",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Accelerating Eigenvalue Dataset Generation via Chebyshev Subspace Filter ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.23215",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Accelerating IC Thermal Simulation Data Generation via Block Krylov and Operator Action ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.23221",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Progressive Growing of Patch Size: Curriculum Learning for Accelerated and Improved Medical Image Segmentation ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.23241",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] PAHQ: Accelerating Automated Circuit Discovery through Mixed-Precision Inference Optimization ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.23264",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Towards a Generalizable AI for Materials Discovery: Validation through Immersion Coolant Screening ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.23371",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] HPC-Driven Modeling with ML-Based Surrogates for Magnon-Photon Dynamics in Hybrid Quantum Systems ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22221",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251028] Reinforcement learning-guided optimization of critical current in high-temperature superconductors ",(0,s.jsx)(e.a,{href:"http://arxiv.org/abs/2510.22424",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-10-29",children:"2025-10-29"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"cs.DC total: 14"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251029] Odyssey: An End-to-End System for Pareto-Optimal Serverless Query Processing"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [serverless data analytics], [query planner, cost model, execution engine, state space pruning, Pareto-optimal plans]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Shyam Jesalpura, Shengda Zhu, Amir Shaikhha, Antonio Barbalace, Boris Grot"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," The University of Edinburgh"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24307",children:"https://arxiv.org/pdf/2510.24307"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," Odyssey introduces an end-to-end serverless data analytics system that automatically generates and evaluates query plans using state space pruning and a novel search algorithm to find Pareto-optimal plans balancing cost and performance. The system consistently outperforms AWS Athena on cost and/or latency while accurately predicting both monetary cost and latency for serverless query processing."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251029] CoMPSeT: A Framework for Comparing Multiparty Session Types"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [formal methods], [multiparty session types, choreographic languages, operational semantics]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Telmo Ribeiro, Jos\xe9 Proen\xe7a, M\xe1rio Florido"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Porto"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24205",children:"https://arxiv.org/pdf/2510.24205"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces CoMPSeT, a framework for comparing different variations of Multiparty Session Types through representative examples and semantic animation. The tool allows researchers and educators to combine different MPST features and analyze their behavior. CoMPSeT is implemented as an open-source JavaScript tool that runs directly in web browsers."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251029] ARIMA_PLUS: Large-scale, Accurate, Automatic and Interpretable In-Database Time Series Forecasting and Anomaly Detection in Google BigQuery"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [ARIMA_PLUS, time series forecasting, anomaly detection, BigQuery, SQL interface, modular decomposition]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Xi Cheng, Weijie Shen, Haoming Chen, Chaoyi Shen, Jean Ortega, Jiashang Liu, Steve Thomas, Honglin Zheng, Haoyun Wu, Yuxiang Li, Casey Lichtendahl, Jenny Ortiz, Gang Liu, Haiyang Qi, Omid Fatemieh, Chris Fry, Jing Jing Long"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Google"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24452",children:"https://arxiv.org/pdf/2510.24452"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," ARIMA_PLUS is a novel framework that combines interpretable time series modeling with scalable cloud infrastructure for forecasting and anomaly detection. It demonstrates superior accuracy over both statistical and neural network methods across 42 benchmark datasets. The system integrates directly into Google BigQuery, enabling automatic scaling to handle millions of time series efficiently through a simple SQL interface."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251029] Differential Privacy: Gradient Leakage Attacks in Federated Learning Environments"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [Differential Privacy, DP-SGD, PDP-SGD, Gradient Leakage Attacks, Federated Learning]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Miguel Fernandez-de-Retana, Unai Zulaika, Rub\xe9n S\xe1nchez-Corcuera, Aitor Almeida"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Basque Center for Applied Mathematics, University of Deusto"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.23931",children:"https://arxiv.org/pdf/2510.23931"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper evaluates Differential Privacy mechanisms (DP-SGD and PDP-SGD) as defenses against gradient leakage attacks in federated learning. The results show DP-SGD effectively mitigates reconstruction attacks with moderate utility trade-off, while PDP-SGD maintains strong model performance but fails as a practical defense. The findings emphasize the need for empirical evaluation of privacy mechanisms beyond theoretical guarantees in distributed learning scenarios."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251029] Fault-Tolerant Multiparty Session Types with Global Escape Loops"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed systems], [multiparty session types, fault-tolerance, global escape loops, non-blocking messages, rotating coordinator algorithm]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Lukas Bartl, Julian Linne, Kirstin Peters"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Universit\xe4t Augsburg"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24203",children:"https://arxiv.org/pdf/2510.24203"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper extends fault-tolerant multiparty session types by introducing a novel global escape loop construct that allows processes to terminate distributed algorithms without global coordination. The approach uses non-blocking exit-messages to enable efficient fault-tolerant termination when solutions are found. The method is demonstrated through analysis of a variant of the rotating coordinator algorithm by Chandra and Toueg."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251029] The SAP Cloud Infrastructure Dataset: A Reality Check of Scheduling and Placement of VMs in Cloud Computing"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [cloud computing], [virtual machine scheduling, resource allocation, workload management, performance metrics]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Arno Uhlig, Iris Braun, Matthias W\xe4hlisch"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," TU Dresden, SAP SE"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.23911",children:"https://arxiv.org/pdf/2510.23911"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper analyzes VM scheduling and placement in SAP's cloud infrastructure using fine-grained telemetry data from 1,800 hypervisors and 48,000 VMs. The study identifies significant resource inefficiencies including CPU contention exceeding 40% and over 80% of VMs using less than 70% of allocated resources. Based on these findings, the authors derive requirements for improved scheduling algorithms and make their dataset publicly available for future research."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251029] Local Performance vs. Out-of-Distribution Generalization: An Empirical Analysis of Personalized Federated Learning in Heterogeneous Data Environments"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [personalized federated learning, FedAvg, client drift, out-of-distribution generalization, FLIU, adaptive personalization factor]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Mortesa Hussaini, Jan Thei\xdf, Anthony Stein"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Hohenheim"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24503",children:"https://arxiv.org/pdf/2510.24503"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes FLIU, a modified FedAvg approach with adaptive personalization factors, and evaluates both local performance and out-of-distribution generalization in personalized federated learning. The study finds that while PFL methods excel at local performance, they inadequately address generalization capabilities compared to traditional FL approaches. The research demonstrates the importance of balancing both local optimization and generalization in heterogeneous data environments."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251029] SPEAR++: Scaling Gradient Inversion via Sparsely-Used Dictionary Learning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [gradient inversion attacks, sparsely-used dictionary learning, federated learning, ReLU activations, linear layers, FedAvg, differential privacy]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Alexander Bakarsky, Dimitar I. Dimitrov, Maximilian Baader, Martin Vechev"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," ETH Zurich, INSAIT Sofia University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24200",children:"https://arxiv.org/pdf/2510.24200"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces SPEAR++, a gradient inversion attack that uses sparsely-used dictionary learning to scale previous theoretical attacks on federated learning systems. The method makes gradient inversion tractable for linear layers with ReLU activations while maintaining robustness to DP noise and FedAvg aggregation. The authors demonstrate their attack can handle batch sizes 10x larger than previous approaches while preserving all desirable properties of the original SPEAR attack."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251029] Towards Exascale Computing for Astrophysical Simulation Leveraging the Leonardo EuroHPC System"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [astrophysical simulation], [GPU scaling, performance analysis, profiling tools, HPC optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Nitin Shukla, Alessandro Romeo, Caterina Caravita, Michael Redenti, Radim Vavrik, Lubomir Riha, Andrea Mignone, Marco Rossazza, Stefano Truzzi, Luca Tornatore, Antonio Ragagnin, Tiago Castro, Geray S. Karademir, Klaus Dolag, Pranab J. Deka, Fabio Bacchini, Rostislav-Paul Wilhelm, Daniele Gregori, Elisabetta Boella"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," CINECA, IT4Innovations, University of Turin, INAF, Ludwig-Maximilians-Universit\xe4t M\xfcnchen, KU Leuven, E4 Computer Engineering"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24175",children:"https://arxiv.org/pdf/2510.24175"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents optimization strategies for three astrophysical simulation codes (gPLUTO, OpenGadget3, iPIC3D) on the Leonardo EuroHPC system using profiling tools to analyze performance. The research demonstrates that all three codes achieve efficient scaling, reaching 80% scalability up to 1,024 GPUs, enabling large-scale astrophysical simulations for the exascale computing era."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251029] A GPU-based Compressible Combustion Solver for Applications Exhibiting Disparate Space and Time Scales"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [computational fluid dynamics], [GPU optimization, AMReX framework, bulk-sparse integration, adaptive mesh refinement, column-major storage, roofline analysis]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Anthony Carreon, Jagmohan Singh, Shivank Sharma, Shuzhi Zhang, Venkat Raman"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Michigan"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.23993",children:"https://arxiv.org/pdf/2510.23993"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper presents a GPU-optimized compressible combustion solver built on the AMReX framework that addresses memory access patterns, workload variability, and multi-GPU load distribution. The solver achieves 2-5\xd7 performance improvements over initial GPU implementations with near-ideal weak scaling across 1-96 NVIDIA H100 GPUs. Roofline analysis confirms efficient utilization of GPU memory bandwidth and computational resources for both convection and chemistry routines."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251029] PanDelos-plus: A parallel algorithm for computing sequence homology in pangenomic analysis"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [bioinformatics], [parallel computing, k-mer profiles, data decomposition, thread pool, lightweight data structures]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Simone Colli, Emiliano Maresi, Vincenzo Bonnici"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Parma"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.23679",children:"https://arxiv.org/pdf/2510.23679"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," PanDelos-plus is a parallel algorithm that improves upon PanDelos by parallelizing computationally intensive phases using data decomposition and thread pool strategies while employing lightweight data structures. Benchmarks show it achieves up to 14x faster execution and 96% memory reduction while maintaining accuracy. These improvements enable large-scale bacterial pangenome analysis to be performed on standard multicore workstations for routine research use."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251029] Distributed Stochastic Momentum Tracking with Local Updates: Achieving Optimal Communication and Iteration Complexities"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [Local Momentum Tracking, momentum tracking, Loopless Chebyshev Acceleration, local updates, distributed optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Kun Huang, Shi Pu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," The Chinese University of Hong Kong, Shenzhen"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24155",children:"https://arxiv.org/pdf/2510.24155"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes Local Momentum Tracking (LMT), a distributed stochastic gradient method that combines local updates with momentum tracking and Loopless Chebyshev Acceleration to reduce communication overhead. LMT achieves linear speedup with respect to both local updates and number of agents, and attains optimal communication and iteration complexities under different local update regimes. This represents the first method to simultaneously achieve these properties for distributed optimization over networks."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251029] Exascale In-situ visualization for Astronomy & Cosmology"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [high-performance computing], [in-situ visualization, distributed database, streaming data, N-body simulation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Nicola Tuccari, Eva Sciacca, Yolanda Becerra, Enric Sosa Cintero, Emiliano Tramontana"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," INAF Astrophysical Observatory of Catania, Universit\xe0 di Catania, Barcelona Supercomputing Center"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24545",children:"https://arxiv.org/pdf/2510.24545"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents an in-situ visualization approach using Hecuba, a distributed database framework that streams astronomy and cosmology simulation data directly into visualization pipelines. By integrating Hecuba with the ChaNGa cosmological simulator, the authors enable real-time visualization of N-body simulations using tools like ParaView and VisIVO. The main conclusion is that this approach overcomes traditional I/O bottlenecks in exascale computing by bypassing disk storage and enabling concurrent simulation and visualization."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251029] In-Situ High Performance Visualization for Astronomy & Cosmology"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [high-performance computing], [in-situ visualization, distributed database, Hecuba framework, ParaView, VisIVO]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Nicola Tuccari, Eva Sciacca, Yolanda Becerra, Enric Sosa Cintero, Robert Wissing, Sijing Shen, Emiliano Tramontana"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," INAF Astrophysical Observatory of Catania, Universit\xe0 di Catania, Barcelona Supercomputing Center, University of Oslo"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24547",children:"https://arxiv.org/pdf/2510.24547"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents an in-situ visualization approach using the Hecuba distributed database framework to process cosmological simulation data concurrently with computation. The method integrates with the Changa cosmological simulator and visualization tools ParaView and VisIVO to bypass storage bottlenecks. The main conclusion is that this approach enables effective visualization of petascale datasets that would otherwise require data filtering or resolution reduction."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 18'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251029] PaTaRM: Bridging Pairwise and Pointwise Signals via Preference-Aware Task-Adaptive Reward Modeling ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24235",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251029] Fill in the Blanks: Accelerating Q-Learning with a Handful of Demonstrations in Sparse Reward Settings ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24432",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251029] ViPER: Empowering the Self-Evolution of Visual Perception Abilities in Vision-Language Model ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24285",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251029] Teaching LLMs to Abstain via Fine-Grained Semantic Confidence Reward ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24020",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251029] Causal-Aware Generative Adversarial Networks with Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24046",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251029] Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking Neural Networks ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24461",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251029] Sample-efficient and Scalable Exploration in Continuous-Time RL ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24482",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251029] BMGQ: A Bottom-up Method for Generating Complex Multi-hop Reasoning Questions from Semi-structured Data ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24151",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251029] Hybrid Modeling, Sim-to-Real Reinforcement Learning, and Large Language Model Driven Control for Digital Twins ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.23882",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251029] GIFT: Group-relative Implicit Fine Tuning Integrates GRPO with DPO and UNA ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.23868",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251029] MiniOneRec: An Open-Source Framework for Scaling Generative Recommendation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24431",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251029] Latent Chain-of-Thought for Visual Reasoning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.23925",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251029] Debiasing Reward Models by Representation Learning with Guarantees ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.23751",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251029] Survey and Tutorial of Reinforcement Learning Methods in Process Systems Engineering ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24272",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251029] Dual-Mind World Models: A General Framework for Learning in Dynamic Wireless Networks ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24546",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251029] Advancing site-specific disease and pest management in precision agriculture: From reasoning-driven foundation models to adaptive, feedback-based learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24650",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251029] Learning to Drive Safely with Hybrid Options ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24674",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251029] Greedy Sampling Is Provably Efficient for RLHF ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24700",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 12'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251029] Fill in the Blanks: Accelerating Q-Learning with a Handful of Demonstrations in Sparse Reward Settings ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24432",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251029] RS-ORT: A Reduced-Space Branch-and-Bound Algorithm for Optimal Regression Trees ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.23901",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251029] Causal-Aware Generative Adversarial Networks with Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24046",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251029] Design and Optimization of Cloud Native Homomorphic Encryption Workflows for Privacy-Preserving ML Inference ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24498",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251029] SymMaP: Improving Computational Efficiency in Linear Solvers through Symbolic Preconditioning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24170",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251029] Fixed Point Neural Acceleration and Inverse Surrogate Model for Battery Parameter Identification ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24135",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251029] FALQON: Accelerating LoRA Fine-tuning with Low-Bit Floating-Point Arithmetic ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24061",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251029] Speeding Up MACE: Low-Precision Tricks for Equivarient Force Fields ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.23621",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251029] Scalable GPU-Based Integrity Verification for Large Machine Learning Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.23938",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251029] Modeling Electric Vehicle Car-Following Behavior: Classical vs Machine Learning Approach ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24085",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251029] Taming the Tail: NoI Topology Synthesis for Mixed DL Workloads on Chiplet-Based Accelerators ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24113",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251029] Trajectory Design for UAV-Based Low-Altitude Wireless Networks in Unknown Environments: A Digital Twin-Assisted TD3 Approach ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24255",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-10-30",children:"2025-10-30"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"cs.DC total: 12"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251030] Scheduling Data-Intensive Workloads in Large-Scale Distributed Systems: Trends and Challenges"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed computing], [gang scheduling, workflow scheduling, bag-of-tasks scheduling, data locality, fault tolerance, energy efficiency]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Georgios L. Stavrinides, Helen D. Karatza"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Aristotle University of Thessaloniki"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25362",children:"https://arxiv.org/pdf/2510.25362"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This chapter provides a classification of data-intensive workloads and surveys scheduling approaches for large-scale distributed systems. It presents novel scheduling strategies that address challenges like parallelism, data locality, and quality of service requirements. The paper concludes by identifying open challenges and future research directions in scheduling data-intensive applications."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251030] Multi-Resolution Model Fusion for Accelerating the Convolutional Neural Network Training"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [Multi-Resolution Model Fusion, reduced-resolution training, model convergence, fine-tuning]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Kewei Wang, Claire Songhyun Lee, Sunwoo Lee, Vishu Gupta, Jan Balewski, Alex Sim, Peter Nugent, Ankit Agrawal, Alok Choudhary, Kesheng Wu, Wei-keng Liao"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Northwestern University, Lawrence Berkeley National Laboratory, Inha University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25170",children:"https://arxiv.org/pdf/2510.25170"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a Multi-Resolution Model Fusion method that accelerates CNN training by first training models on reduced-resolution data and then fine-tuning with original resolution data. This approach speeds up convergence while maintaining model accuracy. Experiments show training time improvements of up to 47% and 44% on scientific applications without affecting accuracy."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251030] Machine Learning and CPU (Central Processing Unit) Scheduling Co-Optimization over a Network of Computing Centers"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [distributed machine learning, CPU scheduling, consensus algorithm, quantization, Lyapunov stability, eigen-spectrum analysis]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Mohammadreza Doostmohammadian, Zulfiya R. Gabidullina, Hamid R. Rabiee"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Sharif University of Technology, Kazan Federal University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25176",children:"https://arxiv.org/pdf/2510.25176"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a co-optimization algorithm that simultaneously trains machine learning models on distributed computing nodes while optimally allocating CPU resources across a network. The method ensures all-time feasibility and handles time-varying networks with quantization constraints. Results show the approach reduces the cost optimality gap by over 50% compared to existing CPU scheduling solutions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251030] The Singularity Theory of Concurrent Programs: A Topological Characterization and Detection of Deadlocks and Livelocks"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [concurrency verification], [algebraic topology, homotopy groups, homology groups, topological invariants, branched topological space]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Di Zhang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Xi'an Jiaotong-Liverpool University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25112",children:"https://arxiv.org/pdf/2510.25112"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces a topological framework called Singularity Theory that models concurrent program execution spaces as branched topological spaces. It uses algebraic topology tools like homotopy and homology groups to characterize deadlocks as attractors and livelocks as non-contractible loops. The method enables systematic detection of concurrency errors without exhaustive state-space exploration, overcoming limitations of traditional model checking."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251030] Effect of Full Common Randomness Replication in Symmetric PIR on Graph-Based Replicated Systems"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [information theory], [symmetric private information retrieval, graph-based replication, common randomness, capacity bounds]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Shreya Meel, Sennur Ulukus"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Maryland"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25736",children:"https://arxiv.org/pdf/2510.25736"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper develops an algorithm to convert PIR schemes into SPIR schemes for graph-based replicated systems where databases are distributed across servers according to graph edges. The authors show that using full common randomness replication across all servers improves SPIR capacity compared to graph-replicated common randomness, specifically achieving a capacity of 1/2 for path graphs with three vertices."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251030] A Privacy-Preserving Ecosystem for Developing Machine Learning Algorithms Using Patient Data: Insights from the TUM.ai Makeathon"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [federated learning, clinical knowledge graph, simulated data, protected execution environment, multi-omics data]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Simon S\xfcwer, Mai Khanh Mai, Christoph Klein, Nicola G\xf6tzenberger, Denis Dali\u0107, Andreas Maier, Jan Baumbach"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Hamburg, Ludwig-Maximilians-Universit\xe4t M\xfcnchen, University of Southern Denmark"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25277",children:"https://arxiv.org/pdf/2510.25277"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a privacy-preserving AI development approach using simulated clinical knowledge graphs and federated learning through FeatureCloud framework. This method allows model development on synthetic data structures followed by secure training within hospital environments without exposing sensitive patient data. The approach was successfully validated in a makeathon challenge where students developed diagnostic models while maintaining data privacy compliance."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251030] Timing Games in Responsive Consensus Protocols"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [blockchain consensus], [timing games, prisoner's dilemma, dynamic block rewards, voting mechanism, optimistic responsiveness]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Kaya Alpturer, Kushal Babel, Aditya Saraf"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Princeton University, Category Labs, Cornell University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25144",children:"https://arxiv.org/pdf/2510.25144"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces dynamic block rewards that decrease with round time and a voting mechanism to measure delays, addressing timing games in responsive consensus protocols. It shows that by carefully setting protocol parameters, validators can coordinate to reach cooperative equilibria where responsiveness promotes faster block proposals. The analysis demonstrates this approach effectively mitigates timing game issues while having only minor effects on latency disparities between validators."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251030] Radar DataTree: A FAIR and Cloud-Native Framework for Scalable Weather Radar Archives"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [weather data infrastructure], [xarray DataTree, Zarr, FM-301/CfRadial 2.1, Icechunk, ACID-compliant storage]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Alfonso Ladino-Rincon, Stephen W. Nesbitt"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Illinois Urbana-Champaign"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24943",children:"https://arxiv.org/pdf/2510.24943"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," Radar DataTree introduces a cloud-native framework that transforms fragmented weather radar archives into FAIR-compliant datasets using hierarchical organization with xarray DataTree and Zarr storage. The system enables efficient parallel computation across thousands of radar scans with minimal preprocessing. Case studies demonstrate significant performance gains in meteorological workflows while providing a reproducible foundation for radar data stewardship and high-performance geoscience."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251030] Can Like Attract Like? A Study of Homonymous Gathering in Networks"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed algorithms], [deterministic algorithm, mobile agents, synchronous rounds, termination detection]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," St\xe9phane Devismes, Yoann Dieudonn\xe9, Arnaud Labourel"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Universit\xe9 de Picardie Jules Verne, Aix Marseille University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25451",children:"https://arxiv.org/pdf/2510.25451"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper studies the gathering problem for mobile agents in networks where agents may share identical labels. The authors provide a complete characterization of gatherable teams and design a deterministic algorithm that gathers all such teams in polynomial time with minimal common knowledge. They also show their approach achieves near-optimal dependency on common knowledge and provides the first deterministic poly-time algorithm for gathering teams with distinct labels without requiring common knowledge."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251030] MoEntwine: Unleashing the Potential of Wafer-scale Chips for Large-scale Expert Parallel Inference"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [expert parallelism, wafer-scale chips, mesh topology, entwined ring mapping, non-invasive balancer, communication optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Xinru Tang, Jingxiang Hou, Dingcheng Jiang, Taiquan Wei, Jiaxin Liu, Jinyi Deng, Huizheng Wang, Qize Yang, Haoran Shang, Chao Li, Yang Hu, Shouyi Yin"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Tsinghua University, Shanghai Jiao Tong University, Shanghai Artificial Intelligence Laboratory"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25258",children:"https://arxiv.org/pdf/2510.25258"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes MoEntwine, a system that optimizes mixture-of-experts model inference on wafer-scale chips through two key techniques: ER-Mapping for balanced communication pressure and NI-Balancer for hiding expert migration overhead. The methods achieve significant communication reduction (up to 62%) and performance improvements in both computation (54%) and communication (22%). Compared to state-of-the-art GPU clusters, the wafer-scale platform delivers 39% higher per-device MoE performance due to better scalability for expert parallelism."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251030] CFL-SparseMed: Communication-Efficient Federated Learning for Medical Imaging with Top-k Sparse Updates"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [Top-K Sparsification, Federated Learning, Non-IID Data, Medical Imaging, Communication Efficiency]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Gousia Habib, Aniket Bhardwaj, Ritvik Sharma, Shoeib Amin Banday, Ishfaq Ahmad Malik"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Helsinki, Shoolini University, King Khalid University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24776",children:"https://arxiv.org/pdf/2510.24776"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes CFL-SparseMed, a federated learning approach that uses Top-k gradient sparsification to reduce communication overhead in medical imaging applications. The method effectively handles non-IID data heterogeneity while maintaining model accuracy, improving FL efficiency and preserving privacy for better diagnostic outcomes."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251030] Holon Streaming: Global Aggregations with Windowed CRDTs"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [stream processing], [Windowed CRDTs, decentralized coordination, global aggregations, exactly-once processing]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Jonas Spenger, Kolya Krafeld, Ruben van Gemeren, Philipp Haller, Paris Carbone"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," KTH Royal Institute of Technology, RISE Research Institutes of Sweden"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25757",children:"https://arxiv.org/pdf/2510.25757"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents Holon Streaming, a stream processing system that uses Windowed Conflict-Free Replicated Data Types (CRDTs) to enable scalable global aggregations through decentralized coordination. The system achieves 5x lower latency and 2x higher throughput compared to existing approaches, with particularly significant improvements during failure scenarios. The research demonstrates that decentralized coordination with deterministic programming models can effectively overcome scalability bottlenecks in stream processing systems."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 19'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251030] KnowCoder-A1: Incentivizing Agentic Reasoning Capability with Outcome Supervision for KBQA ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25101",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251030] Reasoning-Aware GRPO using Process Mining ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25065",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251030] Enhancing Hierarchical Reinforcement Learning through Change Point Detection in Time Series ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24988",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251030] Energy-Efficient Autonomous Driving with Adaptive Perception and Robust Decision ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25205",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251030] Off-policy Reinforcement Learning with Model-based Exploration Augmentation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25529",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251030] RAVR: Reference-Answer-guided Variational Reasoning for Large Language Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25206",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251030] Learning to Attack: Uncovering Privacy Risks in Sequential Data Releases ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24807",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251030] Zero Reinforcement Learning Towards General Domains ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25528",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251030] One-shot Humanoid Whole-body Motion Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25241",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251030] Dense and Diverse Goal Coverage in Multi Goal Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25311",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251030] FELA: A Multi-Agent Evolutionary System for Feature Engineering of Industrial Event Log Data ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25223",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251030] LRT-Diffusion: Calibrated Risk-Aware Guidance for Diffusion Policies ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24983",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251030] MTIR-SQL: Multi-turn Tool-Integrated Reasoning Reinforcement Learning for Text-to-SQL ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25510",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251030] Navigation in a Three-Dimensional Urban Flow using Deep Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25679",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251030] GAP: Graph-Based Agent Planning with Parallel Tool Use and Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25320",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251030] ALDEN: Reinforcement Learning for Active Navigation and Evidence Gathering in Long Documents ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25668",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251030] Multi-party Agent Relation Sampling for Multi-party Ad Hoc Teamwork ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25340",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251030] Learning to Plan & Schedule with Reinforcement-Learned Bimanual Robot Skills ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25634",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251030] Scheduling Your LLM Reinforcement Learning with Reasoning Trees ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24832",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 14'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251030] INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25602",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251030] TV-Rec: Time-Variant Convolutional Filter for Sequential Recommendation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25259",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251030] BSFA: Leveraging the Subspace Dichotomy to Accelerate Neural Network Training ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25244",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251030] Enhancing Hierarchical Reinforcement Learning through Change Point Detection in Time Series ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24988",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251030] RegionE: Adaptive Region-Aware Generation for Efficient Image Editing ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25590",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251030] FaRAccel: FPGA-Accelerated Defense Architecture for Efficient Bit-Flip Attack Resilience in Transformer Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24985",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251030] ESCA: Enabling Seamless Codec Avatar Execution through Algorithm and Hardware Co-Optimization for Virtual Reality ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24787",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251030] CDFlow: Building Invertible Layers with Circulant and Diagonal Matrices ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25323",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251030] Resource-Efficient and Robust Inference of Deep and Bayesian Neural Networks on Embedded and Analog Computing Platforms ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24951",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251030] RLMEval: Evaluating Research-Level Neural Theorem Proving ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25427",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251030] Learning to Plan & Schedule with Reinforcement-Learned Bimanual Robot Skills ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25634",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251030] Idea2Plan: Exploring AI-Powered Research Planning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.24891",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251030] Hawk: Leveraging Spatial Context for Faster Autoregressive Text-to-Image Generation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25739",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251030] MLPrE -- A tool for preprocessing and exploratory data analysis prior to machine learning model construction ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25755",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-10-31",children:"2025-10-31"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"cs.DC total: 8"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251031] Foundations of Fiat-Denominated Loans Collateralized by Cryptocurrencies"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [blockchain finance], [limited-custodial protocols, trusted arbitration, game-theoretical analysis, subgame perfect equilibrium]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Pavel Hub\xe1\u010dek, Jan V\xe1clavek, Michelle Yeo"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Czech Academy of Sciences, Charles University, Firefish, National University of Singapore"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25878",children:"https://arxiv.org/pdf/2510.25878"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper develops secure limited-custodial protocols for fiat-denominated loans collateralized by cryptocurrencies using trusted arbitration. The authors provide a game-theoretical analysis showing these protocols achieve subgame perfect equilibrium. The work establishes foundations for cryptocurrency-backed lending while highlighting future research directions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251031] ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for Efficient MoE Inference"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [adaptive expert prefetching, cache-aware routing, hybrid cross-layer prediction, runtime statistics optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Zixu Shen, Kexin Chu, Yifan Zhang, Dawei Xiang, Runxin Wu, Wei Zhang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Connecticut"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26730",children:"https://arxiv.org/pdf/2510.26730"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," ExpertFlow introduces an adaptive runtime system that combines expert prefetching and cache-aware routing to optimize MoE inference. It continuously adjusts prediction horizons using runtime statistics and fuses pregating information with computational states to anticipate expert needs. The system reduces model stall time to less than 0.1% of baseline, demonstrating efficient MoE inference under memory constraints."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251031] Detecting Anomalies in Machine Learning Infrastructure via Hardware Telemetry"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [hardware telemetry, unsupervised learning, anomaly detection, hardware-centric approach]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Ziji Chen, Steven Chien, Peng Qian, Noa Zilberman"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Oxford"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26008",children:"https://arxiv.org/pdf/2510.26008"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a hardware-centric approach called Reveal that uses hardware telemetry and unsupervised learning to detect anomalies in ML infrastructure without requiring workload knowledge. The system analyzes low-level hardware signals accessible to cloud operators to identify performance issues. This method successfully identified network and configuration problems, accelerating the DeepSeek model by 5.97% while maintaining operator privacy."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251031] Non-Convex Over-the-Air Heterogeneous Federated Learning: A Bias-Variance Trade-off"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [over-the-air federated learning, stochastic gradient descent, successive convex approximation, power-control design, bias-variance trade-off]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Muhammad Faraz Ul Abrar, Nicol\xf2 Michelusi"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Arizona State University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26722",children:"https://arxiv.org/pdf/2510.26722"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a novel over-the-air federated learning approach that allows structured model bias to reduce variance in heterogeneous wireless environments. The method develops a joint power-control design optimized via successive convex approximation using only statistical channel state information. Experiments show this approach accelerates convergence and improves generalization compared to existing baselines by optimizing the bias-variance trade-off."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251031] An All-Reduce Compatible Top-K Compressor for Communication-Efficient Distributed Learning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [gradient sparsification, all-reduce, error feedback, top-k compression, communication compression]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Chuyan Chen, Chenyang Ma, Zhangxin Li, Yutong He, Yanjie Dong, Kun Yuan"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Peking University, Shenzhen MSU-BIT University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26709",children:"https://arxiv.org/pdf/2510.26709"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes ARC-Top-K, an All-Reduce compatible Top-K compressor that aligns sparsity patterns across nodes using gradient sketches to enable efficient communication. This method preserves contraction properties while avoiding costly All-Gather operations. Empirical results show it matches Top-K accuracy while reducing training time by up to 60.7%, combining Rand-K's robustness with Top-K's performance."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251031] Wireless Sensor Networks as Parallel and Distributed Hardware Platform for Artificial Neural Networks"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [wireless sensor networks, parallel distributed processing, artificial neural networks, real-time computation, large-scale problems]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Gursel Serpen"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," The University of Toledo"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26492",children:"https://arxiv.org/pdf/2510.26492"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes using wireless sensor networks as a massively parallel and distributed hardware platform to implement artificial neural network algorithms. The approach enables real-time computation of large-scale complex problems by leveraging hundreds of thousands of processing nodes with onboard processing and wireless communication capabilities. This implementation could revolutionize computing by making it possible to solve very large-scale scientific and engineering problems in real time through massive parallelism and distributed computing."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251031] ReSpec: Towards Optimizing Speculative Decoding in Reinforcement Learning Systems"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [speculative decoding, reinforcement learning, knowledge distillation, dynamic configuration tuning, reward-weighted updates]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Qiaoling Chen, Zijun Liu, Peng Sun, Shenggui Li, Guoteng Wang, Ziming Liu, Yonggang Wen, Siyuan Feng, Tianwei Zhang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Nanyang Technological University, Shanghai Qiji Zhifeng Co., Ltd., Tsinghua University, National University of Singapore, Shanghai Innovation Institute"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26475",children:"https://arxiv.org/pdf/2510.26475"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," ReSpec optimizes speculative decoding for RL-based LLM training by dynamically tuning configurations, updating drafters via knowledge distillation, and weighting updates with rollout rewards. The system achieves up to 4.5\xd7 speedup while maintaining reward convergence and training stability, providing an efficient solution for RL adaptation of large language models."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251031] Environmental Impact of CI/CD Pipelines"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [cloud computing sustainability], [Cloud Carbon Footprint framework, workflow analysis, computational resource optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Nuno Saavedra, Alexandra Mendes, Jo\xe3o F. Ferreira"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," INESC-ID, University of Lisbon, INESC TEC, University of Porto"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26413",children:"https://arxiv.org/pdf/2510.26413"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This study analyzes the environmental impact of GitHub Actions CI/CD pipelines using the Cloud Carbon Footprint framework on a dataset of over 2.2 million workflow runs. The research reveals substantial carbon and water footprints, estimating 456.9 MTCO2e and 5,738.2 kiloliters respectively in the most likely scenario. The paper recommends mitigation strategies including regional deployment optimization, stricter scheduling policies, and repository size reduction to reduce environmental impact."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 32'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] GUI Knowledge Bench: Revealing the Knowledge Gap Behind VLM Failures in GUI Tasks ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26098",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] Conformal Prediction Beyond the Horizon: Distribution-Free Inference for Policy Evaluation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26026",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] Do Not Step Into the Same River Twice: Learning to Reason from Trial and Error ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26109",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] Reinforcement Learning for Pollution Detection in a Randomized, Sparse and Nonstationary Environment with an Autonomous Underwater Vehicle ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26347",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] MedVLSynther: Synthesizing High-Quality Visual Question Answering from Medical Documents with Generator-Verifier LMMs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25867",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] Adaptive Context Length Optimization with Low-Frequency Truncation for Multi-Agent Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26389",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] Estimating cognitive biases with attention-aware inverse planning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25951",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] Human-in-the-loop Online Rejection Sampling for Robotic Manipulation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26406",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] Approximating Human Preferences Using a Multi-Judge Learned System ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25884",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] Learning to Manage Investment Portfolios beyond Simple Utility Functions ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26165",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] Data-Efficient RLVR via Off-Policy Influence Guidance ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26491",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] Metis-SPECS: Decoupling Multimodal Learning via Self-distilled Preference-based Cold Start ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25801",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] Kimi Linear: An Expressive, Efficient Attention Architecture ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26692",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] Reasoning Curriculum: Bootstrapping Broad LLM Reasoning from Math ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26143",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25992",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] Action-Driven Processes for Continuous-Time Control ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26672",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] Graph-Enhanced Policy Optimization in LLM Agent Training ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26270",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] ",(0,s.jsxs)(e.span,{className:"katex",children:[(0,s.jsx)(e.span,{className:"katex-mathml",children:(0,s.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,s.jsxs)(e.semantics,{children:[(0,s.jsx)(e.mrow,{children:(0,s.jsxs)(e.msub,{children:[(0,s.jsx)(e.mi,{children:"\u03c0"}),(0,s.jsx)(e.mtext,{mathvariant:"monospace",children:"RL"})]})}),(0,s.jsx)(e.annotation,{encoding:"application/x-tex",children:"\u03c0_\\texttt{RL}"})]})})}),(0,s.jsx)(e.span,{className:"katex-html","aria-hidden":"true",children:(0,s.jsxs)(e.span,{className:"base",children:[(0,s.jsx)(e.span,{className:"strut",style:{height:"0.5806em",verticalAlign:"-0.15em"}}),(0,s.jsxs)(e.span,{className:"mord",children:[(0,s.jsx)(e.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"\u03c0"}),(0,s.jsx)(e.span,{className:"msupsub",children:(0,s.jsxs)(e.span,{className:"vlist-t vlist-t2",children:[(0,s.jsxs)(e.span,{className:"vlist-r",children:[(0,s.jsx)(e.span,{className:"vlist",style:{height:"0.2778em"},children:(0,s.jsxs)(e.span,{style:{top:"-2.55em",marginLeft:"-0.0359em",marginRight:"0.05em"},children:[(0,s.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,s.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,s.jsx)(e.span,{className:"mord text mtight",children:(0,s.jsx)(e.span,{className:"mord texttt mtight",children:"RL"})})})]})}),(0,s.jsx)(e.span,{className:"vlist-s",children:"\u200b"})]}),(0,s.jsx)(e.span,{className:"vlist-r",children:(0,s.jsx)(e.span,{className:"vlist",style:{height:"0.15em"},children:(0,s.jsx)(e.span,{})})})]})})]})]})})]}),": Online RL Fine-tuning for Flow-based Vision-Language-Action Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25889",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] One Model to Critique Them All: Rewarding Agentic Tool-Use via Efficient Reasoning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26167",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] A Game-Theoretic Spatio-Temporal Reinforcement Learning Framework for Collaborative Public Resource Allocation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26184",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] A General Incentives-Based Framework for Fairness in Multi-agent Resource Allocation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26740",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] Think Outside the Policy: In-Context Steered Policy Optimization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26519",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] Multi-Agent Reinforcement Learning for Market Making: Competition without Collusion ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25929",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] Non-myopic Matching and Rebalancing in Large-Scale On-Demand Ride-Pooling Systems Using Simulation-Informed Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25796",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] Defeating the Training-Inference Mismatch via FP16 ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26788",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] PORTool: Tool-Use LLM Training with Rewarded Tree ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26020",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] EgoExo-Con: Exploring View-Invariant Video Temporal Understanding ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26113",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] Network-Constrained Policy Optimization for Adaptive Multi-agent Vehicle Routing ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26089",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] InputDSA: Demixing then Comparing Recurrent and Externally Driven Dynamics ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25943",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] InfoFlow: Reinforcing Search Agent Via Reward Density Optimization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26575",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] The Era of Agentic Organization: Learning to Organize with Language Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26658",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] Offline Clustering of Preference Learning with Active-data Augmentation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26301",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 9'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] Risks and Opportunities in Human-Machine Teaming in Operationalizing Machine Learning Target Variables ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25974",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] PRESTO: Preimage-Informed Instruction Optimization for Prompting Black-Box LLMs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25808",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] Data-Efficient RLVR via Off-Policy Influence Guidance ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26491",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] AttnCache: Accelerating Self-Attention Inference for LLM Prefill via Attention Cache ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25979",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] The FM Agent ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26144",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] FlowQ-Net: A Generative Framework for Automated Quantum Circuit Design ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26688",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] Evaluating the Impact of LLM-Assisted Annotation in a Perspectivized Setting: the Case of FrameNet Annotation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25904",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] Polybasic Speculative Decoding Through a Theoretical Perspective ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26527",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251031] The Kinetics of Reasoning: How Chain-of-Thought Shapes Learning in Transformers? ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25791",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-11-01",children:"2025-11-01"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"cs.DC total: 8"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251101] Detecting Anomalies in Machine Learning Infrastructure via Hardware Telemetry"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [hardware telemetry, unsupervised learning, anomaly detection, performance optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Ziji Chen, Steven Chien, Peng Qian, Noa Zilberman"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Oxford"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26008",children:"https://arxiv.org/pdf/2510.26008"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes System-X/Reveal, a hardware-centric framework that uses low-level hardware telemetry and unsupervised learning to detect anomalies in ML infrastructure. This approach enables performance optimization without requiring workload knowledge from virtualized cloud environments. The system successfully identified configuration issues and accelerated the DeepSeek model by 5.97%."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251101] Foundations of Fiat-Denominated Loans Collateralized by Cryptocurrencies"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [decentralized finance], [trusted arbitration, game-theoretical analysis, subgame perfect equilibrium, limited-custodial protocols]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Pavel Hub\xe1\u010dek, Jan V\xe1clavek, Michelle Yeo"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Czech Academy of Sciences, Charles University, National University of Singapore, Firefish"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25878",children:"https://arxiv.org/pdf/2510.25878"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes secure protocols for fiat-denominated loans collateralized by cryptocurrencies using trusted arbitration and limited-custodial approaches. The authors provide game-theoretical analysis showing these protocols achieve subgame perfect equilibrium. The work establishes foundations for cryptocurrency-backed lending systems while highlighting future research directions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251101] Wireless Sensor Networks as Parallel and Distributed Hardware Platform for Artificial Neural Networks"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [wireless sensor networks, parallel distributed processing, artificial neural networks, real-time computation, large-scale problems]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Gursel Serpen"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," The University of Toledo"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26492",children:"https://arxiv.org/pdf/2510.26492"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes using wireless sensor networks as a massively parallel and distributed hardware platform to implement artificial neural network algorithms. The method leverages networks of processing nodes with onboard computation and wireless communication capabilities to enable real-time solutions for large-scale complex problems. The authors conclude this approach could revolutionize computing by making affordable, practical neural network solutions possible for very large-scale scientific and engineering applications."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251101] ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for Efficient MoE Inference"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [adaptive expert prefetching, cache-aware routing, hybrid cross-layer prediction, runtime statistics, memory coordination]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Zixu Shen, Kexin Chu, Yifan Zhang, Dawei Xiang, Runxin Wu, Wei Zhang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Connecticut"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26730",children:"https://arxiv.org/pdf/2510.26730"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," ExpertFlow introduces an adaptive runtime system for MoE inference that combines expert prefetching and cache-aware routing using runtime statistics and hybrid prediction. It dynamically adjusts prediction horizons and fuses pregating information with computational states to anticipate expert needs. The system reduces model stall time to less than 0.1% of baseline, demonstrating efficient MoE inference under memory constraints."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251101] Non-Convex Over-the-Air Heterogeneous Federated Learning: A Bias-Variance Trade-off"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [over-the-air federated learning, stochastic gradient descent, successive convex approximation, power-control design, bias-variance trade-off]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Muhammad Faraz Ul Abrar, Nicol\xf2 Michelusi"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Arizona State University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26722",children:"https://arxiv.org/pdf/2510.26722"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper develops a novel over-the-air federated learning approach that allows structured model bias to reduce variance under heterogeneous wireless conditions. The method uses successive convex approximation for joint power-control optimization requiring only statistical channel state information. Experiments show this approach accelerates convergence and improves generalization compared to prior baselines by optimizing the bias-variance trade-off."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251101] An All-Reduce Compatible Top-K Compressor for Communication-Efficient Distributed Learning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [gradient sparsification, All-Reduce, Top-K compression, error feedback, communication compression]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Chuyan Chen, Chenyang Ma, Zhangxin Li, Yutong He, Yanjie Dong, Kun Yuan"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Peking University, Shenzhen MSU-BIT University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26709",children:"https://arxiv.org/pdf/2510.26709"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes ARC-Top-K, an All-Reduce compatible Top-K compressor that aligns sparsity patterns across nodes using gradient sketches to enable efficient distributed training. This method preserves globally significant gradient information while avoiding costly All-Gather operations. Empirical results show it matches Top-K accuracy while reducing training time by up to 60.7%, combining Rand-K's robustness with Top-K's performance."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251101] Environmental Impact of CI/CD Pipelines"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [cloud computing sustainability], [carbon footprint analysis, water footprint analysis, GitHub Actions workflow analysis, Cloud Carbon Footprint framework]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Nuno Saavedra, Alexandra Mendes, Jo\xe3o F. Ferreira"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," INESC-ID, University of Lisbon, INESC TEC, University of Porto"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26413",children:"https://arxiv.org/pdf/2510.26413"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This study analyzes the environmental impact of GitHub Actions CI/CD pipelines using the Cloud Carbon Footprint framework on a dataset of over 2.2 million workflow runs. The research reveals substantial carbon and water footprints, estimating 456.9 MTCO2e carbon emissions and 5,738.2 kiloliters water consumption in the most likely scenario. The paper recommends mitigation strategies including regional deployment optimization, stricter deactivation policies, and repository size reduction."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251101] ReSpec: Towards Optimizing Speculative Decoding in Reinforcement Learning Systems"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [speculative decoding, reinforcement learning, knowledge distillation, EAGLE-3, policy optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Qiaoling Chen, Zijun Liu, Peng Sun, Shenggui Li, Guoteng Wang, Ziming Liu, Yonggang Wen, Siyuan Feng, Tianwei Zhang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Nanyang Technological University, Shanghai Qiji Zhifeng Co., Ltd., Tsinghua University, National University of Singapore, Shanghai Innovation Institute"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26475",children:"https://arxiv.org/pdf/2510.26475"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces ReSpec, a system that optimizes speculative decoding for reinforcement learning-based LLM training by dynamically tuning configurations, updating drafters via knowledge distillation, and weighting updates with rollout rewards. The method achieves up to 4.5\xd7 speedup while maintaining reward convergence and training stability, providing an efficient solution for RL-based LLM adaptation."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 32'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] EgoExo-Con: Exploring View-Invariant Video Temporal Understanding ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26113",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] Graph-Enhanced Policy Optimization in LLM Agent Training ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26270",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] Network-Constrained Policy Optimization for Adaptive Multi-agent Vehicle Routing ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26089",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] InfoFlow: Reinforcing Search Agent Via Reward Density Optimization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26575",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] A General Incentives-Based Framework for Fairness in Multi-agent Resource Allocation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26740",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] Data-Efficient RLVR via Off-Policy Influence Guidance ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26491",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] Kimi Linear: An Expressive, Efficient Attention Architecture ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26692",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] Offline Clustering of Preference Learning with Active-data Augmentation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26301",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] GUI Knowledge Bench: Revealing the Knowledge Gap Behind VLM Failures in GUI Tasks ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26098",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] MedVLSynther: Synthesizing High-Quality Visual Question Answering from Medical Documents with Generator-Verifier LMMs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25867",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] Do Not Step Into the Same River Twice: Learning to Reason from Trial and Error ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26109",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] Defeating the Training-Inference Mismatch via FP16 ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26788",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] Reinforcement Learning for Pollution Detection in a Randomized, Sparse and Nonstationary Environment with an Autonomous Underwater Vehicle ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26347",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] InputDSA: Demixing then Comparing Recurrent and Externally Driven Dynamics ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25943",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] Adaptive Context Length Optimization with Low-Frequency Truncation for Multi-Agent Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26389",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] Approximating Human Preferences Using a Multi-Judge Learned System ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25884",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] The Era of Agentic Organization: Learning to Organize with Language Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26658",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] Reasoning Curriculum: Bootstrapping Broad LLM Reasoning from Math ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26143",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] Human-in-the-loop Online Rejection Sampling for Robotic Manipulation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26406",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] Conformal Prediction Beyond the Horizon: Distribution-Free Inference for Policy Evaluation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26026",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] Metis-SPECS: Decoupling Multimodal Learning via Self-distilled Preference-based Cold Start ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25801",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] Learning to Manage Investment Portfolios beyond Simple Utility Functions ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26165",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] One Model to Critique Them All: Rewarding Agentic Tool-Use via Efficient Reasoning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26167",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] A Game-Theoretic Spatio-Temporal Reinforcement Learning Framework for Collaborative Public Resource Allocation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26184",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25992",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] Action-Driven Processes for Continuous-Time Control ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26672",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] Estimating cognitive biases with attention-aware inverse planning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25951",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] ",(0,s.jsxs)(e.span,{className:"katex",children:[(0,s.jsx)(e.span,{className:"katex-mathml",children:(0,s.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,s.jsxs)(e.semantics,{children:[(0,s.jsx)(e.mrow,{children:(0,s.jsxs)(e.msub,{children:[(0,s.jsx)(e.mi,{children:"\u03c0"}),(0,s.jsx)(e.mtext,{mathvariant:"monospace",children:"RL"})]})}),(0,s.jsx)(e.annotation,{encoding:"application/x-tex",children:"\u03c0_\\texttt{RL}"})]})})}),(0,s.jsx)(e.span,{className:"katex-html","aria-hidden":"true",children:(0,s.jsxs)(e.span,{className:"base",children:[(0,s.jsx)(e.span,{className:"strut",style:{height:"0.5806em",verticalAlign:"-0.15em"}}),(0,s.jsxs)(e.span,{className:"mord",children:[(0,s.jsx)(e.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"\u03c0"}),(0,s.jsx)(e.span,{className:"msupsub",children:(0,s.jsxs)(e.span,{className:"vlist-t vlist-t2",children:[(0,s.jsxs)(e.span,{className:"vlist-r",children:[(0,s.jsx)(e.span,{className:"vlist",style:{height:"0.2778em"},children:(0,s.jsxs)(e.span,{style:{top:"-2.55em",marginLeft:"-0.0359em",marginRight:"0.05em"},children:[(0,s.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,s.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,s.jsx)(e.span,{className:"mord text mtight",children:(0,s.jsx)(e.span,{className:"mord texttt mtight",children:"RL"})})})]})}),(0,s.jsx)(e.span,{className:"vlist-s",children:"\u200b"})]}),(0,s.jsx)(e.span,{className:"vlist-r",children:(0,s.jsx)(e.span,{className:"vlist",style:{height:"0.15em"},children:(0,s.jsx)(e.span,{})})})]})})]})]})})]}),": Online RL Fine-tuning for Flow-based Vision-Language-Action Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25889",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] Non-myopic Matching and Rebalancing in Large-Scale On-Demand Ride-Pooling Systems Using Simulation-Informed Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25796",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] Think Outside the Policy: In-Context Steered Policy Optimization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26519",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] Multi-Agent Reinforcement Learning for Market Making: Competition without Collusion ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25929",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] PORTool: Tool-Use LLM Training with Rewarded Tree ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26020",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 9'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] Data-Efficient RLVR via Off-Policy Influence Guidance ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26491",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] PRESTO: Preimage-Informed Instruction Optimization for Prompting Black-Box LLMs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25808",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] The FM Agent ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26144",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] Risks and Opportunities in Human-Machine Teaming in Operationalizing Machine Learning Target Variables ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25974",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] AttnCache: Accelerating Self-Attention Inference for LLM Prefill via Attention Cache ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25979",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] FlowQ-Net: A Generative Framework for Automated Quantum Circuit Design ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26688",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] The Kinetics of Reasoning: How Chain-of-Thought Shapes Learning in Transformers? ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25791",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] Evaluating the Impact of LLM-Assisted Annotation in a Perspectivized Setting: the Case of FrameNet Annotation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25904",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251101] Polybasic Speculative Decoding Through a Theoretical Perspective ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26527",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-11-02",children:"2025-11-02"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"cs.DC total: 8"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251102] Foundations of Fiat-Denominated Loans Collateralized by Cryptocurrencies"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [decentralized finance], [blockchain protocols, trusted arbitration, game-theoretical analysis, subgame perfect equilibrium]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Pavel Hub\xe1\u010dek, Jan V\xe1clavek, Michelle Yeo"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Czech Academy of Sciences, Charles University, National University of Singapore, Firefish"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25878",children:"https://arxiv.org/pdf/2510.25878"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper develops limited-custodial protocols for fiat-denominated loans collateralized by cryptocurrencies using trusted arbitration. The authors provide a game-theoretical analysis showing these protocols can achieve secure lending arrangements. The work establishes foundations for cryptocurrency-backed lending systems while highlighting future research directions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251102] Detecting Anomalies in Machine Learning Infrastructure via Hardware Telemetry"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [hardware telemetry, unsupervised learning, anomaly detection, hardware-centric approach]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Ziji Chen, Steven Chien, Peng Qian, Noa Zilberman"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Oxford"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26008",children:"https://arxiv.org/pdf/2510.26008"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a hardware-centric approach called Reveal that uses hardware telemetry signals and unsupervised learning to detect anomalies in ML infrastructure. This method requires no workload knowledge and relies solely on hardware metrics accessible to cloud operators. The system successfully identified configuration issues and accelerated the DeepSeek model by 5.97%, demonstrating effective system-level optimization without user workload insights."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251102] ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for Efficient MoE Inference"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [adaptive expert prefetching, cache-aware routing, hybrid cross-layer prediction, runtime statistics optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Zixu Shen, Kexin Chu, Yifan Zhang, Dawei Xiang, Runxin Wu, Wei Zhang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Connecticut"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26730",children:"https://arxiv.org/pdf/2510.26730"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," ExpertFlow introduces an adaptive runtime system for MoE inference that combines expert prefetching and cache-aware routing using runtime statistics and hybrid prediction schemes. The system dynamically adjusts prediction horizons and fuses pregating information with computational states to anticipate expert needs. Evaluation shows it reduces model stall time to less than 0.1% of baseline, optimizing inference under memory constraints."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251102] Wireless Sensor Networks as Parallel and Distributed Hardware Platform for Artificial Neural Networks"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [wireless sensor networks, parallel distributed processing, artificial neural networks, real-time computation, large-scale problems]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Gursel Serpen"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," The University of Toledo"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26492",children:"https://arxiv.org/pdf/2510.26492"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes using wireless sensor networks as a massively parallel and distributed hardware platform to implement artificial neural network algorithms. The approach enables real-time computation of large-scale complex problems by leveraging hundreds of thousands of processing nodes with onboard processing and wireless communication capabilities. This hardware realization could revolutionize computing by making it possible to solve very large-scale scientific and engineering problems in real time."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251102] Non-Convex Over-the-Air Heterogeneous Federated Learning: A Bias-Variance Trade-off"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [over-the-air federated learning, stochastic gradient descent, successive convex approximation, power-control design, bias-variance trade-off]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Muhammad Faraz Ul Abrar, Nicol\xf2 Michelusi"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Arizona State University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26722",children:"https://arxiv.org/pdf/2510.26722"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper develops a novel over-the-air federated learning approach that allows structured model bias to reduce variance under heterogeneous wireless conditions. The method uses successive convex approximation for joint power-control optimization requiring only statistical channel state information. Experiments show this bias-variance optimized design accelerates convergence and improves generalization compared to existing baselines."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251102] An All-Reduce Compatible Top-K Compressor for Communication-Efficient Distributed Learning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [gradient sparsification, all-reduce, error feedback, top-k compression]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Chuyan Chen, Chenyang Ma, Zhangxin Li, Yutong He, Yanjie Dong, Kun Yuan"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Peking University, Shenzhen MSU-BIT University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26709",children:"https://arxiv.org/pdf/2510.26709"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes ARC-Top-K, an All-Reduce compatible Top-K compressor that aligns sparsity patterns across nodes using gradient sketches to enable efficient communication. This method preserves globally significant gradient information while maintaining the contraction property and compatibility with All-Reduce operations. Empirical results show it matches Top-K accuracy while reducing training time by up to 60.7%, combining Rand-K's robustness with Top-K's performance."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251102] Environmental Impact of CI/CD Pipelines"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [cloud computing sustainability], [carbon footprint analysis, water footprint analysis, GitHub Actions, workflow optimization, regional energy mix analysis]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Nuno Saavedra, Alexandra Mendes, Jo\xe3o F. Ferreira"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," INESC-ID, University of Lisbon, INESC TEC, University of Porto"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26413",children:"https://arxiv.org/pdf/2510.26413"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper analyzes the environmental impact of GitHub Actions CI/CD pipelines using the Cloud Carbon Footprint framework on a dataset of over 2.2 million workflow runs. The study reveals substantial carbon and water footprints, estimating 456.9 MTCO2e and 5,738.2 kiloliters respectively in the most likely scenario. The authors recommend mitigation strategies including deploying runners in environmentally favorable regions and optimizing computational resource usage."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251102] ReSpec: Towards Optimizing Speculative Decoding in Reinforcement Learning Systems"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [speculative decoding, reinforcement learning, knowledge distillation, dynamic configuration tuning, rollout reward weighting]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Qiaoling Chen, Zijun Liu, Peng Sun, Shenggui Li, Guoteng Wang, Ziming Liu, Yonggang Wen, Siyuan Feng, Tianwei Zhang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Nanyang Technological University, Shanghai Qiji Zhifeng Co., Ltd., Tsinghua University, National University of Singapore, Shanghai Innovation Institute"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26475",children:"https://arxiv.org/pdf/2510.26475"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces ReSpec, a system that optimizes speculative decoding for reinforcement learning training of large language models. It addresses three key challenges in RL systems through dynamic configuration tuning, drafter evolution via knowledge distillation, and reward-weighted updates. The method achieves up to 4.5\xd7 speedup while maintaining reward convergence and training stability across Qwen models from 3B to 14B parameters."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 32'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] GUI Knowledge Bench: Revealing the Knowledge Gap Behind VLM Failures in GUI Tasks ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26098",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] A General Incentives-Based Framework for Fairness in Multi-agent Resource Allocation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26740",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] Do Not Step Into the Same River Twice: Learning to Reason from Trial and Error ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26109",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] Reinforcement Learning for Pollution Detection in a Randomized, Sparse and Nonstationary Environment with an Autonomous Underwater Vehicle ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26347",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] Defeating the Training-Inference Mismatch via FP16 ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26788",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] MedVLSynther: Synthesizing High-Quality Visual Question Answering from Medical Documents with Generator-Verifier LMMs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25867",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] Adaptive Context Length Optimization with Low-Frequency Truncation for Multi-Agent Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26389",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] InputDSA: Demixing then Comparing Recurrent and Externally Driven Dynamics ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25943",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] Estimating cognitive biases with attention-aware inverse planning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25951",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] Human-in-the-loop Online Rejection Sampling for Robotic Manipulation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26406",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] Approximating Human Preferences Using a Multi-Judge Learned System ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25884",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] Conformal Prediction Beyond the Horizon: Distribution-Free Inference for Policy Evaluation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26026",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] Offline Clustering of Preference Learning with Active-data Augmentation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26301",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] EgoExo-Con: Exploring View-Invariant Video Temporal Understanding ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26113",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] Learning to Manage Investment Portfolios beyond Simple Utility Functions ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26165",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] Data-Efficient RLVR via Off-Policy Influence Guidance ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26491",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] Metis-SPECS: Decoupling Multimodal Learning via Self-distilled Preference-based Cold Start ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25801",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] Reasoning Curriculum: Bootstrapping Broad LLM Reasoning from Math ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26143",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25992",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] Graph-Enhanced Policy Optimization in LLM Agent Training ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26270",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] Action-Driven Processes for Continuous-Time Control ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26672",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] ",(0,s.jsxs)(e.span,{className:"katex",children:[(0,s.jsx)(e.span,{className:"katex-mathml",children:(0,s.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,s.jsxs)(e.semantics,{children:[(0,s.jsx)(e.mrow,{children:(0,s.jsxs)(e.msub,{children:[(0,s.jsx)(e.mi,{children:"\u03c0"}),(0,s.jsx)(e.mtext,{mathvariant:"monospace",children:"RL"})]})}),(0,s.jsx)(e.annotation,{encoding:"application/x-tex",children:"\u03c0_\\texttt{RL}"})]})})}),(0,s.jsx)(e.span,{className:"katex-html","aria-hidden":"true",children:(0,s.jsxs)(e.span,{className:"base",children:[(0,s.jsx)(e.span,{className:"strut",style:{height:"0.5806em",verticalAlign:"-0.15em"}}),(0,s.jsxs)(e.span,{className:"mord",children:[(0,s.jsx)(e.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"\u03c0"}),(0,s.jsx)(e.span,{className:"msupsub",children:(0,s.jsxs)(e.span,{className:"vlist-t vlist-t2",children:[(0,s.jsxs)(e.span,{className:"vlist-r",children:[(0,s.jsx)(e.span,{className:"vlist",style:{height:"0.2778em"},children:(0,s.jsxs)(e.span,{style:{top:"-2.55em",marginLeft:"-0.0359em",marginRight:"0.05em"},children:[(0,s.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,s.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,s.jsx)(e.span,{className:"mord text mtight",children:(0,s.jsx)(e.span,{className:"mord texttt mtight",children:"RL"})})})]})}),(0,s.jsx)(e.span,{className:"vlist-s",children:"\u200b"})]}),(0,s.jsx)(e.span,{className:"vlist-r",children:(0,s.jsx)(e.span,{className:"vlist",style:{height:"0.15em"},children:(0,s.jsx)(e.span,{})})})]})})]})]})})]}),": Online RL Fine-tuning for Flow-based Vision-Language-Action Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25889",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] One Model to Critique Them All: Rewarding Agentic Tool-Use via Efficient Reasoning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26167",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] A Game-Theoretic Spatio-Temporal Reinforcement Learning Framework for Collaborative Public Resource Allocation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26184",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] Think Outside the Policy: In-Context Steered Policy Optimization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26519",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] Multi-Agent Reinforcement Learning for Market Making: Competition without Collusion ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25929",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] Non-myopic Matching and Rebalancing in Large-Scale On-Demand Ride-Pooling Systems Using Simulation-Informed Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25796",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] Kimi Linear: An Expressive, Efficient Attention Architecture ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26692",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] PORTool: Tool-Use LLM Training with Rewarded Tree ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26020",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] Network-Constrained Policy Optimization for Adaptive Multi-agent Vehicle Routing ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26089",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] InfoFlow: Reinforcing Search Agent Via Reward Density Optimization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26575",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] The Era of Agentic Organization: Learning to Organize with Language Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26658",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 9'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] Risks and Opportunities in Human-Machine Teaming in Operationalizing Machine Learning Target Variables ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25974",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] PRESTO: Preimage-Informed Instruction Optimization for Prompting Black-Box LLMs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25808",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] Data-Efficient RLVR via Off-Policy Influence Guidance ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26491",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] AttnCache: Accelerating Self-Attention Inference for LLM Prefill via Attention Cache ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25979",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] The FM Agent ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26144",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] FlowQ-Net: A Generative Framework for Automated Quantum Circuit Design ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26688",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] Evaluating the Impact of LLM-Assisted Annotation in a Perspectivized Setting: the Case of FrameNet Annotation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25904",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] Polybasic Speculative Decoding Through a Theoretical Perspective ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26527",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251102] The Kinetics of Reasoning: How Chain-of-Thought Shapes Learning in Transformers? ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25791",children:"link"})]}),"\n"]})]})}function d(i={}){const{wrapper:e}={...(0,a.R)(),...i.components};return e?(0,s.jsx)(e,{...i,children:(0,s.jsx)(h,{...i})}):h(i)}},8453:(i,e,n)=>{n.d(e,{R:()=>t,x:()=>l});var r=n(6540);const s={},a=r.createContext(s);function t(i){const e=r.useContext(a);return r.useMemo(function(){return"function"==typeof i?i(e):{...e,...i}},[e,i])}function l(i){let e;return e=i.disableParentContext?"function"==typeof i.components?i.components(s):i.components||s:t(i.components),r.createElement(a.Provider,{value:e},i.children)}}}]);