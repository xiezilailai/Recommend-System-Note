"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[659],{7196:(i,e,n)=>{n.r(e),n.d(e,{assets:()=>o,contentTitle:()=>l,default:()=>d,frontMatter:()=>t,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"daily/20251110-20251116","title":"20251110-20251116","description":"2025-11-12","source":"@site/docs/daily/20251110-20251116.md","sourceDirName":"daily","slug":"/daily/20251110-20251116","permalink":"/Recommend-System-Note/daily/20251110-20251116","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1763475705000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Introduction","permalink":"/Recommend-System-Note/intro"},"next":{"title":"20251117-20251123","permalink":"/Recommend-System-Note/daily/20251117-20251123"}}');var s=n(4848),a=n(8453);const t={},l="20251110-20251116",o={},c=[{value:"2025-11-12",id:"2025-11-12",level:2},{value:"2025-11-13",id:"2025-11-13",level:2},{value:"2025-11-14",id:"2025-11-14",level:2}];function h(i){const e={a:"a",annotation:"annotation",h1:"h1",h2:"h2",header:"header",li:"li",math:"math",mn:"mn",mrow:"mrow",msub:"msub",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,a.R)(),...i.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"20251110-20251116",children:"20251110-20251116"})}),"\n",(0,s.jsx)(e.h2,{id:"2025-11-12",children:"2025-11-12"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"cs.DC total: 21"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251112] HyProv: Hybrid Provenance Management for Scientific Workflows"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [hybrid provenance management, centralized component, federated querying, workflow-aware queries, Airflow, Kubernetes]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Vasilis Bountris, Lauritz Thamsen, Ulf Leser"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Humboldt-Universit\xe4t zu Berlin, University of Glasgow"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07574",children:"https://arxiv.org/pdf/2511.07574"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," HyProv introduces a hybrid provenance management system that combines centralized and federated approaches to handle workflow provenance data. The system uses a centralized component for workflow-specific provenance and federated querying for large-scale execution logs. Experiments show that HyProv scales to large workflows, provides sub-second query latencies, and adds minimal overhead to cluster resources."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251112] Network and Systems Performance Characterization of MCP-Enabled LLM Agents"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [Model Context Protocol, token efficiency, parallel tool calls, task abort mechanisms, performance characterization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Zihao Ding, Mufeng Zhu, Yao Liu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Rutgers University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07426",children:"https://arxiv.org/pdf/2511.07426"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper conducts a measurement-based analysis of MCP-enabled LLM interactions, examining how different models and configurations affect token usage, costs, and performance. The study reveals significant trade-offs between capability enhancement and increased computational overhead in MCP workflows. The findings suggest optimizations like parallel tool calls to develop more efficient and cost-effective agent systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251112] Enhancing reliability in AI inference services: An empirical study on real production incidents"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [incident taxonomy, traffic routing, GPU capacity-aware routing, connection liveness, endpoint isolation, auto-detection, hotfix]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Bhala Ranganathan, Mickey Zhang, Kai Wu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Microsoft"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07424",children:"https://arxiv.org/pdf/2511.07424"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents an empirical study of 156 high-severity production incidents in large language model inference services, developing a taxonomy and methodology grounded in operational experience. The research identifies dominant failure modes and mitigation strategies, showing that systematic analysis can drive more reliable and cost-efficient LLM serving at scale. The study also provides a practitioner-oriented adoption checklist to help others replicate their approach."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251112] DynaKV: Enabling Accurate and Efficient Long-Sequence LLM Decoding on Smartphones"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [KVCache management, cluster adaptation, flash management, memory virtualization, retrieval-based methods]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Tuowei Wang, Minxing Huang, Fengzu Li, Ligeng Chen, Jinrui Zhang, Ju Ren"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Tsinghua University, Honor Device Co., Ltd."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07427",children:"https://arxiv.org/pdf/2511.07427"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," DynaKV is an adaptive KVCache management system for smartphones that uses migration-free cluster adaptation, continuity-centric flash management, and memory-efficient cache design to handle long-sequence LLM decoding. It improves retrieval accuracy by 1.38\xd7 and reduces latency by 1.47\xd7 compared to state-of-the-art solutions while addressing smartphone-specific memory and bandwidth constraints."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251112] An Evaluation of LLMs Inference on Popular Single-board Computers"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [quantization, on-device inference, edge computing, benchmarking, single-board computers]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Tung, Nguyen, Tuyen Nguyen"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," BillulloNex, University of Technology Sydney"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07425",children:"https://arxiv.org/pdf/2511.07425"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper benchmarks the performance of 25 quantized LLMs across three single-board computers using Ollama and Llamafile runtimes. The study evaluates generation throughput, memory usage, and power consumption under realistic workloads. Results show SBCs can reliably support models up to 1.5B parameters, with Llamafile achieving 4x higher throughput and 30-40% lower power usage than Ollama."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251112] From Attention to Disaggregation: Tracing the Evolution of LLM Inference"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [disaggregated inference, distributed systems, service decomposition, resource disaggregation, workload partitioning, prefill phase, decode phase]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Madabattula Rajesh Kumar, Srinivasa Rao Aravilli, Mustafa Saify, Shashank Srivastava"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Capital One"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07422",children:"https://arxiv.org/pdf/2511.07422"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes disaggregated inference as an architectural shift for LLM deployment, separating compute-intensive prefill from memory-intensive decode phases into independently scalable components. This approach addresses the multi-objective optimization challenge of minimizing latency while maximizing throughput and reducing costs. The main conclusion is that disaggregation mitigates resource contention and enables independent optimization of key inference metrics like Time to First Token and Inter Token Latency."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251112] SemanticForge: Repository-Level Code Generation through Semantic Knowledge Graphs and Constraint Satisfaction"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [semantic knowledge graphs, constraint satisfaction, SMT solving, beam search, dual static-dynamic graphs]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Wuyang Zhang, Chenkai Zhang, Zhen Luo, Jianming Ma, Wangming Yuan, Chuqiao Gu, Chenwei Feng"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Massachusetts Amherst, Northeastern University, George Mason University, Carnegie Mellon University, Auckland University of Technology"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07584",children:"https://arxiv.org/pdf/2511.07584"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," SemanticForge introduces a repository-level code generation system that combines semantic knowledge graphs with constraint satisfaction to address LLM hallucinations. The system uses dual static-dynamic knowledge graphs and integrated SMT solving during beam search to ensure semantic correctness. Evaluation shows 49.8% Pass@1 performance with significant reductions in logical and schematic hallucinations while maintaining sub-3s latency."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251112] Towards Affordable, Adaptive and Automatic GNN Training on CPU-GPU Heterogeneous Platforms"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [locality-aware sampling, fine-grained parallelism scheduling, reinforcement learning, CPU-GPU heterogeneous platforms]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Tong Qiao, Ao Zhou, Yingjie Qi, Yiou Wang, Han Wan, Jianlei Yang, Chunming Hu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Beihang University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07421",children:"https://arxiv.org/pdf/2511.07421"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces A3GNN, a framework that optimizes GNN training on CPU-GPU platforms through locality-aware sampling and fine-grained parallelism scheduling, using reinforcement learning to achieve optimal trade-offs. The system enables affordable GNN training by efficiently utilizing resource-constrained hardware. Experiments show it can outperform high-end GPUs, with seven 2080Ti GPUs achieving up to 1.8\xd7 higher throughput than two A100 GPUs with minimal accuracy loss."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251112] Synera: Synergistic LLM Serving across Device and Cloud at Scale"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [device-cloud synergy, selective offloading, parallel inference, scalable batching, SLM-LLM synergy]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Genglin Wang, Liekang Zeng, Bufang Yang, Kaiwei Liu, Guoliang Xing, Chumin Sun, Li Zhou, Jie Sun, Zhenyu Yan"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," The Chinese University of Hong Kong, Huawei Technologies Co. Ltd."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07423",children:"https://arxiv.org/pdf/2511.07423"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," Synera is a device-cloud synergistic LLM serving system that uses an efficient SLM-LLM synergistic mechanism with communication-efficient selective offloading, stall-free parallel inference, and scalable cloud batching. The system achieves 1.20-5.47\xd7 better generation quality compared to baselines while maintaining similar latency, and reduces cloud serving costs by 8.2-16.5% compared to existing cloud serving approaches."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251112] Parallel Sampling via Autospeculation"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [diffusion inference], [speculative rejection sampling, autospeculation, parallel sampling]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Nima Anari, Carlo Baronio, CJ Chen, Alireza Haqi, Frederic Koehler, Anqi Li, Thuy-Duong Vuong"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Stanford University, University of Arizona, University of Chicago, UC Berkeley"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07869",children:"https://arxiv.org/pdf/2511.07869"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces speculative rejection sampling, a novel technique that uses autospeculation to accelerate sampling from autoregressive and diffusion models. By building speculative distributions from the same oracle that defines the target distribution and making sequence-level speculations, the method achieves parallel sampling. This reduces expected sampling time from O(n) to O(n^{1/2}), improving previous bounds and providing the first parallel speedup for diffusion models in high-accuracy regimes."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251112] BIPPO: Budget-Aware Independent PPO for Energy-Efficient Federated Learning Services"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [federated learning, reinforcement learning, proximal policy optimization, client selection, energy efficiency, multi-agent RL]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Anna Lackinger, Andrea Morichetta, Pantelis A. Frangoudis, Schahram Dustdar"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," TU Wien"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08142",children:"https://arxiv.org/pdf/2511.08142"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes BIPPO, a budget-aware independent proximal policy optimization method for energy-efficient client selection in federated learning. This multi-agent reinforcement learning approach improves performance while consuming minimal budget in resource-constrained IoT environments. Experimental results show BIPPO achieves higher accuracy than traditional methods while maintaining scalability and sustainability."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251112] UniFormer: Unified and Efficient Transformer for Reasoning Across General and Custom Computing"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [transformer architecture, parallelism, compute-storage fusion, FPGA optimization, GPU acceleration]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Zhuoheng Ran, Chong Wu, Renjie Xu, Maolin Che, Hong Yan"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," City University of Hong Kong, Guizhou University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08135",children:"https://arxiv.org/pdf/2511.08135"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces UniFormer, a unified Transformer architecture designed for both general-purpose and custom computing platforms. It achieves higher parallelism and compute-storage fusion to optimize performance across different hardware. The method demonstrates state-of-the-art accuracy and latency on GPUs while maintaining strong adaptability on FPGAs."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251112] Intelligence per Watt: Measuring Intelligence Efficiency of Local AI"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [intelligence per watt, local inference, power efficiency, model-accelerator pairs, empirical benchmarking]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Jon Saad-Falcon, Avanika Narayan, Hakki Orhun Akengin, J. Wes Griffin, Herumb Shandilya, Adrian Gamarra Lafuente, Medhya Goel, Rebecca Joseph, Shlok Natarajan, Etash Kumar Guha, Shang Zhu, Ben Athiwaratkun, John Hennessy, Azalia Mirhoseini, Christopher R\xe9"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Stanford University, Together AI"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07885",children:"https://arxiv.org/pdf/2511.07885"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes Intelligence Per Watt (IPW) as a metric to evaluate the efficiency of local AI inference across model-accelerator pairs. Through large-scale empirical analysis of local language models and hardware accelerators, the study demonstrates that local inference can accurately handle most single-turn queries and meaningfully redistribute demand from centralized cloud infrastructure. The findings show significant improvements in IPW over time and reveal optimization opportunities for local accelerators."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251112] Generic Algorithm for Universal TDM Communication Over Inter Satellite Links"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [TDM communication, federated learning, inter satellite links, time division multiplexing, peer data exchange, satellite constellations]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Miroslav Popovic, Marko Popovic, Pavle Vasiljevic, Ilija Basicevic"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Novi Sad, RT-RK Institute for Computer Based Systems"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08034",children:"https://arxiv.org/pdf/2511.08034"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents a new generic algorithm for universal TDM communication that extends beyond pairwise node communication, allowing nodes to communicate with multiple peers simultaneously. The algorithm was developed within a federated learning framework and specifically addresses communication needs for satellite constellations with multiple antennas. The main advantage is enabling real-world TDM communications over inter satellite links for applications like orbit determination and time synchronization."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251112] ProbSelect: Stochastic Client Selection for GPU-Accelerated Compute Devices in the 3D Continuum"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [probabilistic forecasting, analytical modeling, GPU-accelerated training, federated learning, client selection]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Andrija Stanisic, Stefan Nastic"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," TU Wien"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08147",children:"https://arxiv.org/pdf/2511.08147"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces ProbSelect, a novel client selection approach for federated learning that uses analytical modeling and probabilistic forecasting to select GPU-accelerated devices in the 3D compute continuum. The method operates without requiring historical data or continuous monitoring and models client selection within user-defined SLOs. Evaluation shows ProbSelect improves SLO compliance by 13.77% on average while reducing computational waste by 72.5% compared to baseline approaches."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251112] ACGraph: An Efficient Asynchronous Out-of-Core Graph Processing Framework"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [graph processing systems], [asynchronous execution, out-of-core processing, block-centric scheduling, hybrid storage format, SSD optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Dechuang Chen, Sibo Wang, Qintian Guo"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," The Chinese University of Hong Kong, The Hong Kong University of Science and Technology"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07886",children:"https://arxiv.org/pdf/2511.07886"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," ACGraph introduces an asynchronous out-of-core graph processing framework that uses dynamic block-centric scheduling and pipelined I/O-computation execution to overcome limitations of synchronous systems. It employs an online asynchronous worklist and hybrid storage format to minimize redundant disk accesses and optimize memory usage. Experimental results show ACGraph significantly outperforms state-of-the-art out-of-core graph processing systems in both runtime and I/O efficiency."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251112] Forgetting Alternation and Blossoms: A New Framework for Fast Matching Augmentation and Its Applications to Sequential/Distributed/Streaming Computation"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [ai], [graph algorithms], [alternating base trees, maximum cardinality matching, shortest alternating paths, matching augmentation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Taisuke Izumi, Naoki Kitamura, Yutaro Yamaguchi"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Osaka University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08210",children:"https://arxiv.org/pdf/2511.08210"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"}),' The paper proposes a new framework for maximum matching that simplifies the complex structure of shortest alternating paths by "forgetting alternation" and using alternating base trees. This approach yields a more implementable algorithm that is easier to verify than the classical Micali-Vazirani algorithm. The framework also enables improved deterministic approximation algorithms for distributed and streaming settings with substantially better time bounds.']}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251112] Foam Segmentation in Wastewater Treatment Plants: A Federated Learning Approach with Segment Anything Model 2"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal training], [Federated Learning, Segment Anything Model 2, Image Segmentation, Flower framework, Fog computing]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Mehmet Batuhan Duman, Alejandro Carnero, Cristian Mart\xedn, Daniel Garrido, Manuel D\xedaz"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," ITIS Software, University of Malaga"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08130",children:"https://arxiv.org/pdf/2511.08130"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a framework combining Federated Learning with Segment Anything Model 2 (SAM2) for foam segmentation in wastewater treatment plants. The approach enables privacy-preserving collaborative training across multiple plants without sharing sensitive data while leveraging SAM2's pre-trained weights for improved performance. The research demonstrates that integrating large-scale foundational models with federated learning provides a practical solution for industrial applications with distributed and sensitive data."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251112] Gathering in Vertex- and Edge-Transitive Graphs without Multiplicity Detection under Round Robin"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed robotics algorithms], [round-robin scheduling, vertex-transitive graphs, edge-transitive graphs, OBLOT model, multiplicity detection]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Serafino Cicerone, Alessia Di Fonso, Gabriele Di Stefano, Alfredo Navarra"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Universit\xe0 degli Studi dell'Aquila, Universit\xe0 degli Studi di Perugia"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08222",children:"https://arxiv.org/pdf/2511.08222"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes distributed algorithms for solving the Gathering problem in swarm robotics on vertex- and edge-transitive graphs under hostile conditions where robots cannot detect multiplicities. The algorithms specifically target infinite grids and hypercubes, exploiting their topological properties to achieve time-optimal performance. The authors conclude that no general algorithm likely exists for all solvable cases due to the heavy reliance on specific graph properties."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251112] LOw-cost yet High-Performant Sparse Matrix-Matrix Multiplication on Arm SME Architectures"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [GPU kernels], [SME, NEON, CSR, BCSR, two-level parallelization, performance modeling]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Kelun Lei, Hailong Yang, Kaige Zhang, Kejie Ma, Yiqing Wang, Xin You, Yufan Xu, Enrique S. Quintana-Orti, Zhongzhi Luan, Yi Liu, Depei Qian"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Beihang University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08158",children:"https://arxiv.org/pdf/2511.08158"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes LOOPS, a hybrid execution framework that combines row-wise CSR and vector-wise BCSR layouts to cooperatively utilize both NEON vector instructions and Arm SME matrix extensions for sparse matrix multiplication. Experimental results show LOOPS achieves significant speedups over CPU baselines and GPU methods while delivering better energy efficiency on Apple M4Pro CPUs compared to NVIDIA A100 GPU implementations."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251112] Priority Matters: Optimising Kubernetes Clusters Usage with Constraint-Based Pod Packing"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [constraint programming, pod packing, OR-Tools, Kubernetes scheduler, resource optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Henrik Daniel Christensen, Saverio Giallorenzo, Jacopo Mauro"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Southern Denmark, University of Bologna"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08373",children:"https://arxiv.org/pdf/2511.08373"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes using constraint programming with OR-Tools to optimize Kubernetes pod scheduling, implemented as a fallback plugin when the default scheduler fails. The method improves pod placement by finding optimal allocations that satisfy priorities and resource requirements. Experimental results show the approach places more high-priority pods in over 44% of problematic scenarios within 1 second, and over 73% within 10 seconds, while certifying default scheduler optimality in 19% of cases."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 31'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] A Negotiation-Based Multi-Agent Reinforcement Learning Approach for Dynamic Scheduling of Reconfigurable Manufacturing Systems ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07707",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] Think Before You Retrieve: Learning Test-Time Adaptive Search with Small Language Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07581",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] Diffusion Guided Adversarial State Perturbations in Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07701",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] Partial Action Replacement: Tackling Distribution Shift in Offline MARL ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07629",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] The Polite Liar: Epistemic Pathology in Language Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07477",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] RELEAP: Reinforcement-Enhanced Label-Efficient Active Phenotyping for Electronic Health Records ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07473",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] MURPHY: Multi-Turn GRPO for Self Correcting Code Generation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07833",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] From Exploration to Exploitation: A Two-Stage Entropy RLVR Approach for Noise-Tolerant MLLM Training ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07738",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] Beyond Correctness: Confidence-Aware Reward Modeling for Enhancing Large Language Model Reasoning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07483",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] Intelligent Optimization of Multi-Parameter Micromixers Using a Scientific Machine Learning Framework ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07702",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] ZeroSim: Zero-Shot Analog Circuit Evaluation with Unified Transformer Embeddings ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07658",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] Statistically Assuring Safety of Control Systems using Ensembles of Safety Filters and Conformal Prediction ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07899",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] Test-driven Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07904",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] Feedback Descent: Open-Ended Text Optimization via Pairwise Comparison ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07919",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] SERL: Self-Examining Reinforcement Learning on Open-Domain ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07922",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] SpeechJudge: Towards Human-Level Judgment for Speech Naturalness ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07931",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] Thinker: Training LLMs in Hierarchical Thinking for Deep Search via Multi-Turn Interaction ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07943",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] Knowledge-Augmented Long-CoT Generation for Complex Biomolecular Reasoning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08024",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] Dynamic Sparsity: Challenging Common Sparsity Assumptions for Learning World Models in Robotic Reinforcement Learning Benchmarks ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08086",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] An Efficient Training Pipeline for Reasoning Graphical User Interface Agents ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08172",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] Beyond Distributions: Geometric Action Control for Continuous Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08234",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] PrefPoE: Advantage-Guided Preference Fusion for Learning Where to Explore ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08241",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] Where and What Matters: Sensitivity-Aware Task Vectors for Many-Shot Multimodal In-Context Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08246",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] AgentPRM: Process Reward Models for LLM Agents via Step-Wise Promise and Progress ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08325",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] LPPG-RL: Lexicographically Projected Policy Gradient Reinforcement Learning with Subproblem Exploration ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08339",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] ARAC: Adaptive Regularized Multi-Agent Soft Actor-Critic in Graph-Structured Adversarial Games ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08412",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] Understanding Electro-communication and Electro-sensing in Weakly Electric Fish using Multi-Agent Deep Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08436",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] The Path Not Taken: RLVR Provably Learns Off the Principals ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08567",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] DeepProofLog: Efficient Proving in Deep Stochastic Logic Programs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08581",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] Shocks Under Control: Taming Transonic Compressible Flow over an RAE2822 Airfoil with Deep Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07564",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] Distributionally Robust Online Markov Game with Linear Function Approximation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07831",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 16'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] TurboSAT: Gradient-Guided Boolean Satisfiability Accelerated on GPU-CPU Hybrid System ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07737",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] A Negotiation-Based Multi-Agent Reinforcement Learning Approach for Dynamic Scheduling of Reconfigurable Manufacturing Systems ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07707",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] Operational machine learning for remote spectroscopic detection of CH",(0,s.jsxs)(e.span,{className:"katex",children:[(0,s.jsx)(e.span,{className:"katex-mathml",children:(0,s.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,s.jsxs)(e.semantics,{children:[(0,s.jsx)(e.mrow,{children:(0,s.jsxs)(e.msub,{children:[(0,s.jsx)(e.mrow,{}),(0,s.jsx)(e.mn,{children:"4"})]})}),(0,s.jsx)(e.annotation,{encoding:"application/x-tex",children:"_{4}"})]})})}),(0,s.jsx)(e.span,{className:"katex-html","aria-hidden":"true",children:(0,s.jsxs)(e.span,{className:"base",children:[(0,s.jsx)(e.span,{className:"strut",style:{height:"0.4511em",verticalAlign:"-0.15em"}}),(0,s.jsxs)(e.span,{className:"mord",children:[(0,s.jsx)(e.span,{}),(0,s.jsx)(e.span,{className:"msupsub",children:(0,s.jsxs)(e.span,{className:"vlist-t vlist-t2",children:[(0,s.jsxs)(e.span,{className:"vlist-r",children:[(0,s.jsx)(e.span,{className:"vlist",style:{height:"0.3011em"},children:(0,s.jsxs)(e.span,{style:{top:"-2.55em",marginRight:"0.05em"},children:[(0,s.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,s.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,s.jsx)(e.span,{className:"mord mtight",children:(0,s.jsx)(e.span,{className:"mord mtight",children:"4"})})})]})}),(0,s.jsx)(e.span,{className:"vlist-s",children:"\u200b"})]}),(0,s.jsx)(e.span,{className:"vlist-r",children:(0,s.jsx)(e.span,{className:"vlist",style:{height:"0.15em"},children:(0,s.jsx)(e.span,{})})})]})})]})]})})]})," point sources ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07719",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] FractalCloud: A Fractal-Inspired Architecture for Efficient Large-Scale Point Cloud Processing ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07665",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] Hyperellipsoid Density Sampling: Exploitative Sequences to Accelerate High-Dimensional Optimization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07836",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] A Self-Improving Architecture for Dynamic Safety in Large Language Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07645",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] Streaming Tensor Program: A streaming abstraction for dynamic parallelism ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07776",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] Sharp Eyes and Memory for VideoLLMs: Information-Aware Visual Token Pruning for Efficient and Reliable VideoLLM Reasoning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08003",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] Improving Long-Range Interactions in Graph Neural Simulators via Hamiltonian Dynamics ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08185",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] Dual-Kernel Graph Community Contrastive Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08287",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] LPPG-RL: Lexicographically Projected Policy Gradient Reinforcement Learning with Subproblem Exploration ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08339",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] NeuCLIP: Efficient Large-Scale CLIP Training with Neural Normalizer Optimization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08417",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] Benchmarking Simulacra AI's Quantum Accurate Synthetic Data Generation for Chemical Sciences ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.07433",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] Emulating Radiative Transfer in Astrophysical Environments ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08219",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] Generative AI Meets 6G and Beyond: Diffusion Models for Semantic Communications ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08416",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251112] Galactification: painting galaxies onto dark matter only simulations using a transformer-based model ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08438",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-11-13",children:"2025-11-13"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"cs.DC total: 13"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251113] FedPM: Federated Learning Using Second-order Optimization with Preconditioned Mixing of Local Parameters"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [federated learning, second-order optimization, preconditioned mixing, local parameter updates, convergence analysis]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Hiro Ishii, Kenta Niwa, Hiroshi Sawada, Akinori Fujino, Noboru Harada, Rio Yokota"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Institute of Science Tokyo, NTT Communication Science Laboratories"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09100",children:"https://arxiv.org/pdf/2511.09100"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," FedPM introduces a novel federated learning method that uses second-order optimization with preconditioned mixing of local parameters to address drift issues in local preconditioners. The approach decomposes ideal second-order updates into server-side parameter mixing and client-side local updates. Experimental results show significant improvements in test accuracy compared to conventional methods with simple mixing."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251113] Attack-Centric by Design: A Program-Structure Taxonomy of Smart Contract Vulnerabilities"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [blockchain security], [vulnerability taxonomy, static analysis, dynamic analysis, learning-based tools, root-cause analysis, program structure]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Parsa Hedayatnia, Tina Tavakkoli, Hadi Amini, Mohammad Allahbakhsh, Haleh Amintoosi"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Ferdowsi University of Mashhad"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09051",children:"https://arxiv.org/pdf/2511.09051"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces an attack-centric program-structure taxonomy that organizes smart contract vulnerabilities into eight root-cause families. The taxonomy provides a unified framework for vulnerability detection, audit reproducibility, and security education. It reveals coverage gaps in existing datasets and enables more interpretable security analysis through structural classification."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251113] Minimize Your Critical Path with Combine-and-Exchange Locks"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [concurrency and synchronization], [Combine-and-Exchange Scheduling, coroutines, userspace scheduling, cooperative multitasking, critical sections]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Simon K\xf6nig, Lukas Epple, Christian Becker"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Stuttgart"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09194",children:"https://arxiv.org/pdf/2511.09194"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces Combine-and-Exchange Scheduling (CES), a novel scheduling approach for userspace tasks like coroutines that keeps contended critical sections on the same thread while distributing parallelizable work across other threads. The method addresses limitations in existing userspace synchronization primitives that introduce unnecessary delays. The approach achieves 3-fold performance improvements in application benchmarks and 8-fold improvements in microbenchmarks."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251113] An MLIR pipeline for offloading Fortran to FPGAs via OpenMP"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [compiler systems], [MLIR, OpenMP, FPGA offloading, High-Level Synthesis, Flang]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Gabriel Rodriguez-Canal, David Katz, Nick Brown"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," EPCC, The University of Edinburgh"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08713",children:"https://arxiv.org/pdf/2511.08713"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents an MLIR-based compilation pipeline that enables Fortran code offloading to FPGAs using OpenMP target directives. The approach combines MLIR's OpenMP dialect with a High-Level Synthesis dialect to create a portable FPGA compilation flow. The work demonstrates that building upon existing MLIR components significantly reduces development effort and provides a flexible path for directive-based FPGA acceleration within the MLIR ecosystem."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251113] CheetahGIS: Architecting a Scalable and Efficient Streaming Spatial Query Processing System"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [spatial data processing], [Apache Flink Stateful Functions, grid-based indexing, load balancing, streaming spatial queries]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Jiaping Cao, Ting Sun, Man Lung Yiu, Xiao Yan, Bo Tang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Hong Kong Polytechnic University, Southern University of Science and Technology, Wuhan University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09262",children:"https://arxiv.org/pdf/2511.09262"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," CheetahGIS is a scalable streaming spatial query processing system built on Apache Flink Stateful Functions with optimizations including lightweight grid indexing and load balancing. The system efficiently handles massive moving objects and real-time spatial queries through its modular architecture. Experimental results demonstrate its excellent scalability and performance for various streaming spatial query types."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251113] Evaluating HPC-Style CPU Performance and Cost in Virtualized Cloud Infrastructures"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [cloud computing performance evaluation], [SPEC ACCEL, OpenMP workloads, virtualized cloud infrastructure, CPU performance benchmarking]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Jay Tharwani, Shobhit Aggarwal, Arnab A Purkayastha"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of North Carolina at Charlotte, The Citadel, Western New England University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08948",children:"https://arxiv.org/pdf/2511.08948"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper evaluates HPC-style CPU performance and cost across four major cloud providers using SPEC ACCEL OpenMP workloads on Intel, AMD, and ARM instances. The study found that AWS delivers the fastest performance but at a premium cost, while OCI emerges as the most economical option despite slower runtimes. The research demonstrates that instance selection and provider choice significantly impact both runtime and pricing, suggesting workload priorities should guide deployment decisions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251113] No Cords Attached: Coordination-Free Concurrent Lock-Free Queues"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [concurrent data structures], [Cyclic Memory Protection, lock-free queues, coordination-free, linearizability, bounded reclamation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yusuf Motiwala"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," mesibo, PatANN"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09410",children:"https://arxiv.org/pdf/2511.09410"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces Cyclic Memory Protection (CMP), a coordination-free concurrent lock-free queue that uses bounded protection windows to provide practical reclamation guarantees. The method preserves strict FIFO ordering, unbounded capacity, and lock-free progress while significantly outperforming state-of-the-art queues by up to 4x under high contention. The work demonstrates that highly concurrent queues can maintain fundamental simplicity without compromising queue semantics."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251113] Flex-MIG: Enabling Distributed Execution on MIG"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [GPU sharing, MIG, fragmentation reduction, host-shared-memory collectives, one-to-many allocation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Myungsu Kim, Ikjun Yeom, Younghoon Kim"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," SungKyunKwan University, Ajou University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09143",children:"https://arxiv.org/pdf/2511.09143"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," Flex-MIG is a software framework that replaces the conventional one-to-one allocation model with a one-to-many model for NVIDIA MIG GPUs, enabling distributed execution across MIG instances using host-shared-memory collectives. This approach eliminates drain-required reconfigurations and reduces fragmentation without hardware modifications. The system improves cluster efficiency by up to 17% in makespan across diverse workload traces."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251113] Distribution and Management of Datacenter Load Decoupling"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [datacenter energy management], [load adaptation, energy resources, datacenter-grid cooperation, power capacity decoupling]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Liuzixuan Lin, Andrew A. Chien"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Chicago, Argonne National Laboratory"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08936",children:"https://arxiv.org/pdf/2511.08936"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes datacenter load decoupling using energy resources to separate power capacity from grid load, enabling datacenter flexibility without compromising capacity. The study shows optimized distribution achieves 98% grid carbon reduction with 70% decoupling needs, and DC-grid cooperation provides 1.4x greater carbon reduction. Economic analysis suggests decoupling is viable on average but may require grid intervention due to site variations."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251113] Experiences Building Enterprise-Level Privacy-Preserving Federated Learning to Power AI for Science"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [federated learning, differential privacy, secure aggregation, confidential computing, hybrid cloud-HPC architecture]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Zilinghan Li, Aditya Sinha, Yijiang Li, Kyle Chard, Kibaek Kim, Ravi Madduri"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Argonne National Laboratory, University of Illinois at Urbana-Champaign, University of Chicago"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08998",children:"https://arxiv.org/pdf/2511.08998"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper presents the Advanced Privacy-Preserving Federated Learning (APPFL) framework designed to enable collaborative AI model training without centralized data sharing. It focuses on building an enterprise-level system that bridges local prototyping with distributed deployment across heterogeneous infrastructures while ensuring privacy through techniques like differential privacy and secure aggregation. The framework aims to provide scalable, reliable, and privacy-preserving AI for scientific applications by addressing challenges in distributed execution across diverse computing environments."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251113] Formal Verification of a Generic Algorithm for TDM Communication Over Inter Satellite Links"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [fault-tolerance], [formal verification, CSP process algebra, model checking, deadlock freeness, successful termination]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Miroslav Popovic, Marko Popovic, Pavle Vasiljevic, Miodrag Djukic"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Novi Sad, RT-RK Institute for Computer Based Systems"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09485",children:"https://arxiv.org/pdf/2511.09485"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper formally verifies a TDM communication algorithm for satellite constellations using CSP process algebra and the PAT model checker. The verification process involves constructing a CSP model from Python code and proving deadlock freeness and successful termination properties. The main conclusion is that the algorithm's correctness was automatically verified, ensuring its reliability for safety-critical edge systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251113] SPADA: A Spatial Dataflow Architecture Programming Language"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [spatial dataflow architectures, programming language, dataflow semantics, network-on-chip, compiler design, stencil DSL]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Lukas Gianinazzi, Tal Ben-Nun, Torsten Hoefler"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," ETH Zurich, Lawrence Livermore National Laboratory"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09447",children:"https://arxiv.org/pdf/2511.09447"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," SPADA is a programming language that provides precise control over data placement, dataflow patterns, and asynchronous operations for spatial dataflow architectures while abstracting low-level details. It enables developers to express complex parallel patterns in 6-8x less code than existing approaches while achieving near-ideal weak scaling. The language serves as both a high-level programming interface and intermediate representation for domain-specific languages, advancing both theoretical foundations and practical usability of spatial dataflow architectures."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251113] LLM Inference Beyond a Single Node: From Bottlenecks to Mitigations with Fast All-Reduce Communication"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [all-reduce, tensor parallelism, NVSHMEM, model parallelism, distributed inference]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Prajwal Singhania, Siddharth Singh, Lannie Dalton Hough, Akarsh Srivastava, Harshitha Menon, Charles Fredrick Jekel, Abhinav Bhatele"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Maryland, Lawrence Livermore National Laboratory"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09557",children:"https://arxiv.org/pdf/2511.09557"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces NVRAR, a hierarchical all-reduce algorithm using NVSHMEM to optimize communication bottlenecks in multi-node LLM inference. The method achieves up to 3.6x lower latency than NCCL and reduces end-to-end batch latency by 1.72x for large models like Llama 3.1 405B. The research demonstrates that optimized collective communication significantly improves performance in distributed inference workloads."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 21'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251113] TIGER-MARL: Enhancing Multi-Agent Reinforcement Learning with Temporal Information through Graph-based Embeddings and Representations ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08832",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251113] Planning in Branch-and-Bound: Model-Based Reinforcement Learning for Exact Combinatorial Optimization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09219",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251113] A Distributed Training Architecture For Combinatorial Optimization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09261",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251113] Interpretable by Design: Query-Specific Neural Modules for Explainable Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08749",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251113] Efficient Reasoning via Reward Model ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09158",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251113] Iterated Population Based Training with Task-Agnostic Restarts ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09190",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251113] Structured Uncertainty guided Clarification for LLM Agents ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08798",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251113] Achieving Equilibrium under Utility Heterogeneity: An Agent-Attention Framework for Multi-Agent Multi-Objective Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08926",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251113] Potent but Stealthy: Rethink Profile Pollution against Sequential Recommendation via Bi-level Constrained Reinforcement Paradigm ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09392",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251113] Diffusion Policies with Value-Conditional Optimization for Offline Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08922",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251113] Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09109",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251113] Advancing Autonomous Emergency Response Systems: A Generative AI Perspective ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09044",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251113] Towards a Generalisable Cyber Defence Agent for Real-World Computer Networks ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09114",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251113] History-Aware Reasoning for GUI Agents ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09127",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251113] UCO: A Multi-Turn Interactive Reinforcement Learning Method for Adaptive Teaching with Large Language Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08873",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251113] AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation and Historical Revisiting ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09478",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251113] Quasi-Newton Compatible Actor-Critic for Deterministic Policies ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09509",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251113] WMPO: World Model-based Policy Optimization for Vision-Language-Action Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09515",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251113] Optimal Control of the Future via Prospective Foraging ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08717",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251113] Practical considerations when designing an online learning algorithm for an app-based mHealth intervention ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08719",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251113] DRL-Based Beam Positioning for LEO Satellite Constellations with Weighted Least Squares ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08852",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 9'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251113] Data reuse enables cost-efficient randomized trials of medical AI models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08986",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251113] Leveraging Large Language Models for Use Case Model Generation from Software Requirements ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09231",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251113] CoCo-MILP: Inter-Variable Contrastive and Intra-Constraint Competitive MILP Solution Prediction ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09209",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251113] FSampler: Training Free Acceleration of Diffusion Sampling via Epsilon Extrapolation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09180",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251113] Enabling Agents to Communicate Entirely in Latent Space ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09149",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251113] Tele-LLM-Hub: Building Context-Aware Multi-Agent LLM Systems for Telecom Networks ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09087",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251113] Enhancing Explainability in Solar Energetic Particle Event Prediction: A Global Feature Mapping Approach ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09475",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251113] Fluence Map Prediction with Deep Learning: A Transformer-based Approach ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08645",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251113] Bio AI Agent: A Multi-Agent Artificial Intelligence System for Autonomous CAR-T Cell Therapy Development with Integrated Target Discovery, Toxicity Prediction, and Rational Molecular Design ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.08649",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-11-14",children:"2025-11-14"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"cs.DC total: 18"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251114] Dynamic Edge Server Selection in Time-Varying Environments: A Reliability-Aware Predictive Approach"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [edge computing], [latency prediction, reliability-aware selection, hysteresis-based handover, passive measurements, exponentially modulated rational delay model]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Jaime Sebastian Burbano, Arnova Abdullah, Eldiyar Zhantileuov, Mohan Liyanage, Rolf Schuster"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Applied Sciences and Arts, Dortmund"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.10146",children:"https://arxiv.org/pdf/2511.10146"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes MO-HAN, a lightweight server selection method that combines latency prediction with adaptive reliability and hysteresis-based handover to dynamically select edge servers. Results show the approach reduces mean and tail latencies while cutting handovers by nearly 50% compared to opportunistic selection, making it practical for resource-constrained embedded devices."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251114] MoFa: A Unified Performance Modeling Framework for LLM Pretraining"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [performance modeling, hybrid parallelization, fault tolerance, checkpoint recovery, distributed pretraining]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Lu Zhao, Rong Shi, Shaoqing Zhang, Shangchao Su, Ziqing Yin, Zhiyan Cui, Hongfeng Sun, Baoguo He, Yueqiang Chen, Liang Dong, Xiyuan Li, Lingbin Wang, Lijun Ma, Qiang Huang, Ting Liu, Chong Wang, Can Wei"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," AIH Training Team"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09837",children:"https://arxiv.org/pdf/2511.09837"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," MoFa is a unified performance modeling framework for LLM pretraining that incorporates multi-dimensional optimization features and fault tolerance mechanisms. It uses an enhanced cost model and historical cluster reliability data to accurately predict pretraining performance. The framework provides systematic guidance for optimizing distributed training configurations and reveals key factors influencing pretraining efficiency."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251114] TawPipe: Topology-Aware Weight Pipeline Parallelism for Accelerating Long-Context Large Models Training"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [pipeline parallelism, weight passing, topology-aware communication, hierarchical bandwidth, collective operations, peer-to-peer transfers]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Houming Wu, Ling Chen"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Zhejiang University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09741",children:"https://arxiv.org/pdf/2511.09741"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," TawPipe proposes a topology-aware weight pipeline parallelism method that optimizes communication by grouping devices based on cluster topology and transmitting weights instead of activations. The approach reduces cross-node traffic through intra-node collective operations and overlaps communication with computation. Experiments show TawPipe achieves superior throughput and scalability compared to existing methods for long-context large model training."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251114] Ksurf-Drone: Attention Kalman Filter for Contextual Bandit Optimization in Cloud Resource Allocation"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [Kalman Filter, Contextual Bandits, Gaussian Process Regression, Long Short Term Memory, Transformer Attention]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Michael Dang'ana, Yuqiu Zhang, Hans-Arno Jacobsen"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Toronto"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09766",children:"https://arxiv.org/pdf/2511.09766"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes Ksurf-Drone, which combines attention Kalman filters with contextual bandit optimization for cloud resource allocation. The method addresses high variability in cloud workloads by using variance-minimizing estimation techniques. Results show significant improvements in latency variance reduction, resource usage efficiency, and cost savings on Kubernetes benchmarks."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251114] Lit Silicon: A Case Where Thermal Imbalance Couples Concurrent Execution in Multiple GPUs"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [concurrent computation communication, thermal imbalance, power management, node-level power capping, performance modeling]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Marco Kurzynski, Shaizeen Aga, Di Wu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Central Florida, Advanced Micro Devices, Inc."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09861",children:"https://arxiv.org/pdf/2511.09861"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"}),' This paper identifies the "Lit Silicon" effect where thermal imbalance in multi-GPU systems causes performance variation during LLM training through concurrent computation communication. The authors propose detection methods and power management solutions including GPU power capping and CPU power sloshing. Their approach achieves up to 6% performance improvement and can save significant costs in datacenters with minimal implementation overhead.']}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251114] A Poly-Log Approximation for Transaction Scheduling in Fog-Cloud Computing and Beyond"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed systems], [transaction scheduling, approximation algorithms, doubling dimension graphs, distributed algorithms, fog-cloud computing]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Ramesh Adhikari, Costas Busch, Pavan Poudel"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Augusta University, University of Houston-Clear Lake"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09776",children:"https://arxiv.org/pdf/2511.09776"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper presents poly-logarithmic approximation algorithms for transaction scheduling in fog-cloud computing networks, where both transactions and shared objects can move to meet at optimal locations. The algorithms achieve O(log n \xb7 log D) approximation for single-object transactions and O(k \xb7 log n \xb7 log D) for multi-object transactions in constant doubling dimension networks. The authors also develop fully distributed versions that operate without global transaction knowledge."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251114] Selection of Supervised Learning-based Sparse Matrix Reordering Algorithms"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [supervised learning, sparse matrix reordering, automatic tuning, matrix bandwidth minimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Tao Tang, Youfu Jiang, Yingbo Cui, Jianbin Fang, Peng Zhang, Lin Peng, Chun Huang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," National University of Defense Technology"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.10180",children:"https://arxiv.org/pdf/2511.10180"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a supervised learning model that automatically selects optimal sparse matrix reordering algorithms by learning the relationship between matrix characteristics and algorithm performance. Experiments on the Florida sparse matrix dataset show the model can accurately predict the best reordering algorithm, achieving a 55.37% reduction in solution time compared to using only the AMD algorithm, with an average speedup ratio of 1.45."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251114] SMoFi: Step-wise Momentum Fusion for Split Federated Learning on Heterogeneous Data"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [split federated learning, momentum fusion, staleness-aware alignment, gradient divergence control]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Mingkun Yang, Ran Zhu, Qing Wang, Jie Yang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Delft University of Technology"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09828",children:"https://arxiv.org/pdf/2511.09828"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces SMoFi, a framework that synchronizes momentum buffers across server-side optimizers in split federated learning to counteract gradient divergence from data heterogeneity. It uses a staleness-aware alignment mechanism to constrain gradient updates during training. Experimental results show SMoFi significantly improves global model accuracy (up to 7.1%) and convergence speed (up to 10.25\xd7), particularly with more clients and deeper models."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251114] Revisit to the Bai-Galbraith signature scheme"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [cryptography], [lattice-based cryptography, Fiat-Shamir paradigm, Learning with Errors, signature scheme]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Banhirup Sengupta, Peenal Gupta, Souvik Sengupta"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," PinakashieldTech O\xdc, Tata Institute Of Fundamental Research, IONOS SE"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09582",children:"https://arxiv.org/pdf/2511.09582"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper revisits the Bai-Galbraith lattice-based signature scheme which uses the Fiat-Shamir paradigm and Learning with Errors (LWE) to create digital signatures. The scheme differs from Dilithium by avoiding public key compression and focuses on reducing signature sizes. The authors present this as an alternative approach to practical lattice-based signature schemes."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251114] Optimizing CPU Cache Utilization in Cloud VMs with Accurate Cache Abstraction"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [cloud computing], [cache abstraction, eviction sets, LLC contention-aware scheduling, virtual color-aware page management]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Mani Tofigh, Edward Guo, Weiwei Jia, Xiaoning Ding, Jianchen Shan"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Hofstra University, Columbia University, University of Rhode Island, New Jersey Institute of Technology"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09956",children:"https://arxiv.org/pdf/2511.09956"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes CacheX, a solution that probes accurate cache abstraction within cloud VMs using eviction sets without requiring hardware or hypervisor support. The system implements LLC contention-aware task scheduling and virtual color-aware page cache management to optimize cache utilization. Evaluation shows CacheX effectively improves cache performance for various workloads in public cloud VMs."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251114] Pk-IOTA: Blockchain empowered Programmable Data Plane to secure OPC UA communications in Industry 4.0"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [industrial control system security], [programmable data plane, IOTA Tangle, certificate validation, OPC UA security, blockchain]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Rinieri Lorenzo, Gori Giacomo, Melis Andrea, Girau Roberto, Prandini Marco, Callegati Franco"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Bologna"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.10248",children:"https://arxiv.org/pdf/2511.10248"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," Pk-IOTA integrates programmable data plane switches for in-network certificate validation and uses the IOTA Tangle blockchain for decentralized certificate distribution to secure OPC UA communications in Industry 4.0. The system was evaluated on a physical testbed and demonstrated minimal overhead while providing a scalable, tamper-proof mechanism for automated certificate management in industrial control systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251114] Workload Schedulers -- Genesis, Algorithms and Differences"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [scheduling algorithms], [list scheduling, largest processing time first, highest level first, round robin, weighted round robin, largest remaining processing time on fastest machine, fixed-priority pre-emptive scheduling]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Leszek Sliwko, Vladimir Getov"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Westminster"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.10258",children:"https://arxiv.org/pdf/2511.10258"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents a novel categorization approach for workload schedulers across operating systems, cluster systems, and big data environments. It examines the evolution of scheduling algorithms from simple to complex implementations and identifies similarities in scheduling strategy design that apply to both local and distributed systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251114] On The Performance of Prefix-Sum Parallel Kalman Filters and Smoothers on GPUs"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [GPU kernels], [parallel scan algorithms, prefix-sum algorithms, Kalman filtering, smoothing, temporal parallelization, GPU implementation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Simo S\xe4rkk\xe4, \xc1ngel F. Garc\xeda-Fern\xe1ndez"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Aalto University, Universidad Polit\xe9cnica de Madrid"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.10363",children:"https://arxiv.org/pdf/2511.10363"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper evaluates parallel-in-time Kalman filters and smoothers using prefix-sum algorithms on GPUs. It compares different parallel scan algorithms through operation counts and runtime measurements on real GPU hardware. The experimental results demonstrate that temporal parallelization enables more efficient execution of Kalman filters and smoothers on GPU parallel architectures."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251114] Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [symbolic tensor graphs, parallelization strategies, execution traces, distributed workload modeling]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Changhai Man, Joongun Park, Hanjiang Wu, Huan Xu, Srinivas Sridharan, Tushar Krishna"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Georgia Institute of Technology, Nvidia Inc."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.10480",children:"https://arxiv.org/pdf/2511.10480"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces STAGE, a framework that synthesizes high-fidelity execution traces to model distributed LLM workloads using symbolic tensor graphs. It enables systematic exploration of parallelization strategies and system configurations without requiring access to large-scale infrastructure. The framework demonstrates scalability by accurately modeling LLM workloads across over 32K GPUs while preserving tensor-level compute, memory, and communication accuracy."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251114] Massively Parallel Proof-Number Search for Impartial Games and Beyond"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [ai], [game solving], [Proof-Number Search, parallelization, Grundy numbers, game tree reduction]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Tom\xe1\u0161 \u010c\xed\u017eek, Martin Balko, Martin Schmid"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Charles University, EquiLibre Technologies, Inc."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.10339",children:"https://arxiv.org/pdf/2511.10339"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents a massively parallel version of Proof-Number Search that uses two parallelized levels and shared information to achieve efficient scaling on many CPU cores. The enhanced solver, incorporating Grundy numbers for game tree reduction, achieved a 332.9\xd7 speedup on 1024 cores and verified the Sprouts Conjecture for 42 new positions, nearly doubling known outcomes while generating proofs 1,000\xd7 more complex than previous state-of-the-art methods."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251114] FastGraph: Optimized GPU-Enabled Algorithms for Fast Graph Building and Message Passing"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [GPU kernels], [k-nearest neighbor, bin-partitioned approach, gradient-flow support, adaptive parameter tuning, CUDA]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Aarush Agarwal, Raymond He, Jan Kieseler, Matteo Cremonesi, Shah Rukh Qasim"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Carnegie Mellon University, Karlsruhe Institute of Technology, University of Zurich"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.10442",children:"https://arxiv.org/pdf/2511.10442"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," FastGraph introduces a GPU-optimized k-nearest neighbor algorithm using a bin-partitioned approach with gradient-flow support for efficient graph construction in low-dimensional spaces. The method achieves 20-40x speedup over state-of-the-art libraries like FAISS and SCANN with minimal memory overhead. These improvements significantly accelerate GNN workflows in applications such as particle physics and visual object tracking."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251114] dHPR: A Distributed Halpern Peaceman--Rachford Method for Non-smooth Distributed Optimization Problems"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [ai], [distributed optimization], [Halpern Peaceman-Rachford, symmetric Gauss-Seidel decomposition, proximal operators, non-smooth optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Zhangcheng Feng, Defeng Sun, Yancheng Yuan, Guojun Zhang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," The Hong Kong Polytechnic University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.10069",children:"https://arxiv.org/pdf/2511.10069"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces the distributed Halpern Peaceman-Rachford (dHPR) method for solving non-smooth distributed optimization problems. The method achieves O(1/k) iteration complexity for KKT residuals while maintaining parallelizability through symmetric Gauss-Seidel decomposition. Numerical experiments demonstrate superior performance on distributed LASSO, group LASSO, and L1-regularized logistic regression problems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251114] Unlocking Dynamic Inter-Client Spatial Dependencies: A Federated Spatio-Temporal Graph Learning Method for Traffic Flow Forecasting"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [federated learning, spatio-temporal graphs, dynamic dependencies, graph neural networks, client-server protocol, nonlinear computation decomposition, node embedding augmentation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Feng Wang, Tianxiang Chen, Shuyue Wei, Qian Chu, Yi Zhang, Yifan Sun, Zhiming Zheng"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Beihang University, Renmin University of China"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.10434",children:"https://arxiv.org/pdf/2511.10434"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes FedSTGD, a federated learning framework that models dynamic inter-client spatial dependencies for traffic flow forecasting using nonlinear computation decomposition and node embedding augmentation. The method coordinates learning through a client-server protocol that breaks down dependency learning into parallelizable subtasks. Experimental results show FedSTGD achieves superior performance approaching centralized baselines while maintaining data privacy."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 28'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] SEBA: Sample-Efficient Black-Box Attacks on Visual Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09681",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] Opinion: Towards Unified Expressive Policy Optimization for Robust Robot Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.10087",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] Multi-agent In-context Coordination via Decentralized Memory Retrieval ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.10030",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] DemoTuner: Efficient DBMS Knobs Tuning via LLM-Assisted Demonstration Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09998",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] Uncertainty-Guided Checkpoint Selection for Reinforcement Finetuning of Large Language Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09864",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] HierRouter: Coordinated Routing of Specialized Large Language Models via Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09873",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09737",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] Scaling Environments for LLM Agents in the Era of Learning from Interaction: A Survey ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09586",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] Tree-Based Stochastic Optimization for Solving Large-Scale Urban Network Security Games ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.10072",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] Optimistic Reinforcement Learning with Quantile Objectives ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09652",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] Baby Sophia: A Developmental Approach to Self-Exploration through Self-Touch and Hand Regard ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09727",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] Hail to the Thief: Exploring Attacks and Defenses in Decentralised GRPO ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09780",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] Beyond Monotonicity: Revisiting Factorization Principles in Multi-Agent Q-Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09792",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] ConstrainedSQL: Training LLMs for Text2SQL via Constrained Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09693",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] Improved Offline Reinforcement Learning via Quantum Metric Encoding ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.10187",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] Heuristic Transformer: Belief Augmented In-Context Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.10251",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] Beyond Single-Step Updates: Reinforcement Learning of Heuristics with Limited-Horizon Search ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.10264",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] Causal Model-Based Reinforcement Learning for Sample-Efficient IoT Channel Access ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.10291",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] MonkeyOCR v1.5 Technical Report: Unlocking Robust Document Parsing for Complex Patterns ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.10390",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] AgentEvolver: Towards Efficient Self-Evolving Agent System ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.10395",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] Explaining Decentralized Multi-Agent Reinforcement Learning Policies ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.10409",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] Reasoning About Intent for Ambiguous Requests ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.10453",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] Strategic Opponent Modeling with Graph Neural Networks, Deep Reinforcement Learning and Probabilistic Topic Modeling ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.10501",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] Towards Emotionally Intelligent and Responsible Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.10573",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] Instella: Fully Open Language Models with Stellar Performance ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.10628",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] Robot Crash Course: Learning Soft and Stylized Falling ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.10635",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] Operator Models for Continuous-Time Offline Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.10383",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] Global Solutions to Non-Convex Functional Constrained Problems with Hidden Convexity ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.10626",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 11'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] A General Anchor-Based Framework for Scalable Fair Clustering ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09889",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] The Role of Advanced Computer Architectures in Accelerating Artificial Intelligence Workloads ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.10010",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] Efficient Hyperdimensional Computing with Modular Composite Representations ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09708",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] Steering Pretrained Drafters during Speculative Decoding ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09844",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] On the Convergence of Overparameterized Problems: Inherent Properties of the Compositional Structure of Neural Networks ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09810",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] Quality Assurance of LLM-generated Code: Addressing Non-Functional Quality Characteristics ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.10271",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] EDGC: Entropy-driven Dynamic Gradient Compression for Efficient LLM Training ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.10333",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] Simulating Misinformation Propagation in Social Networks using Large Language Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.10384",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] Pretrained Joint Predictions for Scalable Batch Bayesian Optimization of Molecular Designs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.10590",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] Solvaformer: an SE(3)-equivariant graph transformer for small molecule solubility prediction ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.09774",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251114] MATAI: A Generalist Machine Learning Framework for Property Prediction and Inverse Design of Advanced Alloys ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.10108",children:"link"})]}),"\n"]})]})}function d(i={}){const{wrapper:e}={...(0,a.R)(),...i.components};return e?(0,s.jsx)(e,{...i,children:(0,s.jsx)(h,{...i})}):h(i)}},8453:(i,e,n)=>{n.d(e,{R:()=>t,x:()=>l});var r=n(6540);const s={},a=r.createContext(s);function t(i){const e=r.useContext(a);return r.useMemo(function(){return"function"==typeof i?i(e):{...e,...i}},[e,i])}function l(i){let e;return e=i.disableParentContext?"function"==typeof i.components?i.components(s):i.components||s:t(i.components),r.createElement(a.Provider,{value:e},i.children)}}}]);