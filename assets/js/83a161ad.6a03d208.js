"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[174],{8266:(i,e,n)=>{n.r(e),n.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>t,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"daily/20251103-20251109","title":"20251103-20251109","description":"2025-11-03","source":"@site/docs/daily/20251103-20251109.md","sourceDirName":"daily","slug":"/daily/20251103-20251109","permalink":"/Recommend-System-Note/daily/20251103-20251109","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1762876111000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"20251027-20251102","permalink":"/Recommend-System-Note/daily/20251027-20251102"},"next":{"title":"Paper","permalink":"/Recommend-System-Note/category/paper"}}');var s=n(4848),a=n(8453);const t={},o="20251103-20251109",l={},c=[{value:"2025-11-03",id:"2025-11-03",level:2},{value:"2025-11-04",id:"2025-11-04",level:2},{value:"2025-11-05",id:"2025-11-05",level:2},{value:"2025-11-06",id:"2025-11-06",level:2},{value:"2025-11-07",id:"2025-11-07",level:2}];function h(i){const e={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...i.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"20251103-20251109",children:"20251103-20251109"})}),"\n",(0,s.jsx)(e.h2,{id:"2025-11-03",children:"2025-11-03"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"cs.DC total: 11"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251103] Glia: A Human-Inspired AI for Automated Systems Design and Optimization"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [multi-agent workflow, large language models, interpretable designs, request routing, scheduling, auto-scaling]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Pouya Hamadanian, Pantea Karimi, Arash Nasr-Esfahany, Kimia Noorbakhsh, Joseph Chandler, Ali ParandehGheibi, Mohammad Alizadeh, Hari Balakrishnan"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," MIT CSAIL"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27176",children:"https://arxiv.org/pdf/2510.27176"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," Glia uses a human-inspired multi-agent architecture with LLMs that specialize in reasoning, experimentation, and analysis to automate systems design. It generates interpretable algorithms for distributed GPU clusters performing LLM inference. The system produces human-expert level designs in less time while providing novel insights into workload behavior."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251103] Dynamic Service Scheduling and Resource Management in Energy-Harvesting Multi-access Edge Computing"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [edge computing], [energy harvesting, online scheduling, DAG, DVFS, service module migration]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Shuyi Chen, Panagiotis Oikonomou, Zhengchang Hua, Nikos Tziritas, Karim Djemame, Nan Zhang, Georgios Theodoropoulos"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Leeds, Southern University of Science and Technology, University of Thessaly"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27317",children:"https://arxiv.org/pdf/2510.27317"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes an online strategy for energy-harvesting MEC systems that dynamically schedules computational tasks with dependencies and manages energy consumption through server frequency scaling and service module migration. The method efficiently utilizes harvested energy while maintaining low service latency, as demonstrated through experiments with real-world datasets."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251103] Synergistic Tensor and Pipeline Parallelism"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [tensor parallelism, pipeline parallelism, hybrid model parallelism, scheduling, communication optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Mengshi Qi, Jiaxuan Peng, Jie Zhang, Juan Zhu, Yong Li, Huadong Ma"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Beijing University of Posts and Telecommunications"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27257",children:"https://arxiv.org/pdf/2510.27257"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a synergistic tensor and pipeline parallelism schedule that decouples forward and backward passes into fine-grained computation units and braids them into a composite sequence. This approach simultaneously reduces both tensor parallelism communication bubbles and pipeline parallelism synchronization bubbles. Experimental results show throughput improvements of up to 12% for LLMs and 16% for MLLMs compared to existing methods."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251103] A Cloud-Based Spatio-Temporal GNN-Transformer Hybrid Model for Traffic Flow Forecasting with External Feature Integration"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [Spatio-Temporal Graph Neural Networks, Transformer, Cloud Computing, Feature Fusion]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Zhuo Zheng, Lingran Meng, Ziyu Lin"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Nanchang University, University of Washington, Google"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27039",children:"https://arxiv.org/pdf/2510.27039"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a cloud-based hybrid model combining Spatio-Temporal Graph Neural Networks with Transformer architecture for traffic flow forecasting, integrating external features like weather and holidays. The model leverages GNNs for spatial correlations and Transformers for long-term temporal dependencies. Experimental results show it outperforms baseline methods with RMSE of 17.92 and MAE of 10.53, demonstrating effectiveness for intelligent transportation systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251103] SERFLOW: A Cross-Service Cost Optimization Framework for SLO-Aware Dynamic ML Inference"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [dynamic offloading, FaaS, IaaS, stage-specific resource provisioning, adaptive load balancing, serverless functions, cost optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Zongshun Zhang, Ibrahim Matta"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Boston University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27182",children:"https://arxiv.org/pdf/2510.27182"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," SERFLOW proposes a cross-service optimization framework that dynamically offloads ML model partitions between IaaS VMs and FaaS serverless functions using stage-specific resource provisioning and adaptive load balancing. The framework accounts for variable request exit rates across model stages and real-world factors like VM cold starts. This approach reduces cloud costs by over 23% while maintaining service level objectives for dynamic ML inference workloads."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251103] Secure Communication in the Presence of an RIS-Enhanced Eavesdropper in MIMO Networks"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [wireless communication security], [reconfigurable intelligent surface, MIMO, singular value decomposition, bit-flipping, physical layer security]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Gaoyuan Zhang, Ruisong Si, Boyuan Li, Zijian Li, Baofeng Ji, Chenqi Zhu, Tony Q.S. Quek"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Henan University of Science and Technology, Zhengzhou University, Dalian Maritime University, Singapore University of Technology and Design"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27147",children:"https://arxiv.org/pdf/2510.27147"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a secure communication scheme using random bit-flipping and SVD-based precoding to protect MIMO wireless networks against RIS-enhanced eavesdroppers. The method minimizes mutual information between secret messages and eavesdropper data without requiring full channel state information. Results demonstrate the scheme's effectiveness and robustness across various attacking scenarios."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251103] FlowMesh: A Service Fabric for Composable LLM Workflows"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [post-training], [workflow optimization, multi-tenant service fabric, content-addressable store, operator lineage, batch scheduling]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Junyi Shen, Noppanat Wadlom, Lingfeng Zhou, Dequan Wang, Xu Miao, Lei Fang, Yao Lu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," National University of Singapore, Shanghai Jiao Tong University, DataCanvas"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26913",children:"https://arxiv.org/pdf/2510.26913"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," FlowMesh is a service fabric that decomposes LLM workflows into fine-grained operators and optimizes execution across multiple users through deduplication and intelligent batching. It uses a global control plane for scheduling and stateless workers with content-addressable storage for elasticity. The system achieves up to 3.8\xd7 cost reduction and maintains efficiency under dynamic conditions compared to baseline solutions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251103] ML-Based Optimum Sub-system Size Heuristic for the GPU Implementation of the Tridiagonal Partition Method"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [GPU kernels], [k-nearest neighbors, parallel partition algorithm, CUDA implementation, tridiagonal matrices, recursive algorithm]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Milena Veneva"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," RIKEN Center for Computational Science"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27351",children:"https://arxiv.org/pdf/2510.27351"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper develops a machine learning-based heuristic using k-nearest neighbors classification to determine optimal sub-system sizes for GPU implementations of parallel partition algorithms solving tridiagonal systems. The method was empirically validated through computational experiments on various SLAE sizes and extended to recursive variants. Results showed the algorithm performed acceptably well in predicting optimal parameters for GPU performance optimization."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251103] A Digital Twin-based Multi-Agent Reinforcement Learning Framework for Vehicle-to-Grid Coordination"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [Digital Twin, Multi-Agent Reinforcement Learning, MADDPG, Vehicle-to-Grid, privacy-preserving]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Zhengchang Hua, Panagiotis Oikonomou, Karim Djemame, Nikos Tziritas, Georgios Theodoropoulos"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Southern University of Science and Technology, University of Leeds, University of Thessaly, Research Institute of Trustworthy Autonomous Systems"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27289",children:"https://arxiv.org/pdf/2510.27289"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes DT-MADDPG, a hybrid framework combining multi-agent reinforcement learning with collaborative Digital Twins for Vehicle-to-Grid coordination. The method enhances the centralized critic with a predictive global model built from privacy-preserving data shared by individual Digital Twins. Experimental results show the approach achieves coordination performance comparable to standard MADDPG while providing significant advantages in data privacy and architectural decentralization."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251103] Byzantine Attacks in RIS-Enhanced Cooperative Spectrum Sensing: A Decision Fusion Perspective"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [wireless security], [cooperative spectrum sensing, Byzantine attacks, decision fusion, reconfigurable intelligent surface, decode-and-forward relay]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Gaoyuan Zhang, Gaolei Song, Boyuan Li, Zijian Li, Baofeng Ji, Ruijuan Zheng, Guoqiang Zheng, Tony Q.S. Quek"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Henan University of Science and Technology, Zhengzhou University, Dalian Maritime University, Singapore University of Technology and Design"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27175",children:"https://arxiv.org/pdf/2510.27175"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper investigates Byzantine attacks in RIS-enhanced cooperative spectrum sensing systems from a decision fusion perspective. The authors develop channel-aware attack strategies and show that optimal Byzantine attacks don't require global instantaneous channel state information and effectiveness primarily depends on the fraction of compromised nodes rather than channel dynamics. Their counterintuitive findings demonstrate that heavy reliance on global ICSI and decision fusion rules can be successfully relaxed for practical implementation."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251103] RDMA Point-to-Point Communication for LLM Systems"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [RDMA, point-to-point communication, TransferEngine, one-sided WriteImm, ImmCounter, disaggregated inference, Mixture-of-Experts]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Nandor Licker, Kevin Hu, Vladimir Zaytsev, Lequn Chen"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Perplexity AI"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27656",children:"https://arxiv.org/pdf/2510.27656"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper presents TransferEngine, a portable RDMA communication library that provides uniform point-to-point communication across different network hardware. It enables high-performance communication for LLM systems through one-sided WriteImm operations with ImmCounter completion notification. The system achieves 400 Gbps throughput and demonstrates practical benefits in disaggregated inference, RL fine-tuning, and MoE routing while avoiding vendor lock-in."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 18'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] A Framework for Fair Evaluation of Variance-Aware Bandit Algorithms ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27001",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Limits of Generalization in RLVR: Two Case Studies in Mathematical Reasoning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27044",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Reinforcement Learning for Long-Horizon Unordered Tasks: From Boolean to Coupled Reward Machines ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27329",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] e1: Learning Adaptive Control of Reasoning Effort ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27042",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] GUI-Rise: Structured Reasoning and History Summarization for GUI Navigation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27210",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Reasoning Models Sometimes Output Illegible Chains of Thought ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27338",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] MedCalc-Eval and MedCalc-Env: Advancing Medical Calculation Capabilities of Large Language Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27267",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Towards Understanding Self-play for LLM Reasoning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27072",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Do Vision-Language Models Measure Up? Benchmarking Visual Measurement Reading with MeasureBench ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26865",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] AURA: A Reinforcement Learning Framework for AI-Driven Adaptive Conversational Surveys ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27126",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Realistic pedestrian-driver interaction modelling using multi-agent RL with human perceptual-motor constraints ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27383",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] DeepCompress: A Dual Reward Strategy for Dynamically Exploring and Compressing Reasoning Chains ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27419",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Learning Soft Robotic Dynamics with Active Exploration ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27428",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] VCORE: Variance-Controlled Optimization-based Reweighting for Chain-of-Thought Supervision ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27462",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27606",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Challenges in Credit Assignment for Multi-Agent Reinforcement Learning in Open Agent Systems ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27659",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Reinforcement Learning for Accelerator Beamline Control: a simulation-based approach ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26805",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] When AI Trading Agents Compete: Adverse Selection of Meta-Orders by Reinforcement Learning-Based Market Making ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27334",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 12'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] CAS-Spec: Cascade Adaptive Self-Speculative Decoding for On-the-Fly Lossless Inference Acceleration of LLMs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26843",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] H2-Cache: A Novel Hierarchical Dual-Stage Cache for High-Performance Acceleration of Generative Diffusion Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27171",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Sparse Model Inversion: Efficient Inversion of Vision Transformers for Data-Free Applications ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27186",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] MLPerf Automotive ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27065",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] HiF-DTA: Hierarchical Feature Learning Network for Drug-Target Affinity Prediction ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27281",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Jasmine: A Simple, Performant and Scalable JAX-based World Modeling Codebase ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27002",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] FPS: Feedforward-based Parameter Selection For Efficient Fine-Tuning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27359",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Fine-Tuning Open Video Generators for Cinematic Scene Synthesis: A Small-Data Pipeline with LoRA and Wan2.1 I2V ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27364",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] FedMuon: Accelerating Federated Learning with Matrix Orthogonalization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27403",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] InnovatorBench: Evaluating Agents' Ability to Conduct Innovative LLM Research ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27598",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.27629",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Reinforcement Learning for Accelerator Beamline Control: a simulation-based approach ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26805",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-11-04",children:"2025-11-04"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"cs.DC total: 25"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251104] Tetris: An SLA-aware Application Placement Strategy in the Edge-Cloud Continuum"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [edge-cloud resource management], [heuristic algorithm, SLA-aware placement, resource efficiency, latency-sensitive applications]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Lucas Almeida, Maycon Peixoto"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Federal University of Bahia"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00294",children:"https://arxiv.org/pdf/2511.00294"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes Tetris, a heuristic-based application placement strategy that prioritizes services based on SLA urgencies and resource efficiency in Edge-Cloud Continuum environments. The method reduces SLA violations by approximately 76% compared to baseline approaches, demonstrating improved Quality of Service for latency-sensitive applications."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251104] Fix: externalizing network I/O in serverless computing"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [serverless computing], [externalized I/O, deterministic procedures, dataflow scheduling]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yuhan Deng, Akshay Srivatsan, Sebastian Ingino, Francis Chua, Yasmine Mitchell, Matthew Vilaysack, Keith Winstein"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Stanford University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00205",children:"https://arxiv.org/pdf/2511.00205"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"}),' This paper proposes a serverless computing system where computations are represented as deterministic procedures with explicit data dependencies, enabling the platform to externalize network I/O. By making data requirements explicit, the system allows better scheduling of tasks and network transfers to reduce starvation. The approach suggests shifting cloud computing from a "pay-for-effort" to a "pay-for-results" service model.']}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251104] AReaL-Hex: Accommodating Asynchronous RL Training over Heterogeneous GPUs"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [asynchronous RL training, heterogeneous GPU scheduling, mixed-integer linear programming, graph partitioning, data staleness bounds]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Ran Yan, Youhe Jiang, Tianyuan Wu, Jiaxuan Gao, Zhiyu Mei, Wei Fu, Haohui Mai, Wei Wang, Yi Wu, Binhang Yuan"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," HKUST, Tsinghua University, Ant Group"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00796",children:"https://arxiv.org/pdf/2511.00796"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," AReaL-Hex is a heterogeneity-aware asynchronous RL training system that uses a two-phase scheduler with MILP optimization and graph partitioning to efficiently deploy RL training across heterogeneous GPUs. The system achieves up to 1.50\xd7 higher training throughput at the same budget and reduces training costs by up to 1.46\xd7 while maintaining the same throughput compared to homogeneous deployments."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251104] Split Learning-Enabled Framework for Secure and Light-weight Internet of Medical Things Systems"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [split learning, federated learning, game-theoretic optimization, image-based classification, malware detection]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Siva Sai, Manish Prasad, Animesh Bhargava, Vinay Chamola, Rajkumar Buyya"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," The University of Melbourne, BITS-Pilani"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00336",children:"https://arxiv.org/pdf/2511.00336"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a split learning framework for IoT malware detection that divides neural network training between clients and an edge server to reduce computational burden while maintaining data privacy. Experimental results show the framework outperforms federated learning methods with higher accuracy (+6.35%), faster convergence (+14.96%), and lower resource consumption (33.83%), establishing split learning as a scalable and secure paradigm for IoT security."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251104] Benchmarking Federated Learning Frameworks for Medical Imaging Deployment: A Comparative Study of NVIDIA FLARE, Flower, and Owkin Substra"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [federated learning, medical imaging, PathMNIST, model convergence, communication overhead, scalability assessment]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Riya Gupta, Alexander Chowdhury, Sahil Nalawade"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Harvard T.H. Chan School of Public Health, Dana-Farber Cancer Institute"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00037",children:"https://arxiv.org/pdf/2511.00037"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper benchmarks three federated learning frameworks (NVIDIA FLARE, Flower, and Owkin Substra) using the PathMNIST medical imaging dataset to evaluate performance, scalability, and deployment features. The study found that each framework has distinct strengths: NVIDIA FLARE excels in production scalability, Flower offers research flexibility, and Owkin Substra provides superior privacy compliance for healthcare applications."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251104] EPARA: Parallelizing Categorized AI Inference in Edge Clouds"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [task categorization, parallel inference, edge computing, distributed scheduling, resource allocation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yubo Wang, Yubo Cui, Tuo Shi, Danyang Li, Wenxin Li, Lide Suo, Tao Wang, Xin Xie"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Tianjin University, Aalto University, Nankai University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00603",children:"https://arxiv.org/pdf/2511.00603"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," EPARA introduces a parallel AI inference framework that categorizes tasks based on latency sensitivity and GPU requirements to optimize resource allocation in edge clouds. The system employs task-categorized parallelism allocation, distributed request handling, and state-aware scheduling to improve serving capability. Experimental results show EPARA achieves up to 2.1\xd7 higher goodput compared to prior frameworks while adapting to various edge AI inference tasks."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251104] LongCat-Flash-Omni Technical Report"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal training], [Mixture-of-Experts, modality-decoupled parallelism, curriculum-inspired progressive training, multimodal perception, speech reconstruction]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Meituan LongCat Team, Bairui Wang, Bayan, Bin Xiao, Bo Zhang, Bolin Rong, Borun Chen, Chang Wan, Chao Zhang, Chen Huang, Chen Chen, Chen Chen, Chengxu Yang, Chengzuo Yang, Cong Han, Dandan Peng, Delian Ruan, Detai Xin, Disong Wang, Dongchao Yang, Fanfan Liu, Fengjiao Chen, Fengyu Yang, Gan Dong, Gang Huang, Gang Xu, Guanglu Wan, Guoqiang Tan, Guoqiao Yu, Haibo Qiu, Hao Lu, Hongbo Liu, Hongyu Xiang, Jiaheng Wu, Jian Yang, Jiaxing Liu, Jing Huang, Jingang Wang, Jinrui Ding, Juchao Jiang, Jun Kuang, Jun Wang, Junhui Mei, Ke Ding, Kefeng Zhang, Lei Chen, Liang Shi, Limeng Qiao, Liming Zheng, Lin Ma, Liuyang Guo, Liya Ma, Luying Sun, Man Gao, Mengshen Zhu, Miao Cao, Minliang Lin, Nuo Xu, Peng Shi, Qi Zhang, Qian Fang, Qian Wang, Qian Yang, Quanxiu Wang, Rongxiang Weng, Rongxin Guo, Ruoxuan Liang, Senbin Yang, Shanbo Xu, Shanglin Lei, Shengze Ye, Shimin Chen, Shuaiqi Chen, Shujie Hu, Shuo Li, Siqi Yang, Siyu Xu, Siyu Ren, Song Li, Songxiang Liu, Tianhao Bai, Tianye Dai, Wei Hong, Wei Wang, Weixiao Zhao, Wengang Cao, Wenlong Zhu, Wenlong He, Xi Su, Xi Nan, Xiaohan Zhao, Xiaohao Wang, Xiaoyu Zhao, Xiaoyu Wang, Xiaoyu Li, Xin Pan, Xin Chen, Xiusong Sun, Xu Xiang, Xudong Xing"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Meituan"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00279",children:"https://arxiv.org/pdf/2511.00279"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," LongCat-Flash-Omni is a 560B parameter omni-modal model that uses a Mixture-of-Experts architecture with modality-decoupled parallelism and progressive training strategy. It achieves state-of-the-art performance on multimodal benchmarks while maintaining efficient real-time audio-visual interaction despite its large parameter count. The model demonstrates competitive results across text, image, video, and audio tasks while sustaining over 90% of text-only training throughput."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251104] COOL Is Optimal in Error-Free Asynchronous Byzantine Agreement"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed systems], [Byzantine agreement, error correction codes, asynchronous consensus, information-theoretic security]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Jinyuan Chen"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Unknown (author: Jinyuan Chen)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00263",children:"https://arxiv.org/pdf/2511.00263"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents OciorACOOL, an adaptive variant of the COOL protocol that extends Byzantine agreement to asynchronous settings while maintaining error-free information-theoretic security. The protocol achieves consensus with O(max{n\u2113, nt log q}) communication bits, O(1) rounds, and a single binary BA invocation under optimal resilience n \u2265 3t+1. It preserves the same low-complexity error-correction encoding and decoding as the original COOL protocol."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251104] AeroResQ: Edge-Accelerated UAV Framework for Scalable, Resilient and Collaborative Escape Route Planning in Wildfire Scenarios"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [fault-tolerance], [weighted A* search, edge accelerators, Apache IoTDB, geo-fenced re-partitioning, multi-layer orchestration]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Suman Raj, Radhika Mittal, Rajiv Mayani, Pawel Zuk, Anirban Mandal, Michael Zink, Yogesh Simmhan, Ewa Deelman"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Indian Institute of Science, University of Southern California, University of North Carolina, University of Massachusetts Amherst"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00038",children:"https://arxiv.org/pdf/2511.00038"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," AeroResQ proposes an edge-accelerated UAV framework using service drones for real-time detection and coordinator drones with weighted A* search for escape route planning. The system incorporates resilient mechanisms like automated data redistribution and workload reassignment to handle drone failures. Experimental results show it achieves \u2264500ms latency and over 98% task completion, demonstrating feasibility for real-time wildfire emergency response."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251104] Agentic Auto-Scheduling: An Experimental Study of LLM-Guided Loop Optimization"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [GPU kernels], [loop optimization, polyhedral methods, compiler feedback, code transformation, tiling, fusion, parallelization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Massinissa Merouani, Islem Kara Bernou, Riyadh Baghdadi"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," New York University Abu Dhabi"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00592",children:"https://arxiv.org/pdf/2511.00592"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces ComPilot, a framework that uses Large Language Models as interactive optimization agents to guide loop nest transformations through a closed-loop feedback system with a compiler. The LLM proposes code transformations, receives compiler feedback on legality and performance, and iteratively refines its optimization strategy. The approach achieves significant speedups (2.66x-3.54x) on PolyBench benchmarks and demonstrates competitive performance against the state-of-the-art Pluto polyhedral optimizer."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251104] Scalable Maxflow Processing for Dynamic Graphs"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [graph algorithms], [push-relabel algorithm, GPU parallelization, CUDA optimizations, dynamic graph processing]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Shruthi Kannappan, Ashwina Kumar, Rupesh Nasre"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Indian Institute of Technology Madras"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01235",children:"https://arxiv.org/pdf/2511.01235"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents novel GPU-parallel Max-Flow algorithms for both static and dynamic graphs, building on the Push-Relabel method. The authors introduce CUDA-specific optimizations to enhance performance and scalability. Their approach efficiently handles incremental updates to dynamic graphs without recomputing from scratch."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251104] Boosting performance of computer vision applications through embedded GPUs on the edge"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [GPU kernels], [edge computing, embedded GPUs, computer vision, performance optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Fabio Diniz Rossi"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Federal Institute of Education, Science, and Technology Farroupilha (IFFar)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01129",children:"https://arxiv.org/pdf/2511.01129"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes using embedded GPUs in edge computing devices to accelerate computer vision applications. Experimental results show that GPUs achieve performance gains of up to 820.36% compared to CPUs alone. This approach improves user experience by overcoming the resource limitations of traditional edge devices."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251104] Design of quasi phase matching crystal based on differential gray wolf algorithm"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [GPU kernels], [differential evolution algorithm, gray wolf optimization, GPU parallel computing, quasi-phase matching crystal design]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," He Chen, ZiHua Zheng, JingHua Sun"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Dongguan University of Technology"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01255",children:"https://arxiv.org/pdf/2511.01255"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a hybrid optimization algorithm combining differential evolution and gray wolf optimization with GPU parallel acceleration for quasi-phase matching crystal design. The method achieves hundreds to thousands of times efficiency improvement compared to traditional CPU serial computing while enhancing crystal domain control accuracy. This breakthrough enables better performance in nonlinear optical devices for quantum optics and laser processing applications."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251104] Gradient Clock Synchronization with Practically Constant Local Skew"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed systems], [gradient clock synchronization, local skew, offset estimation, frequency deviation, self-stabilization, external synchronization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Christoph Lenzen"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," CISPA Helmholtz Center for Information Security"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01420",children:"https://arxiv.org/pdf/2511.01420"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents a refined model for gradient clock synchronization that leverages stability of measurement and frequency errors rather than worst-case bounds. The approach achieves local skew bounds of O(\u0394+\u03b4 log D) by adapting to actual link performance and frequency changes, effectively breaking previous lower bounds. The method also ensures self-stabilization and extends to external synchronization scenarios."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251104] TINC: Trusted Intelligent NetChain"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [blockchain sharding], [multi-plane sharding, adaptive node assignment, dynamic workload balancing, control-data plane decoupling, Dynamic Decentralized Identifiers (DDIDs)]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Qi Xia, Hu Xia, Isaac Amankona Obiri, Adjei-Arthur Bonsu, Grace Mupoyi Ntuala, Ansu Badjie, Tienin Bole Wilfried, Jiaqin Liu, Lan Ma, Jianbin Gao, Feng Yao"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Electronic Science and Technology of China, National University of Defense Technology"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00823",children:"https://arxiv.org/pdf/2511.00823"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes TINC, a multi-plane sharding architecture for consortium blockchains that uses intelligent mechanisms for adaptive node assignment and workload balancing. By decoupling control and data planes, TINC improves scalability while maintaining security guarantees. Experimental results show it achieves higher throughput, lower latency, and better load balancing compared to existing sharding frameworks."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251104] Towards Portability at Scale: A Cross-Architecture Performance Evaluation of a GPU-enabled Shallow Water Solver"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [HPC Performance Evaluation], [Kokkos, GPU acceleration, roofline analysis, strong scaling, weak scaling, performance portability]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Johansell Villalobos, Daniel Caviedes-Voulli\xe8me, Silvio Rizzi, Esteban Meneses"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," National High Technology Center Costa Rica, J\xfclich Supercomputing Center, Argonne National Laboratory, Costa Rica Technological Institute"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01001",children:"https://arxiv.org/pdf/2511.01001"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper evaluates the SERGHEI-SWE shallow water solver using Kokkos for performance portability across four HPC systems with different GPU architectures. The study demonstrates strong scalability up to 1024 GPUs and identifies memory bandwidth as the primary performance bottleneck through roofline analysis. Results show the solver achieves good performance portability but requires further kernel optimization for improved efficiency across architectures."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251104] Neuro-Inspired Task Offloading in Edge-IoT Networks Using Spiking Neural Networks"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [Spiking Neural Networks, task offloading, edge computing, IoT networks, Brian2 simulator, YAFS]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Fabio Diniz Rossi"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Federal Institute Farroupilha"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01127",children:"https://arxiv.org/pdf/2511.01127"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a neuro-inspired task offloading framework using Spiking Neural Networks for edge-IoT networks. The SNN-based approach enables real-time, energy-efficient task orchestration by leveraging event-driven computation and temporal pattern processing. Experimental results show the framework achieves significant improvements with up to 26% lower latency, 32% reduced energy consumption, and 25% higher success rate compared to traditional methods."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251104] Real-time Continual Learning on Intel Loihi 2"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [spiking neural networks, neuromorphic computing, continual learning, online learning, catastrophic forgetting, neurogenesis, metaplasticity]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Elvin Hajizada, Danielle Rager, Timothy Shea, Leobardo Campos-Macias, Andreas Wild, Eyke H\xfcllermeier, Yulia Sandamirskaya, Mike Davies"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Intel Labs, University of Munich (LMU), Zurich University of Applied Sciences (ZHAW)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01553",children:"https://arxiv.org/pdf/2511.01553"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents CLP-SNN, a spiking neural network architecture implemented on Intel's Loihi 2 neuromorphic chip for online continual learning. The method uses event-driven sparse learning, self-normalizing learning rules, and integrated neurogenesis to achieve competitive accuracy without rehearsal. The results show dramatic efficiency improvements with 70\xd7 faster inference and 5,600\xd7 better energy efficiency compared to edge GPU implementations."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251104] Transformer-Based Sparse CSI Estimation for Non-Stationary Channels"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [Flash-Attention Transformer, patch-wise self-attention, composite loss function, pilot-aided estimation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Muhammad Ahmed Mohsin, Muhammad Umer, Ahsan Bilal, Hassan Rizwan, Sagnik Bhattacharya, Muhammad Ali Jamshed, John M. Cioffi"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Stanford University, University of Oklahoma, University of California Riverside, University of Glasgow"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01333",children:"https://arxiv.org/pdf/2511.01333"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a Flash-Attention Transformer framework that combines model-driven pilot acquisition with data-driven CSI reconstruction for non-stationary wireless channels. The method uses patch-wise self-attention and a physics-aware composite loss function to achieve accurate channel estimation. Results show it outperforms traditional methods by 13 dB NMSE while reducing pilot overhead by 16 times, demonstrating reliable CSI recovery for 5G and beyond-5G networks."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251104] FREESH: Fair, Resource- and Energy-Efficient Scheduling for LLM Serving on Heterogeneous GPUs"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [joint routing and scheduling, dynamic GPU frequency scaling, Least-Laxity-First (LLF), spatiotemporal computation optimization, heterogeneous GPU clusters]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Xuan He, Zequan Fang, Jinzhao Lian, Danny H.K. Tsang, Baosen Zhang, Yize Chen"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Hong Kong University of Science and Technology (Guangzhou), Huazhong University of Science and Technology, Renmin University of China, University of Washington, University of Alberta"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00807",children:"https://arxiv.org/pdf/2511.00807"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," FREESH proposes a joint routing and scheduling system that optimizes LLM serving across heterogeneous GPU clusters by leveraging spatiotemporal computation flexibility and dynamic GPU frequency scaling. The system matches GPU power-throughput characteristics with query workloads while ensuring latency and fairness through LLF scheduling. Experimental results show FREESH reduces energy consumption by 28.6% and carbon emissions by 45.45% while improving SLO attainment and fairness."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251104] Federated Cyber Defense: Privacy-Preserving Ransomware Detection Across Distributed Systems"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [federated learning, ransomware detection, privacy-preserving AI, distributed systems]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Daniel M. Jimenez-Gutierrez, Enrique Zuazua, Joaquin Del Rio, Oleksii Sliusarenko, Xabi Uribe-Etxebarria"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Sherpa.ai"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01583",children:"https://arxiv.org/pdf/2511.01583"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes using Federated Learning with the Sherpa.ai platform to train ransomware detection models across distributed systems while keeping data local. The approach improves detection accuracy by 9% compared to server-local models and achieves performance comparable to centralized training. This demonstrates FL provides a scalable, privacy-preserving framework for cybersecurity across organizational boundaries."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251104] LARK - Linearizability Algorithms for Replicated Keys in Aerospike"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed databases], [linearizability, synchronous replication, partition availability conditions, log-free replication, TLA+ specification]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Andrew Goodng, Kevin Porter, Thomas Lopatic, Ashish Shinde, Sunil Sayyaparaju, Srinivasan Seshadri, V. Srinivasan"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Aerospike"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01843",children:"https://arxiv.org/pdf/2511.01843"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," LARK introduces a log-free synchronous replication protocol using Partition Availability Conditions that reason across the entire database cluster rather than fixed replica sets. This approach achieves linearizability while providing significantly higher availability than traditional quorum-log consensus protocols like Raft and Paxos, enabling continued commits during node failures and zero-downtime rolling restarts with minimal replication factors."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251104] Edge AI in Highly Volatile Environments: Is Fairness Worth the Accuracy Trade-off?"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [federated learning, client selection, fairness algorithms, RBFF, RBCSF, edge computing]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Obaidullah Zaland, Feras M. Awaysheh, Sawsan Al Zubi, Abdul Rahman Safi, Monowar Bhuyan"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Ume\xe5 University, University of Santiago de Compostela, Kabul University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01737",children:"https://arxiv.org/pdf/2511.01737"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper evaluates fairness-based client selection algorithms (RBFF and RBCSF) in federated learning for volatile edge environments. The study finds that while equitable client selection provides better participation opportunities, it results in slower global training convergence. The research highlights the fundamental trade-off between fairness and performance in edge AI systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251104] Adaptive Multidimensional Quadrature on Multi-GPU Systems"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [high-performance computing], [adaptive quadrature, domain decomposition, load balancing, CUDA-aware MPI, multi-GPU systems]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Melanie Tonarelli, Simone Riva, Pietro Benedusi, Fabrizio Ferrandi, Rolf Krause"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Universit\xe0 della Svizzera Italiana, Politecnico di Milano, King Abdullah University of Science and Technology"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01573",children:"https://arxiv.org/pdf/2511.01573"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents a distributed adaptive quadrature method that performs multidimensional integration through hierarchical domain decomposition on multi-GPU systems. The method uses decentralized load redistribution with cyclic round-robin policy and non-blocking MPI communication to handle load imbalance. Compared to state-of-the-art GPU packages, it achieves higher efficiency in high dimensions and improved robustness regarding integrand regularity and target accuracy."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251104] A Distributed Plug-and-Play MCMC Algorithm for High-Dimensional Inverse Problems"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [MCMC, Plug-and-Play ULA, distributed computing, denoising neural network, Single Program Multiple Data]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Maxime Bouton, Pierre-Antoine Thouvenin, Audrey Repetti, Pierre Chainais"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Lille, Heriot-Watt University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00870",children:"https://arxiv.org/pdf/2511.00870"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a distributed Plug-and-Play Markov Chain Monte Carlo algorithm that uses lightweight denoising neural networks and approximate data augmentation to solve high-dimensional imaging inverse problems. The method efficiently leverages multiple GPUs through a Single Program Multiple Data architecture to address scalability challenges. The approach achieves comparable reconstruction performance to other PnP methods while being scalable and enabling uncertainty quantification for large-scale imaging problems."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 46'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Robust Single-Agent Reinforcement Learning for Regional Traffic Signal Control Under Demand Fluctuations ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00549",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Study on Supply Chain Finance Decision-Making Model and Enterprise Economic Performance Prediction Based on Deep Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00166",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Who Can We Trust? Scope-Aware Video Moment Retrieval with Multi-Agent Conflict ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00370",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Single-agent Reinforcement Learning Model for Regional Adaptive Traffic Signal Control ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00551",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00457",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00405",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Bootstrap Off-policy with World Model ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00423",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] DCcluster-Opt: Benchmarking Dynamic Multi-Objective Optimization for Geo-Distributed Data Center Workloads ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00117",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Real-DRL: Teach and Learn in Reality ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00112",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] PreferThinker: Reasoning-based Personalized Image Preference Assessment ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00609",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Efficient Reinforcement Learning for Large Language Models with Intrinsic Exploration ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00794",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Iterative Foundation Model Fine-Tuning on Multiple Rewards ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00220",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00222",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] End-to-End Framework Integrating Generative AI and Deep Reinforcement Learning for Autonomous Ultrasound Scanning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00114",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] World Simulation with Video Foundation Models for Physical AI ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00062",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] LC-Opt: Benchmarking Reinforcement Learning and Agentic AI for End-to-End Liquid Cooling Optimization in Data Centers ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00116",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Token-Regulated Group Relative Policy Optimization for Stable Reinforcement Learning in Large Language Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00066",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] On the Fundamental Limitations of Decentralized Learnable Reward Shaping in Cooperative Multi-Agent Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00034",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Improving the Robustness of Control of Chaotic Convective Flows with Domain-Informed Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00272",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00710",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00088",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] SpatialTraceGen: High-Fidelity Traces for Efficient VLM Spatial Reasoning Distillation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00054",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] A Dual Large Language Models Architecture with Herald Guided Prompts for Parallel Fine Grained Traffic Signal Control ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00136",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Graph-Attentive MAPPO for Dynamic Retail Pricing ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00039",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] GrowthHacker: Automated Off-Policy Evaluation Optimization Using Code-Modifying LLM Agents ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00802",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Logic-informed reinforcement learning for cross-domain optimization of large-scale cyber-physical systems ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00806",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events? ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00808",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Equilibrium Policy Generalization: A Reinforcement Learning Framework for Cross-Graph Zero-Shot Generalization in Pursuit-Evasion Games ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00811",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] KFCPO: Kronecker-Factored Approximated Constrained Policy Optimization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00880",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] SLAP: Shortcut Learning for Abstract Planning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01107",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] DART: Difficulty-Adaptive Reasoning Truncation for Efficient Large Language Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01170",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Self-Harmony: Learning to Harmonize Self-Supervision and Self-Play in Test-Time Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01191",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Thought-For-Food: Reasoning Chain Induced Food Visual Question Answering ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01213",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Optimizing Electric Vehicle Charging Station Placement Using Reinforcement Learning and Agent-Based Simulations ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01218",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01331",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Diffusion-Based Solver for CNF Placement on the Cloud-Continuum ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01343",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Thinking with DistilQwen: A Tale of Four Distilled Reasoning and Reward Model Series ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01354",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Learning Intractable Multimodal Policies with Reparameterization and Diversity Regularization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01374",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Modulation of temporal decision-making in a deep reinforcement learning agent under the dual-task paradigm ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01415",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Learning to Seek Evidence: A Verifiable Reasoning Agent with Causal Faithfulness Analysis ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01425",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] TPS-Bench: Evaluating AI Agents' Tool Planning & Scheduling Abilities in Compounding Tasks ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01527",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Learning what to say and how precisely: Efficient Communication via Differentiable Discrete Communication Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01554",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] L2T-Tune",":LLM-Guided"," Hybrid Database Tuning with LHS and TD3 ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01602",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Collaborative Large Language Model Inference via Resource-Aware Parallel Speculative Decoding ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01695",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] RLAC: Reinforcement Learning with Adversarial Critic for Free-Form Generation Tasks ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01758",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] GenDexHand: Generative Simulation for Dexterous Hands ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01791",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 25'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00090",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] QuantumBench: A Benchmark for Quantum Problem Solving ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00092",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Structure-Preserving Physics-Informed Neural Network for the Korteweg--de Vries (KdV) Equation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00418",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Efficient Generation of Binary Magic Squares ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00547",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] DCcluster-Opt: Benchmarking Dynamic Multi-Objective Optimization for Geo-Distributed Data Center Workloads ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00117",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] World Simulation with Video Foundation Models for Physical AI ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00062",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Chain of Time: In-Context Physical Simulation with Image Generation Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00110",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Reliable Curation of EHR Dataset via Large Language Models under Environmental Constraints ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00772",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Advancing AI Challenges for the United States Department of the Air Force ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00267",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] FeNN-DMA: A RISC-V SoC for SNN acceleration ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00732",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Position Paper: If Innovation in AI Systematically Violates Fundamental Rights, Is It Innovation at All? ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00027",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Cross-fluctuation phase transitions reveal sampling dynamics in diffusion models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00124",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Scalable Processing-Near-Memory for 1M-Token LLM Inference: CXL-Enabled KV-Cache Management Beyond GPU Limits ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00321",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Leveraging Multi-Agent System (MAS) and Fine-Tuned Small Language Models (SLMs) for Automated Telecom Network Troubleshooting ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00651",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Diffusion Models at the Drug Discovery Frontier: A Review on Generating Small Molecules versus Therapeutic Peptides ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00209",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] LL-ViT: Edge Deployable Vision Transformers with Look Up Table Neurons ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00812",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Transformers as Intrinsic Optimizers: Forward Inference through the Energy Principle ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00907",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Keys in the Weights: Transformer Authentication Using Model-Bound Latent Representations ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00973",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] DART: Difficulty-Adaptive Reasoning Truncation for Efficient Large Language Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01170",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] When, What, and How: Rethinking Retrieval-Enhanced Speculative Decoding ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01282",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Estimation of Toeplitz Covariance Matrices using Overparameterized Gradient Descent ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01605",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Bridging Lifelong and Multi-Task Representation Learning via Algorithm and Complexity Measure ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01847",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Transfer learning discovery of molecular modulators for perovskite solar cells ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.00204",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Quantum Deep Learning Still Needs a Quantum Leap ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01253",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251104] Split-Flows: Measure Transport and Information Loss Across Molecular Resolutions ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01464",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-11-05",children:"2025-11-05"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"cs.DC total: 23"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251105] A Taxonomy of Schedulers -- Operating Systems, Clusters and Big Data Frameworks"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [cluster infrastructure], [workload schedulers, taxonomy, throughput, scalability, Borg]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Leszek Sliwko"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Axis Applications Ltd"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01860",children:"https://arxiv.org/pdf/2511.01860"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents a hierarchical taxonomy of workload schedulers based on architecture and design, with special focus on Google's Borg system. The taxonomy analyzes key design factors affecting throughput and scalability across operating systems, clusters, and big data frameworks. The review identifies incremental architectural improvements that have enhanced scheduler performance in modern computing environments."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251105] Possible Futures for Cloud Cost Models"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [cloud cost models, resource scheduling, HPC, AI/ML optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Vanessa Sochat, Daniel Milroy"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Lawrence Livermore National Laboratory"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01862",children:"https://arxiv.org/pdf/2511.01862"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper analyzes how current cloud cost models optimized for AI/ML workloads are poorly suited for scientific computing needs. It examines historical trends and proposes that without better resource allocation strategies, scientific workloads may be forced to run on suboptimal infrastructure. The authors conclude that new cloud cost models are needed to ensure continued support for scientific discovery."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251105] SPHERE: Spherical partitioning for large-scale routing optimization"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [graph algorithms], [spherical partitioning, Dijkstra, bidirectional search, graph partitioning, METIS, Louvain]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Robert Fabian Lindermann, Paul-Niklas Ken Kandora, Simon Caspar Zeller, Adrian Asmund Fessler, Steffen Rebennack"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Karlsruhe Institute of Technology"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01863",children:"https://arxiv.org/pdf/2511.01863"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," SPHERE is a query-aware heuristic that partitions large graph routing problems by identifying vertices close to both source and target, then recursively solving smaller subproblems. This approach avoids boundary repair issues and enables parallel processing while maintaining solution quality. The method demonstrates faster runtimes and smaller optimality gaps compared to existing approaches on graphs with over a million nodes."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251105] Learned Cost Model for Placement on Reconfigurable Dataflow Hardware"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [learned cost model, dataflow architecture, placement and routing, throughput prediction, reconfigurable hardware]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Etash Guha, Tianxiao Jiang, Andrew Deng, Jian Zhang, Muthu Annamalai"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," SambaNova Systems"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01872",children:"https://arxiv.org/pdf/2511.01872"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces a learned cost model that predicts throughput for mapping dataflow graphs onto reconfigurable hardware, replacing traditional hand-designed analytical models. The learned approach achieves 31%-52% more accurate throughput predictions across various graphs and maintains accuracy without performance annotations. Using this model results in 5.6% faster compiled graphs compared to conventional methods."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251105] HGraphScale: Hierarchical Graph Learning for Autoscaling Microservice Applications in Container-based Cloud Computing"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [hierarchical graph neural network, deep reinforcement learning, autoscaling, microservice dependencies, container-based cloud]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Zhengxin Fang, Hui Ma, Gang Chen, Rajkumar Buyya"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Victoria University of Wellington, University of Melbourne"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01881",children:"https://arxiv.org/pdf/2511.01881"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," HGraphScale uses hierarchical graph neural networks to model microservice dependencies and deployment schemes for autoscaling in container-based cloud environments. The approach dynamically adjusts container resources to handle fluctuating user request workloads. Experimental results show it reduces average response time by up to 80.16% compared to state-of-the-art methods under VM rental budget constraints."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251105] Conceptual Design Report for FAIR Computing"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [research computing infrastructure], [federated infrastructure, centrally-orchestrated, computing model, storage infrastructure, open data policies]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Johan Messchendorp, Mohammad Al-Turany, Volker Friese, Thorsten Kollegger, Bastian Loeher, Jochen Markert, Andrew Mistry, Thomas Neff, Adrian Oeftiger, Michael Papenbrock, Stephane Pietri, Shahab Sanjari, Tobias Stockmanns"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," FAIR, Darmstadt"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01861",children:"https://arxiv.org/pdf/2511.01861"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"}),' This paper presents a conceptual design for a federated and centrally-orchestrated computing infrastructure to support diverse research activities at FAIR. The proposed system aims to provide scalable and flexible computing resources with open data, software, and service policies. The main conclusion is that this infrastructure design will address future data challenges and serve research needs from the "first science (plus)" phase starting in 2028 through FAIR\'s modularized start version.']}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251105] EdgeReasoning: Characterizing Reasoning LLM Deployment on Edge GPUs"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [latency-accuracy tradeoffs, token length control, test-time scaling, prompt engineering, model tuning]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Benjamin Kubwimana, Qijing Huang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," NVIDIA"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01866",children:"https://arxiv.org/pdf/2511.01866"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents EdgeReasoning, a comprehensive study that systematically characterizes reasoning LLM deployment on edge GPUs by analyzing latency-accuracy tradeoffs across different architectures and model sizes. The research evaluates techniques for reducing reasoning token length and profiles test-time scaling methods to optimize accuracy under strict latency constraints. The study maps the Pareto frontier of achievable configurations to provide systematic guidance for optimal edge deployment of reasoning LLMs."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251105] CudaForge: An Agent Framework with Hardware Feedback for CUDA Kernel Optimization"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [GPU kernels], [multi-agent workflow, CUDA kernel optimization, hardware feedback, Nsight Compute metrics, LLM agents]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Zijian Zhang, Rong Wang, Shiyang Li, Yuebo Luo, Mingyi Hong, Caiwen Ding"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Minnesota, Twin Cities"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01884",children:"https://arxiv.org/pdf/2511.01884"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," CudaForge introduces a training-free multi-agent framework using LLM agents (Coder and Judge) that iteratively generate and optimize CUDA kernels with hardware feedback. The system achieves 97.6% correctness and 1.68\xd7 speedup over PyTorch baselines while demonstrating strong generalization across GPUs and models. This approach enables cost-effective, high-performance kernel optimization without requiring expensive training."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251105] Roadrunner: Accelerating Data Delivery to WebAssembly-Based Serverless Functions"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [serverless computing], [zero-copy data transfer, memory mapping, serialization-free communication, virtual data hose]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Cynthia Marcelino, Thomas Pusztai, Stefan Nastic"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," TU Wien"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01888",children:"https://arxiv.org/pdf/2511.01888"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," Roadrunner introduces a sidecar shim that enables near-zero copy data transfer between WebAssembly-based serverless functions by mapping function memory and using a virtual data hose. This approach eliminates serialization/deserialization overhead and reduces context switching between user and kernel space. Experimental results show Roadrunner improves inter-function communication latency by 44-89%, reduces serialization overhead by 97%, and increases throughput by 69x compared to state-of-the-art solutions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251105] Structural Analysis of Multi-Core Processor and Reliability Evaluation Model"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [fault-tolerance], [logical-probabilistic methods, structural analysis, reliability evaluation, fault tolerance modeling, variable structure systems]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," S. Tsiramua, H. Meladze, T. Davitashvili, J.M. Sanchez, F. Criado-Aldeanueva"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Georgia, Muskhelishvili Institute of Computational Mathematics, Georgian Technical University, Ivane Javakhishvili Tbilisi State University, University of Malaga"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01871",children:"https://arxiv.org/pdf/2511.01871"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper develops logical-probabilistic models to analyze the structural reliability and performance efficiency of multi-core processors with variable configurations. The research presents evaluation models for reliability, fault tolerance, and lifetime considering all possible operational states. The study demonstrates trends in improving efficiency indicators through structural analysis of dual-core and quad-core processors."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251105] RobustFSM: Submodular Maximization in Federated Setting with Malicious Clients"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [fault-tolerance], [submodular maximization, federated learning, robust aggregation, malicious clients]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Duc A. Tran, Dung Truong, Duy Le"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Massachusetts Boston"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02029",children:"https://arxiv.org/pdf/2511.02029"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes RobustFSM, a federated submodular maximization method designed to withstand attacks from malicious clients through robust aggregation techniques. Empirical evaluation shows the solution significantly outperforms conventional federated algorithms under severe attack scenarios, achieving up to 200% improvement in solution quality depending on dataset and attack conditions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251105] mLR: Scalable Laminography Reconstruction based on Memoization"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [GPU kernels], [memoization, ADMM-FFT, variable offloading, Fast Fourier Transform, laminography reconstruction]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Bin Ma, Viktor Nikitin, Xi Wang, Tekin Bicer, Dong Li"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of California, Merced, Argonne National Laboratory"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01893",children:"https://arxiv.org/pdf/2511.01893"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces mLR, which uses memoization to replace repetitive FFT operations in ADMM-FFT laminography reconstruction and implements variable offloading for GPU scaling. This approach achieves up to 65.4% performance improvement and enables reconstruction of 2K\xd72K\xd72K volumes, the largest ever handled by ADMM-FFT with limited memory."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251105] Eliminating Multi-GPU Performance Taxes: A Systems Approach to Efficient Distributed LLMs"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [fine-grained synchronization, tile-level producer-consumer pipelines, in-kernel communication, BSP optimization, kernel fusion]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Octavian Alexandru Trifan, Karthik Sangaiah, Muhammad Awad, Muhammad Osama, Sumanth Gudaparthi, Alexandru Nicolau, Alexander Veidenbaum, Ganesh Dasika"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of California, Irvine, AMD Research and Advanced Development"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02168",children:"https://arxiv.org/pdf/2511.02168"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes moving beyond the bulk synchronous parallel model by using fine-grained programming patterns that fuse computation and communication into single kernels. This approach eliminates three key performance taxes through tile-level producer-consumer pipelines and fine-grained dataflow synchronization. The method achieves 10-20% speedup in distributed LLM inference workloads compared to traditional BSP approaches."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251105] Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA Effects"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [GPU kernels], [Swizzled Head-first Mapping, NUMA-aware scheduling, multi-head attention optimization, cache reuse]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Mansi Choudhary, Karthik Sangaiah, Sonali Singh, Muhammad Osama, Lisa Wu Wills, Ganesh Dasika"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Duke University, Advanced Micro Devices Inc."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02132",children:"https://arxiv.org/pdf/2511.02132"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces Swizzled Head-first Mapping, a spatially-aware scheduling strategy that aligns attention heads with GPU NUMA domains to exploit intra-chiplet cache reuse. On AMD's MI300X architecture, this method achieves up to 50% higher performance over conventional attention algorithms and sustains high L2 cache hit rates of 80-97%. The results demonstrate that NUMA-aware scheduling is fundamental for achieving full efficiency on next-generation disaggregated GPUs."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251105] GPoS: Geospatially-aware Proof of Stake"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [blockchain consensus], [GPoS, geospatial decentralization, Proof of Stake, BFT protocols, HotStuff, CometBFT, Eigenvector centrality, Gini coefficient]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Shashank Motepalli, Naman Garg, Gengrui Zhang, Hans-Arno Jacobsen"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Toronto, IIIT-Delhi, Concordia University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02034",children:"https://arxiv.org/pdf/2511.02034"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes Geospatially-aware Proof of Stake (GPoS), a consensus mechanism that integrates geospatial diversity with stake-based voting power to improve blockchain decentralization. Experimental evaluation shows GPoS achieves 45% better geospatial decentralization while maintaining minimal performance overhead in BFT protocols. The results demonstrate GPoS effectively addresses geographic centralization issues in major Proof of Stake blockchains."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251105] From Models to Operators: Rethinking Autoscaling Granularity for Large Generative Models"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [operator-level autoscaling, resource allocation, performance profiling, dynamic scaling]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Xingqi Cui, Chieh-Jan Mike Liang, Jiarong Xing, Haoran Qiu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Rice University, Microsoft Research"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02248",children:"https://arxiv.org/pdf/2511.02248"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes an operator-level autoscaling framework that allocates resources at finer granularity by optimizing scaling, batching, and placement based on individual operator profiles. This approach demonstrates significant improvements over traditional model-level autoscaling, achieving up to 40% fewer GPUs and 35% less energy while preserving SLOs. The results show that operator-level scaling is fundamentally more effective for large generative workloads than model-level scaling."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251105] Lightweight Latency Prediction Scheme for Edge Applications: A Rational Modelling Approach"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [rational modelling, latency prediction, edge computing, 5-fold cross-validation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Mohan Liyanage, Eldiyar Zhantileuov, Ali Kadhum Idrees, Rolf Schuster"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Applied Sciences and Arts Dortmund"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02501",children:"https://arxiv.org/pdf/2511.02501"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a lightweight latency prediction scheme using rational modeling that predicts network latency based on features like frame size and link utilization without active probing. The model achieves high accuracy (MAE = 0.0115, R\xb2 = 0.9847) with competitive inference time, offering better precision-efficiency trade-offs than traditional regressors and neural networks for edge applications."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251105] 3D Point Cloud Object Detection on Edge Devices for Split Computing"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [split computing, voxelization, 3D point cloud, edge devices, deep neural networks]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Taisuke Noguchi, Takuya Azumi"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Saitama University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02293",children:"https://arxiv.org/pdf/2511.02293"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes using split computing to distribute 3D point cloud object detection processing between edge devices and servers. The method reduces computational burden on edge devices by transmitting intermediate data after voxelization or within the network. Experimental results show significant reductions in inference time (up to 70.8%) and edge device execution time (up to 90.0%)."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251105] Quantum-Enhanced Generative Models for Rare Event Prediction"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [variational quantum circuits, hybrid classical-quantum framework, tail-aware likelihood, quantum randomness-driven noise injection, parameter-shift gradients]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," M.Z. Haider, M.U. Ghouri, Tayyaba Noreen, M. Salman"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Universit\xe9 du Qu\xe9bec, The University of Faisalabad, SZABIST University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02042",children:"https://arxiv.org/pdf/2511.02042"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a Quantum-Enhanced Generative Model (QEGM) that combines classical deep latent-variable models with variational quantum circuits to improve rare event prediction. The hybrid framework uses quantum randomness for noise injection and a tail-aware loss function to enhance sample diversity and capture rare events. Results show QEGM reduces tail KL-divergence by up to 50% compared to classical baselines, demonstrating superior performance for rare-event modeling."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251105] Fast Algorithms for Scheduling Many-body Correlation Functions on Accelerators"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [high-performance computing], [scheduling algorithms, tensor contraction, memory optimization, GPU accelerators, temporal locality]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Oguz Selvitopi, Emin Ozturk, Jie Chen, Ponnuswamy Sadayappan, Robert G. Edwards, Ayd\u0131n Bulu\xe7"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Lawrence Berkeley National Laboratory, University of Utah, University of California, Jefferson Lab"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02257",children:"https://arxiv.org/pdf/2511.02257"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes two novel scheduling algorithms that reorder tensor contractions to optimize memory usage and data traffic on GPU accelerators for lattice quantum chromodynamics simulations. The methods achieve up to 2.1x improvement in peak memory and up to 1.9x faster computation time by increasing temporal locality through input and intermediate tensor reuse. These schedulers were integrated into the Redstar software suite, demonstrating significant performance improvements for correlation function computations."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251105] Federated Attention: A Distributed Paradigm for Collaborative LLM Inference over Edge Networks"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [federated attention, KV matrix aggregation, sparse attention, adaptive KV aggregation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Xiumei Deng, Zehui Xiong, Binbin Chen, Dong In Kim, Merouane Debbah, H. Vincent Poor"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Singapore University of Technology and Design, Queen's University Belfast, Sungkyunkwan University, Khalifa University, Princeton University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02647",children:"https://arxiv.org/pdf/2511.02647"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes Federated Attention (FedAttn), a distributed LLM inference framework that integrates federated learning principles into self-attention mechanisms. It enables collaborative response generation through local self-attention computation and periodic KV matrix exchange while preserving privacy. Experimental results demonstrate significant optimization opportunities and validate the framework's potential for scalable and efficient edge deployments."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251105] Implementing Multi-GPU Scientific Computing Miniapps Across Performance Portable Frameworks"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [high-performance computing], [Kokkos, OpenMP, RAJA, OCCA, multi-GPU, performance portability, N-body simulation, structured grid simulation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Johansell Villalobos, Josef Ruzicka, Silvio Rizzi"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," National High Technology Center, Costa Rica Institute of Technology, Argonne National Laboratory"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02655",children:"https://arxiv.org/pdf/2511.02655"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper compares four performance portability frameworks (Kokkos, OpenMP, RAJA, and OCCA) for implementing multi-GPU scientific computing applications including N-body and structured grid simulations. The study found significant performance variability among frameworks, with OCCA showing advantages for small problems but potential scalability limitations, while OpenMP exhibited communication inefficiencies. The results highlight the need for further optimization in reduction algorithms and data communication to maximize framework performance."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251105] Making Democracy Work: Fixing and Simplifying Egalitarian Paxos (Extended Version)"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed consensus], [Paxos, Egalitarian Paxos, state-machine replication, fault tolerance, leaderless consensus]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Fedor Ryabinin, Alexey Gotsman, Pierre Sutra"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," IMDEA Software Institute, Institut Polytechnique de Paris"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02743",children:"https://arxiv.org/pdf/2511.02743"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents EPaxos*, a simplified and corrected variant of Egalitarian Paxos that eliminates the leader dependency in distributed consensus protocols. The key contribution is a simpler failure-recovery algorithm that has been rigorously proven correct, while generalizing the protocol to cover optimal failure thresholds. The work addresses complexity and correctness issues in the original Egalitarian Paxos while maintaining its leaderless advantages."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 20'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251105] Tool Zero: Training Tool-Augmented LLMs via Pure RL from Scratch ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01934",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251105] Shorter but not Worse: Frugal Reasoning via Easy Samples as Length Regularizers in Math RLVR ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01937",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251105] Automated Reward Design for Gran Turismo ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02094",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251105] Training Proactive and Personalized LLM Agents ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02208",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251105] Adaptive Cooperative Transmission Design for Ultra-Reliable Low-Latency Communications via Deep Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02216",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251105] Optimizing Multi-Lane Intersection Performance in Mixed Autonomy Environments ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02217",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251105] Structural Plasticity as Active Inference: A Biologically-Inspired Architecture for Homeostatic Control ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02241",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251105] Reinforcement learning based data assimilation for unknown state model ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02286",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251105] Unlocking the Power of Multi-Agent LLM for Reasoning: From Lazy Agents to Deliberation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02303",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251105] Automata-Conditioned Cooperative Multi-Agent Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02304",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251105] Large-scale automatic carbon ion treatment planning for head and neck cancers via parallel multi-agent reinforcement learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02314",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251105] Auditable-choice reframing unlocks RL-based verification for open-ended tasks ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02463",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251105] An End-to-End Learning Approach for Solving Capacitated Location-Routing Problems ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02525",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251105] Adaptive Neighborhood-Constrained Q Learning for Offline Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02567",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251105] Directional-Clamp PPO ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02577",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251105] Adaptive GR(1) Specification Repair for Liveness-Preserving Shielding in Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02605",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251105] Natural-gas storage modelling by deep reinforcement learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02646",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251105] Curriculum Design for Trajectory-Constrained Agent: Compressing Chain-of-Thought Tokens in LLMs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02690",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251105] From Solo to Symphony: Orchestrating Multi-Agent Collaboration with Single-Agent Demos ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02762",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251105] RL-Aided Cognitive ISAC: Robust Detection and Sensing-Communication Trade-offs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02672",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 15'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251105] Before the Clinic: Transparent and Operable Design Principles for Healthcare AI ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01902",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251105] DeepContour: A Hybrid Deep Learning Framework for Accelerating Generalized Eigenvalue Problem Solving via Efficient Contour Design ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01927",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251105] TapOut: A Bandit-Based Approach to Dynamic Speculative Decoding ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02017",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251105] Beyond Static Cutoffs: One-Shot Dynamic Thresholding for Diffusion Language Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02077",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251105] Optimal-Agent-Selection: State-Aware Routing Framework for Efficient Multi-Agent Collaboration ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02200",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251105] Gradient-Variation Online Adaptivity for Accelerated Optimization with H\xf6lder Smoothness ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02276",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251105] FP8-Flow-MoE: A Casting-Free FP8 Recipe without Double Quantization Error ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02302",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251105] Large-scale automatic carbon ion treatment planning for head and neck cancers via parallel multi-agent reinforcement learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02314",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251105] RoME: Domain-Robust Mixture-of-Experts for MILP Solution Prediction across Domains ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02331",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251105] NOWS: Neural Operator Warm Starts for Accelerating Iterative Solvers ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02481",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251105] Nesterov-Accelerated Robust Federated Learning Over Byzantine Adversaries ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02657",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251105] Curriculum Design for Trajectory-Constrained Agent: Compressing Chain-of-Thought Tokens in LLMs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02690",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251105] ConMeZO: Adaptive Descent-Direction Sampling for Gradient-Free Finetuning of Large Language Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02757",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251105] Improving Bayesian inference in PTA data analysis: importance nested sampling with Normalizing Flows ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.01958",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251105] Accelerated Frank-Wolfe Algorithms: Complementarity Conditions and Sparsity ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02821",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-11-06",children:"2025-11-06"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"cs.DC total: 7"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251106] SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [KV cache compression, static graphs, continuous batching, tensor parallelism, sparse KV attention]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Jonathan Li, Nasim Farahini, Evgenii Iuliugin, Magnus Vesterlund, Christian Haggstrom, Guangtao Wang, Shubhangi Upasani, Ayush Sachdeva, Rui Li, Faline Fu, Chen Wu, Ayesha Siddiqua, John Long, Tuowen Zhao, Matheen Musaddiq, Hakan Zeffer, Yun Du, Mingran Wang, Qinghua Li, Bo Li, Urmish Thakker, Raghu Prabhakar"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," SambaNova Systems, Cartesia AI"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03092",children:"https://arxiv.org/pdf/2511.03092"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," SnapStream introduces a KV cache compression method that reduces on-chip memory usage by 4x while maintaining model accuracy in long sequence decoding. It addresses deployment challenges in production inference systems with static graphs and continuous batching. The method demonstrates minimal accuracy degradation on benchmarks while achieving high throughput in real production settings."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251106] Stone Duality Proofs for Colorless Distributed Computability Theorems"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed computing], [spectral spaces, Stone duality, simplicial complexes, projective limit, Alexandrov topology]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Cameron Calk, Emmanuel Godard"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Aix-Marseille University, CNRS"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03609",children:"https://arxiv.org/pdf/2511.03609"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces a new topological approach using spectral spaces to model distributed computations with round-based full-information adversaries. The main result shows that colorless tasks are solvable against compact adversaries if and only if there exists a compatible spectral map between the limit object and output space. This provides a unified topological framework that explains why colored and uncolored distributed models have equivalent computability power."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251106] A General Input-Dependent Colorless Computability Theorem and Applications to Core-Dependent Adversaries"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed computing], [topological methods, geometric simplicial complex, colorless tasks, k-set agreement, condition-based adversaries, core-resilient adversaries]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yannis Coutouly, Emmanuel Godard"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Aix-Marseille University, CNRS LIS UMR7020"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03662",children:"https://arxiv.org/pdf/2511.03662"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper uses topological and geometric methods to extend computability theorems for distributed computing tasks. It generalizes previous results to input-dependent adversaries and shows that core-resilient adversaries have equivalent computability power regardless of crash timing. The main conclusion provides a complete characterization of when k-Set Agreement is solvable under condition-based, core-dependent adversaries."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251106] Investigating the Impact of Isolation on Synchronized Benchmarks"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [cloud benchmarking], [duet benchmarking, cgroups, CPU pinning, Docker containers, Firecracker MicroVMs, process isolation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Nils Japke, Furat Hamdan, Diana Baumann, David Bermbach"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," TU Berlin"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03533",children:"https://arxiv.org/pdf/2511.03533"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper evaluates different isolation strategies (cgroups/CPU pinning, Docker containers, and Firecracker MicroVMs) for synchronized duet benchmarking in cloud environments. The study found that process isolation generally reduces false positives in performance measurements, except for Docker containers which showed higher susceptibility to performance degradation despite using similar underlying isolation mechanisms. The authors recommend using process isolation for synchronized workloads but advise against Docker containers for this purpose."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251106] Harvesting energy consumption on European HPC systems: Sharing Experience from the CEEC project"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [HPC energy efficiency], [energy-to-solution, time-to-solution, mixed-precision, CFD applications, accelerators]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Kajol Kulkarni, Samuel Kemmler, Anna Schwarz, Gulcin Gedik, Yanxiang Chen, Dimitrios Papageorgiou, Ioannis Kavroulakis, Roman Iakymchuk"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Friedrich-Alexander-University Erlangen-N\xfcrnberg, Bundesanstalt f\xfcr Materialforschung und -pr\xfcfung, University of Stuttgart, Ume\xe5 University, Aristotle University of Thessaloniki, Uppsala University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03029",children:"https://arxiv.org/pdf/2511.03029"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper analyzes energy consumption across European HPC systems using case studies with CFD applications and evaluates energy-to-solution and time-to-solution metrics. The study demonstrates that accelerators and mixed-precision techniques significantly reduce energy consumption while maintaining computational accuracy. The authors advocate for improved energy measurement practices to promote sustainable exascale computing."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251106] Characterising Global Platforms: Centralised, Decentralised, Federated, and Grassroots"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed systems], [multiagent systems, atomic transactions, essential agents, protocol specification]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Ehud Shapiro"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," London School of Economics and Political Science, Weizmann Institute of Science"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03286",children:"https://arxiv.org/pdf/2511.03286"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a formal framework using atomic transactions-based multiagent transition systems to characterize global digital platforms. It introduces the concept of essential agents and shows that their cardinality partitions platforms into four distinct classes: centralized, decentralized, federated, and grassroots. This provides the first mathematical framework for classifying any global platform based on its architectural dependencies."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251106] UMDAM: A Unified Data Layout and DRAM Address Mapping for Heterogenous NPU-PIM"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [unified data layout, DRAM address mapping, column-major tile-based layout, processing-in-memory, NPU-PIM co-execution]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Hai Huang, Xuhong Qiang, Weisheng Zhao, Chenchen Liu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Beihang University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03293",children:"https://arxiv.org/pdf/2511.03293"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes UMDAM, a unified memory-affinity data layout and DRAM address mapping scheme for NPU-PIM co-execution systems. It uses a column-major tile-based layout and configurable DRAM mapping to resolve data layout mismatches without extra memory overhead. Evaluations show UMDAM improves LLM inference efficiency with up to 3.0\xd7 faster time-to-first-token and 2.18\xd7 faster time-to-last-token on edge devices."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 22'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251106] Shrinking the Variance: Shrinkage Baselines for Reinforcement Learning with Verifiable Rewards ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03710",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251106] Periodic Skill Discovery ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03187",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251106] Learning-based Cooperative Robotic Paper Wrapping: A Unified Control Policy with Residual Force Control ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03181",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251106] Adaptable Hindsight Experience Replay for Search-Based Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03405",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251106] Scaling Multi-Agent Environment Co-Design with Diffusion Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03100",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251106] Tensor-Efficient High-Dimensional Q-learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03595",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251106] Value of Information-Enhanced Exploration in Bootstrapped DQN ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02969",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251106] Digital Twin-Driven Pavement Health Monitoring and Maintenance Optimization Using Graph Neural Networks ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.02957",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251106] Optimizing Earth-Moon Transfer and Cislunar Navigation: Integrating Low-Energy Trajectories, AI Techniques and GNSS-R Technologies ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03173",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251106] Climate Adaptation with Reinforcement Learning: Economic vs. Quality of Life Adaptation Pathways ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03243",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251106] PerfDojo: Automated ML Library Generation for Heterogeneous Architectures ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03586",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251106] Reinforcement Learning Using known Invariances ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03473",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251106] Outbidding and Outbluffing Elite Humans: Mastering Liar's Poker via Self-Play and Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03724",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251106] Incorporating Quality of Life in Climate Adaptation Planning via Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03238",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251106] Multi-Objective Adaptive Rate Limiting in Microservices Using Deep Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03279",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251106] Learning Without Critics? Revisiting GRPO in Classical Reinforcement Learning Environments ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03527",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251106] Leveraging Discrete Function Decomposability for Scientific Design ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03032",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251106] Going Beyond Expert Performance via Deep Implicit Imitation Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03616",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251106] Behavior-Adaptive Q-Learning: A Unifying Framework for Offline-to-Online RL ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03695",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251106] Towards Formalizing Reinforcement Learning Theory ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03618",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251106] AnaFlow: Agentic LLM-based Workflow for Reasoning-Driven Explainable and Sample-Efficient Analog Circuit Sizing ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03697",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251106] DQN Performance with Epsilon Greedy Policies and Prioritized Experience Replay ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03670",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 7'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251106] Provable Accelerated Bayesian Optimization with Knowledge Transfer ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03125",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251106] A unified physics-informed generative operator framework for general inverse problems ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03241",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251106] PerfDojo: Automated ML Library Generation for Heterogeneous Architectures ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03586",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251106] Online Learning to Rank under Corruption: A Robust Cascading Bandits Approach ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03074",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251106] Going Beyond Expert Performance via Deep Implicit Imitation Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03616",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251106] Behavior-Adaptive Q-Learning: A Unifying Framework for Offline-to-Online RL ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03695",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251106] AnaFlow: Agentic LLM-based Workflow for Reasoning-Driven Explainable and Sample-Efficient Analog Circuit Sizing ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03697",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-11-07",children:"2025-11-07"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"cs.DC total: 9"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251107] A Reinforced Evolution-Based Approach to Multi-Resource Load Balancing"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [ai], [optimization algorithms], [genetic algorithms, migration operator, random genetic drift, combinatorial optimization, load balancing]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Leszek Sliwko"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Wroc\u0142aw University of Technology"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.04183",children:"https://arxiv.org/pdf/2511.04183"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents a reinforced genetic algorithm approach for multi-resource load balancing optimization problems. The authors developed modifications to classical genetic routines, including a migration operator inspired by biological random genetic drift, to handle strict feasibility constraints. The proposed method addresses NP-hard optimization problems where traditional evolutionary schemas were ineffective."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251107] Stochastic Modeling for Energy-Efficient Edge Infrastructure"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [Markov Chains, stochastic modeling, Monte Carlo simulation, predictive power scaling, sensitivity analysis]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Fabio Diniz Rossi"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Federal Institute Farroupilha"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03941",children:"https://arxiv.org/pdf/2511.03941"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a stochastic modeling approach using Markov Chains to analyze power state transitions in Edge Computing. The research demonstrates that AI-driven predictive power scaling outperforms conventional reactive methods by minimizing unnecessary transitions and optimizing workload distribution. Experimental results show significant improvements in energy efficiency and system responsiveness across heterogeneous edge nodes."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251107] Enabling Dynamic Sparsity in Quantized LLM Inference"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [dynamic sparsity, quantization, zigzag-patterned layout, GEMV kernel, sparse indices gathering]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Rongxiang Wang, Kangyuan Shu, Felix Xiaozhu Lin"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Virginia, Zoom Communications Inc"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.04477",children:"https://arxiv.org/pdf/2511.04477"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes techniques to enable dynamic sparse inference under low-bit quantization for LLMs, featuring a zigzag-patterned quantization layout, specialized GEMV kernel, and efficient sparse indices gathering mechanism. The approach achieves up to 1.55\xd7 faster decoding throughput while maintaining comparable accuracy to dense quantized inference. This demonstrates that structured sparsity and quantization can effectively coexist on commodity GPUs."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251107] A New Probabilistic Mobile Byzantine Failure Model for Self-Protecting Systems"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [fault-tolerance], [Mobile Byzantine Failure, MAPE-K architecture, Markov process, self-protection, self-reconfiguration]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Silvia Bonomi, Giovanni Farina, Roy Friedman, Eviatar B. Procaccia, Sebastien Tixeuil"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Sapienza University of Rome, Niccol\xf2 Cusano University, Technion, Sorbonne University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.04523",children:"https://arxiv.org/pdf/2511.04523"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a new probabilistic Mobile Byzantine Failure model that integrates with the Analysis component of MAPE-K based self-protecting systems. The model mathematically analyzes system behavior under attack-recovery dynamics using Markov processes and provides simulation results. The approach enables distributed systems to autonomously manage security threats through dynamic self-protection and reconfiguration strategies."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251107] Resolving Conflicts with Grace: Dynamically Concurrent Universality"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed computing], [dynamic concurrency, universal construction, consensus, synchronization, conflict detection]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Petr Kuznetsov, Nathan Josia Schrodt"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," T\xe9l\xe9com Paris, Institut Polytechnique de Paris, Technical University of Darmstadt"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.04631",children:"https://arxiv.org/pdf/2511.04631"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces dynamic concurrency, a method that uses strong synchronization primitives only when operations must arbitrate based on the current system state. It presents a dynamically concurrent universal construction that avoids consensus-based synchronization when operations don't conflict in the current state. The approach improves scalability by reducing unnecessary synchronization in distributed systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251107] Parallel Spawning Strategies for Dynamic-Aware MPI Applications"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [HPC resource management], [parallel spawning, dynamic resource management, MPI malleability, process redistribution]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Iker Mart\xedn-\xc1lvarez, Jos\xe9 I. Aliaga, Maribel Castillo, Sergio Iserte"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Universitat Jaume I, Barcelona Supercomputing Center"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.04268",children:"https://arxiv.org/pdf/2511.04268"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a novel parallel spawning strategy for MPI applications where all processes cooperate in spawning before redistribution, reducing execution time and enabling better resource adaptation. The method overcomes limitations of existing approaches by allowing full release of unneeded processes during shrink operations while maintaining competitive expansion performance. The strategy achieves up to 1.25\xd7 overhead for expansion and reduces shrink operation costs by at least 20\xd7, validated on both homogeneous and heterogeneous systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251107] OMPILOT: Harnessing Transformer Models for Auto Parallelization to Shared Memory Computing Paradigms"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [transformer, encoder-decoder, OpenMP, parallelization, function-level translation, OMPBLEU]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Arijit Bhattacharjee, Ali TehraniJamsaz, Le Chen, Niranjan Hasabnis, Mihai Capota, Nesreen Ahmed, Ali Jannesari"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Iowa State University, Mako AI, Argonne National Lab, Code Metal, Intel Labs, Cisco Outshift"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03866",children:"https://arxiv.org/pdf/2511.03866"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces OMPILOT, a domain-specific encoder-decoder transformer model that translates C++ code to OpenMP for shared-memory parallelization. It uses custom pre-training objectives combining unsupervised and supervised learning to operate at function level rather than just loop level. The approach demonstrates improved robustness in code translation and introduces OMPBLEU, a specialized metric for evaluating OpenMP parallel constructs."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251107] Universal Quantum Simulation of 50 Qubits on Europe`s First Exascale Supercomputer Harnessing Its Heterogeneous CPU-GPU Architecture"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [quantum computing simulation], [CPU-GPU heterogeneous architecture, adaptive data encoding, network traffic optimization, LPDDR5 memory, high-bandwidth interconnects]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Hans De Raedt, Jiri Kraus, Andreas Herten, Vrinda Mehta, Mathis Bode, Markus Hrywniak, Kristel Michielsen, Thomas Lippert"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," J\xfclich Supercomputing Centre, NVIDIA"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03359",children:"https://arxiv.org/pdf/2511.03359"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper presents JUQCS-50, a quantum computer simulator that achieves 50-qubit simulations using three key innovations: extending memory beyond GPU limits through CPU-GPU interconnects and LPDDR5 memory, adaptive data encoding to reduce memory footprint, and network traffic optimization. These methods resulted in an 11.4-fold speedup over previous 48-qubit simulations, demonstrating efficient large-scale quantum circuit simulation on Europe's first exascale supercomputer."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251107] Scalable Domain-decomposed Monte Carlo Neutral Transport for Nuclear Fusion"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [high-performance computing], [domain decomposition, Monte Carlo, parallel algorithms, strong scaling, weak scaling]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Oskar Lappi, Huw Leggate, Yannick Marandet, Jan \xc5str\xf6m, Keijo Heljanko, Dmitriy V. Borodin"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Helsinki, Dublin City University, Aix-Marseille University, CSC \u2013 IT Center for Science Ltd., Forschungszentrum J\xfclich GmbH, Helsinki Institute for Information Technology"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.04489",children:"https://arxiv.org/pdf/2511.04489"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents a domain-decomposed Monte Carlo algorithm implemented in a new open-source code called Eiron for nuclear fusion simulations. The domain decomposition approach outperforms existing parallel algorithms in scaling tests and enables simulations that were previously impossible due to memory constraints. The method achieved superlinear strong scaling on grids exceeding L3 cache size and scaled efficiently to 16,384 cores."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 13'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251107] From Static to Dynamic: Enhancing Offline-to-Online Reinforcement Learning via Energy-Guided Diffusion Stratification ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03828",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251107] RLoop: An Self-Improving Framework for Reinforcement Learning with Iterative Policy Initialization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.04285",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251107] GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.04307",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251107] Scaling Agent Learning via Experience Synthesis ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03773",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251107] The Peril of Preference: Why GRPO fails on Ordinal Rewards ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.04439",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251107] Fitting Reinforcement Learning Model to Behavioral Data under Bandits ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.04454",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251107] Opus: A Quantitative Framework for Workflow Evaluation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.04220",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251107] Environment Agnostic Goal-Conditioning, A Study of Reward-Free Autonomous Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.04598",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251107] Shared Spatial Memory Through Predictive Coding ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.04235",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251107] Forgetting is Everywhere ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.04666",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251107] End-to-End Reinforcement Learning of Koopman Models for eNMPC of an Air Separation Unit ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.04522",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251107] RLHF: A comprehensive Survey for Cultural, Multimodal and Low Latency Alignment Methods ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03939",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251107] Exchange Policy Optimization Algorithm for Semi-Infinite Safe Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.04147",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 10'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251107] Machine Learning for Electron-Scale Turbulence Modeling in W7-X ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.04567",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251107] DecoHD: Decomposed Hyperdimensional Classification under Extreme Memory Budgets ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03911",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251107] Data-driven uncertainty-aware seakeeping prediction of the Delft 372 catamaran using ensemble Hankel dynamic mode decomposition ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.04461",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251107] GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.04307",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251107] One Size Does Not Fit All: Architecture-Aware Adaptive Batch Scheduling with DEBA ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03809",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251107] ARETE: an R package for Automated REtrieval from TExt with large language models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.04573",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251107] OpenMENA: An Open-Source Memristor Interfacing and Compute Board for Neuromorphic Edge-AI Applications ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03747",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251107] A convolutional neural network deep learning method for model class selection ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03743",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251107] Hybrid Fuzzing with LLM-Guided Input Mutation and Semantic Feedback ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.03995",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251107] DartQuant: Efficient Rotational Distribution Calibration for LLM Quantization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.04063",children:"link"})]}),"\n"]})]})}function d(i={}){const{wrapper:e}={...(0,a.R)(),...i.components};return e?(0,s.jsx)(e,{...i,children:(0,s.jsx)(h,{...i})}):h(i)}},8453:(i,e,n)=>{n.d(e,{R:()=>t,x:()=>o});var r=n(6540);const s={},a=r.createContext(s);function t(i){const e=r.useContext(a);return r.useMemo(function(){return"function"==typeof i?i(e):{...e,...i}},[e,i])}function o(i){let e;return e=i.disableParentContext?"function"==typeof i.components?i.components(s):i.components||s:t(i.components),r.createElement(a.Provider,{value:e},i.children)}}}]);