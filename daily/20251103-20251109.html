<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/20251103-20251109" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251103-20251109 | Recommend System Note</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://xiezilailai.github.io/Recommend-System-Note/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://xiezilailai.github.io/Recommend-System-Note/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://xiezilailai.github.io/Recommend-System-Note/daily/20251103-20251109"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251103-20251109 | Recommend System Note"><meta data-rh="true" name="description" content="2025-11-03"><meta data-rh="true" property="og:description" content="2025-11-03"><link data-rh="true" rel="icon" href="/Recommend-System-Note/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://xiezilailai.github.io/Recommend-System-Note/daily/20251103-20251109"><link data-rh="true" rel="alternate" href="https://xiezilailai.github.io/Recommend-System-Note/daily/20251103-20251109" hreflang="en"><link data-rh="true" rel="alternate" href="https://xiezilailai.github.io/Recommend-System-Note/daily/20251103-20251109" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://xiezilailai.github.io/Recommend-System-Note/category/daily"},{"@type":"ListItem","position":2,"name":"20251103-20251109","item":"https://xiezilailai.github.io/Recommend-System-Note/daily/20251103-20251109"}]}</script><link rel="stylesheet" href="/Recommend-System-Note/assets/css/styles.2a9d613c.css">
<script src="/Recommend-System-Note/assets/js/runtime~main.7fa2517e.js" defer="defer"></script>
<script src="/Recommend-System-Note/assets/js/main.c40d57e8.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/Recommend-System-Note/img/favicon.ico"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/Recommend-System-Note/"><div class="navbar__logo"><img src="/Recommend-System-Note/img/favicon.ico" alt="Recommend System Note" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/Recommend-System-Note/img/favicon.ico" alt="Recommend System Note" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Recommend System Note</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/xiezilailai" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Recommend-System-Note/intro"><span title="Introduction" class="linkLabel_WmDU">Introduction</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/Recommend-System-Note/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/Recommend-System-Note/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/Recommend-System-Note/daily/20251103-20251109"><span title="20251103-20251109" class="linkLabel_WmDU">20251103-20251109</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/Recommend-System-Note/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/Recommend-System-Note/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/Recommend-System-Note/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251103-20251109</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251103-20251109</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-11-03">2025-11-03<a href="#2025-11-03" class="hash-link" aria-label="Direct link to 2025-11-03" title="Direct link to 2025-11-03" translate="no">​</a></h2>
<p><strong>cs.DC total: 11</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251103] Glia: A Human-Inspired AI for Automated Systems Design and Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [multi-agent workflow, large language models, interpretable designs, request routing, scheduling, auto-scaling]</li>
<li class=""><strong>authors:</strong> Pouya Hamadanian, Pantea Karimi, Arash Nasr-Esfahany, Kimia Noorbakhsh, Joseph Chandler, Ali ParandehGheibi, Mohammad Alizadeh, Hari Balakrishnan</li>
<li class=""><strong>institution:</strong> MIT CSAIL</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2510.27176" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2510.27176</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Glia uses a human-inspired multi-agent architecture with LLMs that specialize in reasoning, experimentation, and analysis to automate systems design. It generates interpretable algorithms for distributed GPU clusters performing LLM inference. The system produces human-expert level designs in less time while providing novel insights into workload behavior.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251103] Dynamic Service Scheduling and Resource Management in Energy-Harvesting Multi-access Edge Computing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [edge computing], [energy harvesting, online scheduling, DAG, DVFS, service module migration]</li>
<li class=""><strong>authors:</strong> Shuyi Chen, Panagiotis Oikonomou, Zhengchang Hua, Nikos Tziritas, Karim Djemame, Nan Zhang, Georgios Theodoropoulos</li>
<li class=""><strong>institution:</strong> University of Leeds, Southern University of Science and Technology, University of Thessaly</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2510.27317" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2510.27317</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an online strategy for energy-harvesting MEC systems that dynamically schedules computational tasks with dependencies and manages energy consumption through server frequency scaling and service module migration. The method efficiently utilizes harvested energy while maintaining low service latency, as demonstrated through experiments with real-world datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251103] Synergistic Tensor and Pipeline Parallelism</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [tensor parallelism, pipeline parallelism, hybrid model parallelism, scheduling, communication optimization]</li>
<li class=""><strong>authors:</strong> Mengshi Qi, Jiaxuan Peng, Jie Zhang, Juan Zhu, Yong Li, Huadong Ma</li>
<li class=""><strong>institution:</strong> Beijing University of Posts and Telecommunications</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2510.27257" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2510.27257</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a synergistic tensor and pipeline parallelism schedule that decouples forward and backward passes into fine-grained computation units and braids them into a composite sequence. This approach simultaneously reduces both tensor parallelism communication bubbles and pipeline parallelism synchronization bubbles. Experimental results show throughput improvements of up to 12% for LLMs and 16% for MLLMs compared to existing methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251103] A Cloud-Based Spatio-Temporal GNN-Transformer Hybrid Model for Traffic Flow Forecasting with External Feature Integration</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Spatio-Temporal Graph Neural Networks, Transformer, Cloud Computing, Feature Fusion]</li>
<li class=""><strong>authors:</strong> Zhuo Zheng, Lingran Meng, Ziyu Lin</li>
<li class=""><strong>institution:</strong> Nanchang University, University of Washington, Google</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2510.27039" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2510.27039</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a cloud-based hybrid model combining Spatio-Temporal Graph Neural Networks with Transformer architecture for traffic flow forecasting, integrating external features like weather and holidays. The model leverages GNNs for spatial correlations and Transformers for long-term temporal dependencies. Experimental results show it outperforms baseline methods with RMSE of 17.92 and MAE of 10.53, demonstrating effectiveness for intelligent transportation systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251103] SERFLOW: A Cross-Service Cost Optimization Framework for SLO-Aware Dynamic ML Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [dynamic offloading, FaaS, IaaS, stage-specific resource provisioning, adaptive load balancing, serverless functions, cost optimization]</li>
<li class=""><strong>authors:</strong> Zongshun Zhang, Ibrahim Matta</li>
<li class=""><strong>institution:</strong> Boston University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2510.27182" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2510.27182</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SERFLOW proposes a cross-service optimization framework that dynamically offloads ML model partitions between IaaS VMs and FaaS serverless functions using stage-specific resource provisioning and adaptive load balancing. The framework accounts for variable request exit rates across model stages and real-world factors like VM cold starts. This approach reduces cloud costs by over 23% while maintaining service level objectives for dynamic ML inference workloads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251103] Secure Communication in the Presence of an RIS-Enhanced Eavesdropper in MIMO Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [wireless communication security], [reconfigurable intelligent surface, MIMO, singular value decomposition, bit-flipping, physical layer security]</li>
<li class=""><strong>authors:</strong> Gaoyuan Zhang, Ruisong Si, Boyuan Li, Zijian Li, Baofeng Ji, Chenqi Zhu, Tony Q.S. Quek</li>
<li class=""><strong>institution:</strong> Henan University of Science and Technology, Zhengzhou University, Dalian Maritime University, Singapore University of Technology and Design</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2510.27147" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2510.27147</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a secure communication scheme using random bit-flipping and SVD-based precoding to protect MIMO wireless networks against RIS-enhanced eavesdroppers. The method minimizes mutual information between secret messages and eavesdropper data without requiring full channel state information. Results demonstrate the scheme&#x27;s effectiveness and robustness across various attacking scenarios.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251103] FlowMesh: A Service Fabric for Composable LLM Workflows</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [workflow optimization, multi-tenant service fabric, content-addressable store, operator lineage, batch scheduling]</li>
<li class=""><strong>authors:</strong> Junyi Shen, Noppanat Wadlom, Lingfeng Zhou, Dequan Wang, Xu Miao, Lei Fang, Yao Lu</li>
<li class=""><strong>institution:</strong> National University of Singapore, Shanghai Jiao Tong University, DataCanvas</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2510.26913" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2510.26913</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FlowMesh is a service fabric that decomposes LLM workflows into fine-grained operators and optimizes execution across multiple users through deduplication and intelligent batching. It uses a global control plane for scheduling and stateless workers with content-addressable storage for elasticity. The system achieves up to 3.8× cost reduction and maintains efficiency under dynamic conditions compared to baseline solutions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251103] ML-Based Optimum Sub-system Size Heuristic for the GPU Implementation of the Tridiagonal Partition Method</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [k-nearest neighbors, parallel partition algorithm, CUDA implementation, tridiagonal matrices, recursive algorithm]</li>
<li class=""><strong>authors:</strong> Milena Veneva</li>
<li class=""><strong>institution:</strong> RIKEN Center for Computational Science</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2510.27351" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2510.27351</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper develops a machine learning-based heuristic using k-nearest neighbors classification to determine optimal sub-system sizes for GPU implementations of parallel partition algorithms solving tridiagonal systems. The method was empirically validated through computational experiments on various SLAE sizes and extended to recursive variants. Results showed the algorithm performed acceptably well in predicting optimal parameters for GPU performance optimization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251103] A Digital Twin-based Multi-Agent Reinforcement Learning Framework for Vehicle-to-Grid Coordination</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Digital Twin, Multi-Agent Reinforcement Learning, MADDPG, Vehicle-to-Grid, privacy-preserving]</li>
<li class=""><strong>authors:</strong> Zhengchang Hua, Panagiotis Oikonomou, Karim Djemame, Nikos Tziritas, Georgios Theodoropoulos</li>
<li class=""><strong>institution:</strong> Southern University of Science and Technology, University of Leeds, University of Thessaly, Research Institute of Trustworthy Autonomous Systems</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2510.27289" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2510.27289</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes DT-MADDPG, a hybrid framework combining multi-agent reinforcement learning with collaborative Digital Twins for Vehicle-to-Grid coordination. The method enhances the centralized critic with a predictive global model built from privacy-preserving data shared by individual Digital Twins. Experimental results show the approach achieves coordination performance comparable to standard MADDPG while providing significant advantages in data privacy and architectural decentralization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251103] Byzantine Attacks in RIS-Enhanced Cooperative Spectrum Sensing: A Decision Fusion Perspective</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [wireless security], [cooperative spectrum sensing, Byzantine attacks, decision fusion, reconfigurable intelligent surface, decode-and-forward relay]</li>
<li class=""><strong>authors:</strong> Gaoyuan Zhang, Gaolei Song, Boyuan Li, Zijian Li, Baofeng Ji, Ruijuan Zheng, Guoqiang Zheng, Tony Q.S. Quek</li>
<li class=""><strong>institution:</strong> Henan University of Science and Technology, Zhengzhou University, Dalian Maritime University, Singapore University of Technology and Design</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2510.27175" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2510.27175</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper investigates Byzantine attacks in RIS-enhanced cooperative spectrum sensing systems from a decision fusion perspective. The authors develop channel-aware attack strategies and show that optimal Byzantine attacks don&#x27;t require global instantaneous channel state information and effectiveness primarily depends on the fraction of compromised nodes rather than channel dynamics. Their counterintuitive findings demonstrate that heavy reliance on global ICSI and decision fusion rules can be successfully relaxed for practical implementation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251103] RDMA Point-to-Point Communication for LLM Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [RDMA, point-to-point communication, TransferEngine, one-sided WriteImm, ImmCounter, disaggregated inference, Mixture-of-Experts]</li>
<li class=""><strong>authors:</strong> Nandor Licker, Kevin Hu, Vladimir Zaytsev, Lequn Chen</li>
<li class=""><strong>institution:</strong> Perplexity AI</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2510.27656" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2510.27656</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents TransferEngine, a portable RDMA communication library that provides uniform point-to-point communication across different network hardware. It enables high-performance communication for LLM systems through one-sided WriteImm operations with ImmCounter completion notification. The system achieves 400 Gbps throughput and demonstrates practical benefits in disaggregated inference, RL fine-tuning, and MoE routing while avoiding vendor lock-in.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 18</strong></p>
<ul>
<li class="">[arXiv251103] A Framework for Fair Evaluation of Variance-Aware Bandit Algorithms <a href="https://arxiv.org/pdf/2510.27001" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251103] Limits of Generalization in RLVR: Two Case Studies in Mathematical Reasoning <a href="https://arxiv.org/pdf/2510.27044" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251103] Reinforcement Learning for Long-Horizon Unordered Tasks: From Boolean to Coupled Reward Machines <a href="https://arxiv.org/pdf/2510.27329" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251103] e1: Learning Adaptive Control of Reasoning Effort <a href="https://arxiv.org/pdf/2510.27042" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251103] GUI-Rise: Structured Reasoning and History Summarization for GUI Navigation <a href="https://arxiv.org/pdf/2510.27210" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251103] Reasoning Models Sometimes Output Illegible Chains of Thought <a href="https://arxiv.org/pdf/2510.27338" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251103] MedCalc-Eval and MedCalc-Env: Advancing Medical Calculation Capabilities of Large Language Models <a href="https://arxiv.org/pdf/2510.27267" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251103] Towards Understanding Self-play for LLM Reasoning <a href="https://arxiv.org/pdf/2510.27072" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251103] Do Vision-Language Models Measure Up? Benchmarking Visual Measurement Reading with MeasureBench <a href="https://arxiv.org/pdf/2510.26865" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251103] AURA: A Reinforcement Learning Framework for AI-Driven Adaptive Conversational Surveys <a href="https://arxiv.org/pdf/2510.27126" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251103] Realistic pedestrian-driver interaction modelling using multi-agent RL with human perceptual-motor constraints <a href="https://arxiv.org/pdf/2510.27383" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251103] DeepCompress: A Dual Reward Strategy for Dynamically Exploring and Compressing Reasoning Chains <a href="https://arxiv.org/pdf/2510.27419" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251103] Learning Soft Robotic Dynamics with Active Exploration <a href="https://arxiv.org/pdf/2510.27428" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251103] VCORE: Variance-Controlled Optimization-based Reweighting for Chain-of-Thought Supervision <a href="https://arxiv.org/pdf/2510.27462" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251103] Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised Reinforcement Learning <a href="https://arxiv.org/pdf/2510.27606" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251103] Challenges in Credit Assignment for Multi-Agent Reinforcement Learning in Open Agent Systems <a href="https://arxiv.org/pdf/2510.27659" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251103] Reinforcement Learning for Accelerator Beamline Control: a simulation-based approach <a href="https://arxiv.org/pdf/2510.26805" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251103] When AI Trading Agents Compete: Adverse Selection of Meta-Orders by Reinforcement Learning-Based Market Making <a href="https://arxiv.org/pdf/2510.27334" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 12</strong></p>
<ul>
<li class="">[arXiv251103] CAS-Spec: Cascade Adaptive Self-Speculative Decoding for On-the-Fly Lossless Inference Acceleration of LLMs <a href="https://arxiv.org/pdf/2510.26843" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251103] H2-Cache: A Novel Hierarchical Dual-Stage Cache for High-Performance Acceleration of Generative Diffusion Models <a href="https://arxiv.org/pdf/2510.27171" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251103] Sparse Model Inversion: Efficient Inversion of Vision Transformers for Data-Free Applications <a href="https://arxiv.org/pdf/2510.27186" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251103] MLPerf Automotive <a href="https://arxiv.org/pdf/2510.27065" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251103] HiF-DTA: Hierarchical Feature Learning Network for Drug-Target Affinity Prediction <a href="https://arxiv.org/pdf/2510.27281" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251103] Jasmine: A Simple, Performant and Scalable JAX-based World Modeling Codebase <a href="https://arxiv.org/pdf/2510.27002" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251103] FPS: Feedforward-based Parameter Selection For Efficient Fine-Tuning <a href="https://arxiv.org/pdf/2510.27359" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251103] Fine-Tuning Open Video Generators for Cinematic Scene Synthesis: A Small-Data Pipeline with LoRA and Wan2.1 I2V <a href="https://arxiv.org/pdf/2510.27364" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251103] FedMuon: Accelerating Federated Learning with Matrix Orthogonalization <a href="https://arxiv.org/pdf/2510.27403" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251103] InnovatorBench: Evaluating Agents&#x27; Ability to Conduct Innovative LLM Research <a href="https://arxiv.org/pdf/2510.27598" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251103] Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation Models <a href="https://arxiv.org/pdf/2510.27629" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251103] Reinforcement Learning for Accelerator Beamline Control: a simulation-based approach <a href="https://arxiv.org/pdf/2510.26805" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-11-04">2025-11-04<a href="#2025-11-04" class="hash-link" aria-label="Direct link to 2025-11-04" title="Direct link to 2025-11-04" translate="no">​</a></h2>
<p><strong>cs.DC total: 25</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251104] Tetris: An SLA-aware Application Placement Strategy in the Edge-Cloud Continuum</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [edge-cloud resource management], [heuristic algorithm, SLA-aware placement, resource efficiency, latency-sensitive applications]</li>
<li class=""><strong>authors:</strong> Lucas Almeida, Maycon Peixoto</li>
<li class=""><strong>institution:</strong> Federal University of Bahia</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.00294" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.00294</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Tetris, a heuristic-based application placement strategy that prioritizes services based on SLA urgencies and resource efficiency in Edge-Cloud Continuum environments. The method reduces SLA violations by approximately 76% compared to baseline approaches, demonstrating improved Quality of Service for latency-sensitive applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251104] Fix: externalizing network I/O in serverless computing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [serverless computing], [externalized I/O, deterministic procedures, dataflow scheduling]</li>
<li class=""><strong>authors:</strong> Yuhan Deng, Akshay Srivatsan, Sebastian Ingino, Francis Chua, Yasmine Mitchell, Matthew Vilaysack, Keith Winstein</li>
<li class=""><strong>institution:</strong> Stanford University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.00205" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.00205</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a serverless computing system where computations are represented as deterministic procedures with explicit data dependencies, enabling the platform to externalize network I/O. By making data requirements explicit, the system allows better scheduling of tasks and network transfers to reduce starvation. The approach suggests shifting cloud computing from a &quot;pay-for-effort&quot; to a &quot;pay-for-results&quot; service model.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251104] AReaL-Hex: Accommodating Asynchronous RL Training over Heterogeneous GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [asynchronous RL training, heterogeneous GPU scheduling, mixed-integer linear programming, graph partitioning, data staleness bounds]</li>
<li class=""><strong>authors:</strong> Ran Yan, Youhe Jiang, Tianyuan Wu, Jiaxuan Gao, Zhiyu Mei, Wei Fu, Haohui Mai, Wei Wang, Yi Wu, Binhang Yuan</li>
<li class=""><strong>institution:</strong> HKUST, Tsinghua University, Ant Group</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.00796" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.00796</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AReaL-Hex is a heterogeneity-aware asynchronous RL training system that uses a two-phase scheduler with MILP optimization and graph partitioning to efficiently deploy RL training across heterogeneous GPUs. The system achieves up to 1.50× higher training throughput at the same budget and reduces training costs by up to 1.46× while maintaining the same throughput compared to homogeneous deployments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251104] Split Learning-Enabled Framework for Secure and Light-weight Internet of Medical Things Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [split learning, federated learning, game-theoretic optimization, image-based classification, malware detection]</li>
<li class=""><strong>authors:</strong> Siva Sai, Manish Prasad, Animesh Bhargava, Vinay Chamola, Rajkumar Buyya</li>
<li class=""><strong>institution:</strong> The University of Melbourne, BITS-Pilani</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.00336" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.00336</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a split learning framework for IoT malware detection that divides neural network training between clients and an edge server to reduce computational burden while maintaining data privacy. Experimental results show the framework outperforms federated learning methods with higher accuracy (+6.35%), faster convergence (+14.96%), and lower resource consumption (33.83%), establishing split learning as a scalable and secure paradigm for IoT security.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251104] Benchmarking Federated Learning Frameworks for Medical Imaging Deployment: A Comparative Study of NVIDIA FLARE, Flower, and Owkin Substra</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [federated learning, medical imaging, PathMNIST, model convergence, communication overhead, scalability assessment]</li>
<li class=""><strong>authors:</strong> Riya Gupta, Alexander Chowdhury, Sahil Nalawade</li>
<li class=""><strong>institution:</strong> Harvard T.H. Chan School of Public Health, Dana-Farber Cancer Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.00037" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.00037</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper benchmarks three federated learning frameworks (NVIDIA FLARE, Flower, and Owkin Substra) using the PathMNIST medical imaging dataset to evaluate performance, scalability, and deployment features. The study found that each framework has distinct strengths: NVIDIA FLARE excels in production scalability, Flower offers research flexibility, and Owkin Substra provides superior privacy compliance for healthcare applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251104] EPARA: Parallelizing Categorized AI Inference in Edge Clouds</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [task categorization, parallel inference, edge computing, distributed scheduling, resource allocation]</li>
<li class=""><strong>authors:</strong> Yubo Wang, Yubo Cui, Tuo Shi, Danyang Li, Wenxin Li, Lide Suo, Tao Wang, Xin Xie</li>
<li class=""><strong>institution:</strong> Tianjin University, Aalto University, Nankai University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.00603" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.00603</a></li>
<li class=""><strong>Simple LLM Summary:</strong> EPARA introduces a parallel AI inference framework that categorizes tasks based on latency sensitivity and GPU requirements to optimize resource allocation in edge clouds. The system employs task-categorized parallelism allocation, distributed request handling, and state-aware scheduling to improve serving capability. Experimental results show EPARA achieves up to 2.1× higher goodput compared to prior frameworks while adapting to various edge AI inference tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251104] LongCat-Flash-Omni Technical Report</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [Mixture-of-Experts, modality-decoupled parallelism, curriculum-inspired progressive training, multimodal perception, speech reconstruction]</li>
<li class=""><strong>authors:</strong> Meituan LongCat Team, Bairui Wang, Bayan, Bin Xiao, Bo Zhang, Bolin Rong, Borun Chen, Chang Wan, Chao Zhang, Chen Huang, Chen Chen, Chen Chen, Chengxu Yang, Chengzuo Yang, Cong Han, Dandan Peng, Delian Ruan, Detai Xin, Disong Wang, Dongchao Yang, Fanfan Liu, Fengjiao Chen, Fengyu Yang, Gan Dong, Gang Huang, Gang Xu, Guanglu Wan, Guoqiang Tan, Guoqiao Yu, Haibo Qiu, Hao Lu, Hongbo Liu, Hongyu Xiang, Jiaheng Wu, Jian Yang, Jiaxing Liu, Jing Huang, Jingang Wang, Jinrui Ding, Juchao Jiang, Jun Kuang, Jun Wang, Junhui Mei, Ke Ding, Kefeng Zhang, Lei Chen, Liang Shi, Limeng Qiao, Liming Zheng, Lin Ma, Liuyang Guo, Liya Ma, Luying Sun, Man Gao, Mengshen Zhu, Miao Cao, Minliang Lin, Nuo Xu, Peng Shi, Qi Zhang, Qian Fang, Qian Wang, Qian Yang, Quanxiu Wang, Rongxiang Weng, Rongxin Guo, Ruoxuan Liang, Senbin Yang, Shanbo Xu, Shanglin Lei, Shengze Ye, Shimin Chen, Shuaiqi Chen, Shujie Hu, Shuo Li, Siqi Yang, Siyu Xu, Siyu Ren, Song Li, Songxiang Liu, Tianhao Bai, Tianye Dai, Wei Hong, Wei Wang, Weixiao Zhao, Wengang Cao, Wenlong Zhu, Wenlong He, Xi Su, Xi Nan, Xiaohan Zhao, Xiaohao Wang, Xiaoyu Zhao, Xiaoyu Wang, Xiaoyu Li, Xin Pan, Xin Chen, Xiusong Sun, Xu Xiang, Xudong Xing</li>
<li class=""><strong>institution:</strong> Meituan</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.00279" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.00279</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LongCat-Flash-Omni is a 560B parameter omni-modal model that uses a Mixture-of-Experts architecture with modality-decoupled parallelism and progressive training strategy. It achieves state-of-the-art performance on multimodal benchmarks while maintaining efficient real-time audio-visual interaction despite its large parameter count. The model demonstrates competitive results across text, image, video, and audio tasks while sustaining over 90% of text-only training throughput.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251104] COOL Is Optimal in Error-Free Asynchronous Byzantine Agreement</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed systems], [Byzantine agreement, error correction codes, asynchronous consensus, information-theoretic security]</li>
<li class=""><strong>authors:</strong> Jinyuan Chen</li>
<li class=""><strong>institution:</strong> Unknown (author: Jinyuan Chen)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.00263" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.00263</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents OciorACOOL, an adaptive variant of the COOL protocol that extends Byzantine agreement to asynchronous settings while maintaining error-free information-theoretic security. The protocol achieves consensus with O(max{nℓ, nt log q}) communication bits, O(1) rounds, and a single binary BA invocation under optimal resilience n ≥ 3t+1. It preserves the same low-complexity error-correction encoding and decoding as the original COOL protocol.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251104] AeroResQ: Edge-Accelerated UAV Framework for Scalable, Resilient and Collaborative Escape Route Planning in Wildfire Scenarios</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [weighted A* search, edge accelerators, Apache IoTDB, geo-fenced re-partitioning, multi-layer orchestration]</li>
<li class=""><strong>authors:</strong> Suman Raj, Radhika Mittal, Rajiv Mayani, Pawel Zuk, Anirban Mandal, Michael Zink, Yogesh Simmhan, Ewa Deelman</li>
<li class=""><strong>institution:</strong> Indian Institute of Science, University of Southern California, University of North Carolina, University of Massachusetts Amherst</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.00038" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.00038</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AeroResQ proposes an edge-accelerated UAV framework using service drones for real-time detection and coordinator drones with weighted A* search for escape route planning. The system incorporates resilient mechanisms like automated data redistribution and workload reassignment to handle drone failures. Experimental results show it achieves ≤500ms latency and over 98% task completion, demonstrating feasibility for real-time wildfire emergency response.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251104] Agentic Auto-Scheduling: An Experimental Study of LLM-Guided Loop Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [loop optimization, polyhedral methods, compiler feedback, code transformation, tiling, fusion, parallelization]</li>
<li class=""><strong>authors:</strong> Massinissa Merouani, Islem Kara Bernou, Riyadh Baghdadi</li>
<li class=""><strong>institution:</strong> New York University Abu Dhabi</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.00592" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.00592</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces ComPilot, a framework that uses Large Language Models as interactive optimization agents to guide loop nest transformations through a closed-loop feedback system with a compiler. The LLM proposes code transformations, receives compiler feedback on legality and performance, and iteratively refines its optimization strategy. The approach achieves significant speedups (2.66x-3.54x) on PolyBench benchmarks and demonstrates competitive performance against the state-of-the-art Pluto polyhedral optimizer.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251104] Scalable Maxflow Processing for Dynamic Graphs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [graph algorithms], [push-relabel algorithm, GPU parallelization, CUDA optimizations, dynamic graph processing]</li>
<li class=""><strong>authors:</strong> Shruthi Kannappan, Ashwina Kumar, Rupesh Nasre</li>
<li class=""><strong>institution:</strong> Indian Institute of Technology Madras</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.01235" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.01235</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents novel GPU-parallel Max-Flow algorithms for both static and dynamic graphs, building on the Push-Relabel method. The authors introduce CUDA-specific optimizations to enhance performance and scalability. Their approach efficiently handles incremental updates to dynamic graphs without recomputing from scratch.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251104] Boosting performance of computer vision applications through embedded GPUs on the edge</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [edge computing, embedded GPUs, computer vision, performance optimization]</li>
<li class=""><strong>authors:</strong> Fabio Diniz Rossi</li>
<li class=""><strong>institution:</strong> Federal Institute of Education, Science, and Technology Farroupilha (IFFar)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.01129" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.01129</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes using embedded GPUs in edge computing devices to accelerate computer vision applications. Experimental results show that GPUs achieve performance gains of up to 820.36% compared to CPUs alone. This approach improves user experience by overcoming the resource limitations of traditional edge devices.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251104] Design of quasi phase matching crystal based on differential gray wolf algorithm</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [differential evolution algorithm, gray wolf optimization, GPU parallel computing, quasi-phase matching crystal design]</li>
<li class=""><strong>authors:</strong> He Chen, ZiHua Zheng, JingHua Sun</li>
<li class=""><strong>institution:</strong> Dongguan University of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.01255" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.01255</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a hybrid optimization algorithm combining differential evolution and gray wolf optimization with GPU parallel acceleration for quasi-phase matching crystal design. The method achieves hundreds to thousands of times efficiency improvement compared to traditional CPU serial computing while enhancing crystal domain control accuracy. This breakthrough enables better performance in nonlinear optical devices for quantum optics and laser processing applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251104] Gradient Clock Synchronization with Practically Constant Local Skew</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed systems], [gradient clock synchronization, local skew, offset estimation, frequency deviation, self-stabilization, external synchronization]</li>
<li class=""><strong>authors:</strong> Christoph Lenzen</li>
<li class=""><strong>institution:</strong> CISPA Helmholtz Center for Information Security</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.01420" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.01420</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a refined model for gradient clock synchronization that leverages stability of measurement and frequency errors rather than worst-case bounds. The approach achieves local skew bounds of O(Δ+δ log D) by adapting to actual link performance and frequency changes, effectively breaking previous lower bounds. The method also ensures self-stabilization and extends to external synchronization scenarios.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251104] TINC: Trusted Intelligent NetChain</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [blockchain sharding], [multi-plane sharding, adaptive node assignment, dynamic workload balancing, control-data plane decoupling, Dynamic Decentralized Identifiers (DDIDs)]</li>
<li class=""><strong>authors:</strong> Qi Xia, Hu Xia, Isaac Amankona Obiri, Adjei-Arthur Bonsu, Grace Mupoyi Ntuala, Ansu Badjie, Tienin Bole Wilfried, Jiaqin Liu, Lan Ma, Jianbin Gao, Feng Yao</li>
<li class=""><strong>institution:</strong> University of Electronic Science and Technology of China, National University of Defense Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.00823" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.00823</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes TINC, a multi-plane sharding architecture for consortium blockchains that uses intelligent mechanisms for adaptive node assignment and workload balancing. By decoupling control and data planes, TINC improves scalability while maintaining security guarantees. Experimental results show it achieves higher throughput, lower latency, and better load balancing compared to existing sharding frameworks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251104] Towards Portability at Scale: A Cross-Architecture Performance Evaluation of a GPU-enabled Shallow Water Solver</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [HPC Performance Evaluation], [Kokkos, GPU acceleration, roofline analysis, strong scaling, weak scaling, performance portability]</li>
<li class=""><strong>authors:</strong> Johansell Villalobos, Daniel Caviedes-Voullième, Silvio Rizzi, Esteban Meneses</li>
<li class=""><strong>institution:</strong> National High Technology Center Costa Rica, Jülich Supercomputing Center, Argonne National Laboratory, Costa Rica Technological Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.01001" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.01001</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper evaluates the SERGHEI-SWE shallow water solver using Kokkos for performance portability across four HPC systems with different GPU architectures. The study demonstrates strong scalability up to 1024 GPUs and identifies memory bandwidth as the primary performance bottleneck through roofline analysis. Results show the solver achieves good performance portability but requires further kernel optimization for improved efficiency across architectures.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251104] Neuro-Inspired Task Offloading in Edge-IoT Networks Using Spiking Neural Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Spiking Neural Networks, task offloading, edge computing, IoT networks, Brian2 simulator, YAFS]</li>
<li class=""><strong>authors:</strong> Fabio Diniz Rossi</li>
<li class=""><strong>institution:</strong> Federal Institute Farroupilha</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.01127" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.01127</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a neuro-inspired task offloading framework using Spiking Neural Networks for edge-IoT networks. The SNN-based approach enables real-time, energy-efficient task orchestration by leveraging event-driven computation and temporal pattern processing. Experimental results show the framework achieves significant improvements with up to 26% lower latency, 32% reduced energy consumption, and 25% higher success rate compared to traditional methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251104] Real-time Continual Learning on Intel Loihi 2</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [spiking neural networks, neuromorphic computing, continual learning, online learning, catastrophic forgetting, neurogenesis, metaplasticity]</li>
<li class=""><strong>authors:</strong> Elvin Hajizada, Danielle Rager, Timothy Shea, Leobardo Campos-Macias, Andreas Wild, Eyke Hüllermeier, Yulia Sandamirskaya, Mike Davies</li>
<li class=""><strong>institution:</strong> Intel Labs, University of Munich (LMU), Zurich University of Applied Sciences (ZHAW)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.01553" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.01553</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents CLP-SNN, a spiking neural network architecture implemented on Intel&#x27;s Loihi 2 neuromorphic chip for online continual learning. The method uses event-driven sparse learning, self-normalizing learning rules, and integrated neurogenesis to achieve competitive accuracy without rehearsal. The results show dramatic efficiency improvements with 70× faster inference and 5,600× better energy efficiency compared to edge GPU implementations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251104] Transformer-Based Sparse CSI Estimation for Non-Stationary Channels</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Flash-Attention Transformer, patch-wise self-attention, composite loss function, pilot-aided estimation]</li>
<li class=""><strong>authors:</strong> Muhammad Ahmed Mohsin, Muhammad Umer, Ahsan Bilal, Hassan Rizwan, Sagnik Bhattacharya, Muhammad Ali Jamshed, John M. Cioffi</li>
<li class=""><strong>institution:</strong> Stanford University, University of Oklahoma, University of California Riverside, University of Glasgow</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.01333" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.01333</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a Flash-Attention Transformer framework that combines model-driven pilot acquisition with data-driven CSI reconstruction for non-stationary wireless channels. The method uses patch-wise self-attention and a physics-aware composite loss function to achieve accurate channel estimation. Results show it outperforms traditional methods by 13 dB NMSE while reducing pilot overhead by 16 times, demonstrating reliable CSI recovery for 5G and beyond-5G networks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251104] FREESH: Fair, Resource- and Energy-Efficient Scheduling for LLM Serving on Heterogeneous GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [joint routing and scheduling, dynamic GPU frequency scaling, Least-Laxity-First (LLF), spatiotemporal computation optimization, heterogeneous GPU clusters]</li>
<li class=""><strong>authors:</strong> Xuan He, Zequan Fang, Jinzhao Lian, Danny H.K. Tsang, Baosen Zhang, Yize Chen</li>
<li class=""><strong>institution:</strong> Hong Kong University of Science and Technology (Guangzhou), Huazhong University of Science and Technology, Renmin University of China, University of Washington, University of Alberta</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.00807" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.00807</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FREESH proposes a joint routing and scheduling system that optimizes LLM serving across heterogeneous GPU clusters by leveraging spatiotemporal computation flexibility and dynamic GPU frequency scaling. The system matches GPU power-throughput characteristics with query workloads while ensuring latency and fairness through LLF scheduling. Experimental results show FREESH reduces energy consumption by 28.6% and carbon emissions by 45.45% while improving SLO attainment and fairness.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251104] Federated Cyber Defense: Privacy-Preserving Ransomware Detection Across Distributed Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, ransomware detection, privacy-preserving AI, distributed systems]</li>
<li class=""><strong>authors:</strong> Daniel M. Jimenez-Gutierrez, Enrique Zuazua, Joaquin Del Rio, Oleksii Sliusarenko, Xabi Uribe-Etxebarria</li>
<li class=""><strong>institution:</strong> Sherpa.ai</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.01583" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.01583</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes using Federated Learning with the Sherpa.ai platform to train ransomware detection models across distributed systems while keeping data local. The approach improves detection accuracy by 9% compared to server-local models and achieves performance comparable to centralized training. This demonstrates FL provides a scalable, privacy-preserving framework for cybersecurity across organizational boundaries.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251104] LARK - Linearizability Algorithms for Replicated Keys in Aerospike</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed databases], [linearizability, synchronous replication, partition availability conditions, log-free replication, TLA+ specification]</li>
<li class=""><strong>authors:</strong> Andrew Goodng, Kevin Porter, Thomas Lopatic, Ashish Shinde, Sunil Sayyaparaju, Srinivasan Seshadri, V. Srinivasan</li>
<li class=""><strong>institution:</strong> Aerospike</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.01843" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.01843</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LARK introduces a log-free synchronous replication protocol using Partition Availability Conditions that reason across the entire database cluster rather than fixed replica sets. This approach achieves linearizability while providing significantly higher availability than traditional quorum-log consensus protocols like Raft and Paxos, enabling continued commits during node failures and zero-downtime rolling restarts with minimal replication factors.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251104] Edge AI in Highly Volatile Environments: Is Fairness Worth the Accuracy Trade-off?</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, client selection, fairness algorithms, RBFF, RBCSF, edge computing]</li>
<li class=""><strong>authors:</strong> Obaidullah Zaland, Feras M. Awaysheh, Sawsan Al Zubi, Abdul Rahman Safi, Monowar Bhuyan</li>
<li class=""><strong>institution:</strong> Umeå University, University of Santiago de Compostela, Kabul University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.01737" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.01737</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper evaluates fairness-based client selection algorithms (RBFF and RBCSF) in federated learning for volatile edge environments. The study finds that while equitable client selection provides better participation opportunities, it results in slower global training convergence. The research highlights the fundamental trade-off between fairness and performance in edge AI systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251104] Adaptive Multidimensional Quadrature on Multi-GPU Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [high-performance computing], [adaptive quadrature, domain decomposition, load balancing, CUDA-aware MPI, multi-GPU systems]</li>
<li class=""><strong>authors:</strong> Melanie Tonarelli, Simone Riva, Pietro Benedusi, Fabrizio Ferrandi, Rolf Krause</li>
<li class=""><strong>institution:</strong> Università della Svizzera Italiana, Politecnico di Milano, King Abdullah University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.01573" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.01573</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a distributed adaptive quadrature method that performs multidimensional integration through hierarchical domain decomposition on multi-GPU systems. The method uses decentralized load redistribution with cyclic round-robin policy and non-blocking MPI communication to handle load imbalance. Compared to state-of-the-art GPU packages, it achieves higher efficiency in high dimensions and improved robustness regarding integrand regularity and target accuracy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251104] A Distributed Plug-and-Play MCMC Algorithm for High-Dimensional Inverse Problems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [MCMC, Plug-and-Play ULA, distributed computing, denoising neural network, Single Program Multiple Data]</li>
<li class=""><strong>authors:</strong> Maxime Bouton, Pierre-Antoine Thouvenin, Audrey Repetti, Pierre Chainais</li>
<li class=""><strong>institution:</strong> University of Lille, Heriot-Watt University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.00870" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.00870</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a distributed Plug-and-Play Markov Chain Monte Carlo algorithm that uses lightweight denoising neural networks and approximate data augmentation to solve high-dimensional imaging inverse problems. The method efficiently leverages multiple GPUs through a Single Program Multiple Data architecture to address scalability challenges. The approach achieves comparable reconstruction performance to other PnP methods while being scalable and enabling uncertainty quantification for large-scale imaging problems.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 46</strong></p>
<ul>
<li class="">[arXiv251104] Robust Single-Agent Reinforcement Learning for Regional Traffic Signal Control Under Demand Fluctuations <a href="https://arxiv.org/pdf/2511.00549" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Study on Supply Chain Finance Decision-Making Model and Enterprise Economic Performance Prediction Based on Deep Reinforcement Learning <a href="https://arxiv.org/pdf/2511.00166" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Who Can We Trust? Scope-Aware Video Moment Retrieval with Multi-Agent Conflict <a href="https://arxiv.org/pdf/2511.00370" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Single-agent Reinforcement Learning Model for Regional Adaptive Traffic Signal Control <a href="https://arxiv.org/pdf/2511.00551" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining <a href="https://arxiv.org/pdf/2511.00457" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings <a href="https://arxiv.org/pdf/2511.00405" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Bootstrap Off-policy with World Model <a href="https://arxiv.org/pdf/2511.00423" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] DCcluster-Opt: Benchmarking Dynamic Multi-Objective Optimization for Geo-Distributed Data Center Workloads <a href="https://arxiv.org/pdf/2511.00117" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Real-DRL: Teach and Learn in Reality <a href="https://arxiv.org/pdf/2511.00112" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] PreferThinker: Reasoning-based Personalized Image Preference Assessment <a href="https://arxiv.org/pdf/2511.00609" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Efficient Reinforcement Learning for Large Language Models with Intrinsic Exploration <a href="https://arxiv.org/pdf/2511.00794" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Iterative Foundation Model Fine-Tuning on Multiple Rewards <a href="https://arxiv.org/pdf/2511.00220" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning <a href="https://arxiv.org/pdf/2511.00222" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] End-to-End Framework Integrating Generative AI and Deep Reinforcement Learning for Autonomous Ultrasound Scanning <a href="https://arxiv.org/pdf/2511.00114" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] World Simulation with Video Foundation Models for Physical AI <a href="https://arxiv.org/pdf/2511.00062" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] LC-Opt: Benchmarking Reinforcement Learning and Agentic AI for End-to-End Liquid Cooling Optimization in Data Centers <a href="https://arxiv.org/pdf/2511.00116" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Token-Regulated Group Relative Policy Optimization for Stable Reinforcement Learning in Large Language Models <a href="https://arxiv.org/pdf/2511.00066" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] On the Fundamental Limitations of Decentralized Learnable Reward Shaping in Cooperative Multi-Agent Reinforcement Learning <a href="https://arxiv.org/pdf/2511.00034" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Improving the Robustness of Control of Chaotic Convective Flows with Domain-Informed Reinforcement Learning <a href="https://arxiv.org/pdf/2511.00272" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries <a href="https://arxiv.org/pdf/2511.00710" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail <a href="https://arxiv.org/pdf/2511.00088" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] SpatialTraceGen: High-Fidelity Traces for Efficient VLM Spatial Reasoning Distillation <a href="https://arxiv.org/pdf/2511.00054" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] A Dual Large Language Models Architecture with Herald Guided Prompts for Parallel Fine Grained Traffic Signal Control <a href="https://arxiv.org/pdf/2511.00136" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Graph-Attentive MAPPO for Dynamic Retail Pricing <a href="https://arxiv.org/pdf/2511.00039" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] GrowthHacker: Automated Off-Policy Evaluation Optimization Using Code-Modifying LLM Agents <a href="https://arxiv.org/pdf/2511.00802" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Logic-informed reinforcement learning for cross-domain optimization of large-scale cyber-physical systems <a href="https://arxiv.org/pdf/2511.00806" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events? <a href="https://arxiv.org/pdf/2511.00808" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Equilibrium Policy Generalization: A Reinforcement Learning Framework for Cross-Graph Zero-Shot Generalization in Pursuit-Evasion Games <a href="https://arxiv.org/pdf/2511.00811" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] KFCPO: Kronecker-Factored Approximated Constrained Policy Optimization <a href="https://arxiv.org/pdf/2511.00880" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] SLAP: Shortcut Learning for Abstract Planning <a href="https://arxiv.org/pdf/2511.01107" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] DART: Difficulty-Adaptive Reasoning Truncation for Efficient Large Language Models <a href="https://arxiv.org/pdf/2511.01170" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Self-Harmony: Learning to Harmonize Self-Supervision and Self-Play in Test-Time Reinforcement Learning <a href="https://arxiv.org/pdf/2511.01191" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Thought-For-Food: Reasoning Chain Induced Food Visual Question Answering <a href="https://arxiv.org/pdf/2511.01213" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Optimizing Electric Vehicle Charging Station Placement Using Reinforcement Learning and Agent-Based Simulations <a href="https://arxiv.org/pdf/2511.01218" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models <a href="https://arxiv.org/pdf/2511.01331" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Diffusion-Based Solver for CNF Placement on the Cloud-Continuum <a href="https://arxiv.org/pdf/2511.01343" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Thinking with DistilQwen: A Tale of Four Distilled Reasoning and Reward Model Series <a href="https://arxiv.org/pdf/2511.01354" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Learning Intractable Multimodal Policies with Reparameterization and Diversity Regularization <a href="https://arxiv.org/pdf/2511.01374" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Modulation of temporal decision-making in a deep reinforcement learning agent under the dual-task paradigm <a href="https://arxiv.org/pdf/2511.01415" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Learning to Seek Evidence: A Verifiable Reasoning Agent with Causal Faithfulness Analysis <a href="https://arxiv.org/pdf/2511.01425" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] TPS-Bench: Evaluating AI Agents&#x27; Tool Planning &amp; Scheduling Abilities in Compounding Tasks <a href="https://arxiv.org/pdf/2511.01527" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Learning what to say and how precisely: Efficient Communication via Differentiable Discrete Communication Learning <a href="https://arxiv.org/pdf/2511.01554" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] L2T-Tune<!-- -->:LLM-Guided<!-- --> Hybrid Database Tuning with LHS and TD3 <a href="https://arxiv.org/pdf/2511.01602" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Collaborative Large Language Model Inference via Resource-Aware Parallel Speculative Decoding <a href="https://arxiv.org/pdf/2511.01695" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] RLAC: Reinforcement Learning with Adversarial Critic for Free-Form Generation Tasks <a href="https://arxiv.org/pdf/2511.01758" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] GenDexHand: Generative Simulation for Dexterous Hands <a href="https://arxiv.org/pdf/2511.01791" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 25</strong></p>
<ul>
<li class="">[arXiv251104] LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation <a href="https://arxiv.org/pdf/2511.00090" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] QuantumBench: A Benchmark for Quantum Problem Solving <a href="https://arxiv.org/pdf/2511.00092" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Structure-Preserving Physics-Informed Neural Network for the Korteweg--de Vries (KdV) Equation <a href="https://arxiv.org/pdf/2511.00418" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Efficient Generation of Binary Magic Squares <a href="https://arxiv.org/pdf/2511.00547" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] DCcluster-Opt: Benchmarking Dynamic Multi-Objective Optimization for Geo-Distributed Data Center Workloads <a href="https://arxiv.org/pdf/2511.00117" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] World Simulation with Video Foundation Models for Physical AI <a href="https://arxiv.org/pdf/2511.00062" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Chain of Time: In-Context Physical Simulation with Image Generation Models <a href="https://arxiv.org/pdf/2511.00110" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Reliable Curation of EHR Dataset via Large Language Models under Environmental Constraints <a href="https://arxiv.org/pdf/2511.00772" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Advancing AI Challenges for the United States Department of the Air Force <a href="https://arxiv.org/pdf/2511.00267" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] FeNN-DMA: A RISC-V SoC for SNN acceleration <a href="https://arxiv.org/pdf/2511.00732" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Position Paper: If Innovation in AI Systematically Violates Fundamental Rights, Is It Innovation at All? <a href="https://arxiv.org/pdf/2511.00027" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Cross-fluctuation phase transitions reveal sampling dynamics in diffusion models <a href="https://arxiv.org/pdf/2511.00124" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Scalable Processing-Near-Memory for 1M-Token LLM Inference: CXL-Enabled KV-Cache Management Beyond GPU Limits <a href="https://arxiv.org/pdf/2511.00321" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Leveraging Multi-Agent System (MAS) and Fine-Tuned Small Language Models (SLMs) for Automated Telecom Network Troubleshooting <a href="https://arxiv.org/pdf/2511.00651" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Diffusion Models at the Drug Discovery Frontier: A Review on Generating Small Molecules versus Therapeutic Peptides <a href="https://arxiv.org/pdf/2511.00209" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] LL-ViT: Edge Deployable Vision Transformers with Look Up Table Neurons <a href="https://arxiv.org/pdf/2511.00812" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Transformers as Intrinsic Optimizers: Forward Inference through the Energy Principle <a href="https://arxiv.org/pdf/2511.00907" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Keys in the Weights: Transformer Authentication Using Model-Bound Latent Representations <a href="https://arxiv.org/pdf/2511.00973" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] DART: Difficulty-Adaptive Reasoning Truncation for Efficient Large Language Models <a href="https://arxiv.org/pdf/2511.01170" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] When, What, and How: Rethinking Retrieval-Enhanced Speculative Decoding <a href="https://arxiv.org/pdf/2511.01282" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Estimation of Toeplitz Covariance Matrices using Overparameterized Gradient Descent <a href="https://arxiv.org/pdf/2511.01605" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Bridging Lifelong and Multi-Task Representation Learning via Algorithm and Complexity Measure <a href="https://arxiv.org/pdf/2511.01847" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Transfer learning discovery of molecular modulators for perovskite solar cells <a href="https://arxiv.org/pdf/2511.00204" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Quantum Deep Learning Still Needs a Quantum Leap <a href="https://arxiv.org/pdf/2511.01253" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251104] Split-Flows: Measure Transport and Information Loss Across Molecular Resolutions <a href="https://arxiv.org/pdf/2511.01464" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-11-05">2025-11-05<a href="#2025-11-05" class="hash-link" aria-label="Direct link to 2025-11-05" title="Direct link to 2025-11-05" translate="no">​</a></h2>
<p><strong>cs.DC total: 23</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251105] A Taxonomy of Schedulers -- Operating Systems, Clusters and Big Data Frameworks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [cluster infrastructure], [workload schedulers, taxonomy, throughput, scalability, Borg]</li>
<li class=""><strong>authors:</strong> Leszek Sliwko</li>
<li class=""><strong>institution:</strong> Axis Applications Ltd</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.01860" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.01860</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a hierarchical taxonomy of workload schedulers based on architecture and design, with special focus on Google&#x27;s Borg system. The taxonomy analyzes key design factors affecting throughput and scalability across operating systems, clusters, and big data frameworks. The review identifies incremental architectural improvements that have enhanced scheduler performance in modern computing environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251105] Possible Futures for Cloud Cost Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [cloud cost models, resource scheduling, HPC, AI/ML optimization]</li>
<li class=""><strong>authors:</strong> Vanessa Sochat, Daniel Milroy</li>
<li class=""><strong>institution:</strong> Lawrence Livermore National Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.01862" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.01862</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes how current cloud cost models optimized for AI/ML workloads are poorly suited for scientific computing needs. It examines historical trends and proposes that without better resource allocation strategies, scientific workloads may be forced to run on suboptimal infrastructure. The authors conclude that new cloud cost models are needed to ensure continued support for scientific discovery.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251105] SPHERE: Spherical partitioning for large-scale routing optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [graph algorithms], [spherical partitioning, Dijkstra, bidirectional search, graph partitioning, METIS, Louvain]</li>
<li class=""><strong>authors:</strong> Robert Fabian Lindermann, Paul-Niklas Ken Kandora, Simon Caspar Zeller, Adrian Asmund Fessler, Steffen Rebennack</li>
<li class=""><strong>institution:</strong> Karlsruhe Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.01863" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.01863</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SPHERE is a query-aware heuristic that partitions large graph routing problems by identifying vertices close to both source and target, then recursively solving smaller subproblems. This approach avoids boundary repair issues and enables parallel processing while maintaining solution quality. The method demonstrates faster runtimes and smaller optimality gaps compared to existing approaches on graphs with over a million nodes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251105] Learned Cost Model for Placement on Reconfigurable Dataflow Hardware</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [learned cost model, dataflow architecture, placement and routing, throughput prediction, reconfigurable hardware]</li>
<li class=""><strong>authors:</strong> Etash Guha, Tianxiao Jiang, Andrew Deng, Jian Zhang, Muthu Annamalai</li>
<li class=""><strong>institution:</strong> SambaNova Systems</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.01872" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.01872</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a learned cost model that predicts throughput for mapping dataflow graphs onto reconfigurable hardware, replacing traditional hand-designed analytical models. The learned approach achieves 31%-52% more accurate throughput predictions across various graphs and maintains accuracy without performance annotations. Using this model results in 5.6% faster compiled graphs compared to conventional methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251105] HGraphScale: Hierarchical Graph Learning for Autoscaling Microservice Applications in Container-based Cloud Computing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [hierarchical graph neural network, deep reinforcement learning, autoscaling, microservice dependencies, container-based cloud]</li>
<li class=""><strong>authors:</strong> Zhengxin Fang, Hui Ma, Gang Chen, Rajkumar Buyya</li>
<li class=""><strong>institution:</strong> Victoria University of Wellington, University of Melbourne</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.01881" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.01881</a></li>
<li class=""><strong>Simple LLM Summary:</strong> HGraphScale uses hierarchical graph neural networks to model microservice dependencies and deployment schemes for autoscaling in container-based cloud environments. The approach dynamically adjusts container resources to handle fluctuating user request workloads. Experimental results show it reduces average response time by up to 80.16% compared to state-of-the-art methods under VM rental budget constraints.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251105] Conceptual Design Report for FAIR Computing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [research computing infrastructure], [federated infrastructure, centrally-orchestrated, computing model, storage infrastructure, open data policies]</li>
<li class=""><strong>authors:</strong> Johan Messchendorp, Mohammad Al-Turany, Volker Friese, Thorsten Kollegger, Bastian Loeher, Jochen Markert, Andrew Mistry, Thomas Neff, Adrian Oeftiger, Michael Papenbrock, Stephane Pietri, Shahab Sanjari, Tobias Stockmanns</li>
<li class=""><strong>institution:</strong> FAIR, Darmstadt</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.01861" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.01861</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a conceptual design for a federated and centrally-orchestrated computing infrastructure to support diverse research activities at FAIR. The proposed system aims to provide scalable and flexible computing resources with open data, software, and service policies. The main conclusion is that this infrastructure design will address future data challenges and serve research needs from the &quot;first science (plus)&quot; phase starting in 2028 through FAIR&#x27;s modularized start version.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251105] EdgeReasoning: Characterizing Reasoning LLM Deployment on Edge GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [latency-accuracy tradeoffs, token length control, test-time scaling, prompt engineering, model tuning]</li>
<li class=""><strong>authors:</strong> Benjamin Kubwimana, Qijing Huang</li>
<li class=""><strong>institution:</strong> NVIDIA</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.01866" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.01866</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents EdgeReasoning, a comprehensive study that systematically characterizes reasoning LLM deployment on edge GPUs by analyzing latency-accuracy tradeoffs across different architectures and model sizes. The research evaluates techniques for reducing reasoning token length and profiles test-time scaling methods to optimize accuracy under strict latency constraints. The study maps the Pareto frontier of achievable configurations to provide systematic guidance for optimal edge deployment of reasoning LLMs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251105] CudaForge: An Agent Framework with Hardware Feedback for CUDA Kernel Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [multi-agent workflow, CUDA kernel optimization, hardware feedback, Nsight Compute metrics, LLM agents]</li>
<li class=""><strong>authors:</strong> Zijian Zhang, Rong Wang, Shiyang Li, Yuebo Luo, Mingyi Hong, Caiwen Ding</li>
<li class=""><strong>institution:</strong> University of Minnesota, Twin Cities</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.01884" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.01884</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CudaForge introduces a training-free multi-agent framework using LLM agents (Coder and Judge) that iteratively generate and optimize CUDA kernels with hardware feedback. The system achieves 97.6% correctness and 1.68× speedup over PyTorch baselines while demonstrating strong generalization across GPUs and models. This approach enables cost-effective, high-performance kernel optimization without requiring expensive training.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251105] Roadrunner: Accelerating Data Delivery to WebAssembly-Based Serverless Functions</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [serverless computing], [zero-copy data transfer, memory mapping, serialization-free communication, virtual data hose]</li>
<li class=""><strong>authors:</strong> Cynthia Marcelino, Thomas Pusztai, Stefan Nastic</li>
<li class=""><strong>institution:</strong> TU Wien</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.01888" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.01888</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Roadrunner introduces a sidecar shim that enables near-zero copy data transfer between WebAssembly-based serverless functions by mapping function memory and using a virtual data hose. This approach eliminates serialization/deserialization overhead and reduces context switching between user and kernel space. Experimental results show Roadrunner improves inter-function communication latency by 44-89%, reduces serialization overhead by 97%, and increases throughput by 69x compared to state-of-the-art solutions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251105] Structural Analysis of Multi-Core Processor and Reliability Evaluation Model</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [fault-tolerance], [logical-probabilistic methods, structural analysis, reliability evaluation, fault tolerance modeling, variable structure systems]</li>
<li class=""><strong>authors:</strong> S. Tsiramua, H. Meladze, T. Davitashvili, J.M. Sanchez, F. Criado-Aldeanueva</li>
<li class=""><strong>institution:</strong> University of Georgia, Muskhelishvili Institute of Computational Mathematics, Georgian Technical University, Ivane Javakhishvili Tbilisi State University, University of Malaga</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.01871" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.01871</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper develops logical-probabilistic models to analyze the structural reliability and performance efficiency of multi-core processors with variable configurations. The research presents evaluation models for reliability, fault tolerance, and lifetime considering all possible operational states. The study demonstrates trends in improving efficiency indicators through structural analysis of dual-core and quad-core processors.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251105] RobustFSM: Submodular Maximization in Federated Setting with Malicious Clients</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [submodular maximization, federated learning, robust aggregation, malicious clients]</li>
<li class=""><strong>authors:</strong> Duc A. Tran, Dung Truong, Duy Le</li>
<li class=""><strong>institution:</strong> University of Massachusetts Boston</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.02029" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.02029</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes RobustFSM, a federated submodular maximization method designed to withstand attacks from malicious clients through robust aggregation techniques. Empirical evaluation shows the solution significantly outperforms conventional federated algorithms under severe attack scenarios, achieving up to 200% improvement in solution quality depending on dataset and attack conditions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251105] mLR: Scalable Laminography Reconstruction based on Memoization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [memoization, ADMM-FFT, variable offloading, Fast Fourier Transform, laminography reconstruction]</li>
<li class=""><strong>authors:</strong> Bin Ma, Viktor Nikitin, Xi Wang, Tekin Bicer, Dong Li</li>
<li class=""><strong>institution:</strong> University of California, Merced, Argonne National Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.01893" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.01893</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces mLR, which uses memoization to replace repetitive FFT operations in ADMM-FFT laminography reconstruction and implements variable offloading for GPU scaling. This approach achieves up to 65.4% performance improvement and enables reconstruction of 2K×2K×2K volumes, the largest ever handled by ADMM-FFT with limited memory.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251105] Eliminating Multi-GPU Performance Taxes: A Systems Approach to Efficient Distributed LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [fine-grained synchronization, tile-level producer-consumer pipelines, in-kernel communication, BSP optimization, kernel fusion]</li>
<li class=""><strong>authors:</strong> Octavian Alexandru Trifan, Karthik Sangaiah, Muhammad Awad, Muhammad Osama, Sumanth Gudaparthi, Alexandru Nicolau, Alexander Veidenbaum, Ganesh Dasika</li>
<li class=""><strong>institution:</strong> University of California, Irvine, AMD Research and Advanced Development</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.02168" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.02168</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes moving beyond the bulk synchronous parallel model by using fine-grained programming patterns that fuse computation and communication into single kernels. This approach eliminates three key performance taxes through tile-level producer-consumer pipelines and fine-grained dataflow synchronization. The method achieves 10-20% speedup in distributed LLM inference workloads compared to traditional BSP approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251105] Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA Effects</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [Swizzled Head-first Mapping, NUMA-aware scheduling, multi-head attention optimization, cache reuse]</li>
<li class=""><strong>authors:</strong> Mansi Choudhary, Karthik Sangaiah, Sonali Singh, Muhammad Osama, Lisa Wu Wills, Ganesh Dasika</li>
<li class=""><strong>institution:</strong> Duke University, Advanced Micro Devices Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.02132" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.02132</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Swizzled Head-first Mapping, a spatially-aware scheduling strategy that aligns attention heads with GPU NUMA domains to exploit intra-chiplet cache reuse. On AMD&#x27;s MI300X architecture, this method achieves up to 50% higher performance over conventional attention algorithms and sustains high L2 cache hit rates of 80-97%. The results demonstrate that NUMA-aware scheduling is fundamental for achieving full efficiency on next-generation disaggregated GPUs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251105] GPoS: Geospatially-aware Proof of Stake</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [blockchain consensus], [GPoS, geospatial decentralization, Proof of Stake, BFT protocols, HotStuff, CometBFT, Eigenvector centrality, Gini coefficient]</li>
<li class=""><strong>authors:</strong> Shashank Motepalli, Naman Garg, Gengrui Zhang, Hans-Arno Jacobsen</li>
<li class=""><strong>institution:</strong> University of Toronto, IIIT-Delhi, Concordia University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.02034" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.02034</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Geospatially-aware Proof of Stake (GPoS), a consensus mechanism that integrates geospatial diversity with stake-based voting power to improve blockchain decentralization. Experimental evaluation shows GPoS achieves 45% better geospatial decentralization while maintaining minimal performance overhead in BFT protocols. The results demonstrate GPoS effectively addresses geographic centralization issues in major Proof of Stake blockchains.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251105] From Models to Operators: Rethinking Autoscaling Granularity for Large Generative Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [operator-level autoscaling, resource allocation, performance profiling, dynamic scaling]</li>
<li class=""><strong>authors:</strong> Xingqi Cui, Chieh-Jan Mike Liang, Jiarong Xing, Haoran Qiu</li>
<li class=""><strong>institution:</strong> Rice University, Microsoft Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.02248" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.02248</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes an operator-level autoscaling framework that allocates resources at finer granularity by optimizing scaling, batching, and placement based on individual operator profiles. This approach demonstrates significant improvements over traditional model-level autoscaling, achieving up to 40% fewer GPUs and 35% less energy while preserving SLOs. The results show that operator-level scaling is fundamentally more effective for large generative workloads than model-level scaling.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251105] Lightweight Latency Prediction Scheme for Edge Applications: A Rational Modelling Approach</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [rational modelling, latency prediction, edge computing, 5-fold cross-validation]</li>
<li class=""><strong>authors:</strong> Mohan Liyanage, Eldiyar Zhantileuov, Ali Kadhum Idrees, Rolf Schuster</li>
<li class=""><strong>institution:</strong> University of Applied Sciences and Arts Dortmund</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.02501" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.02501</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a lightweight latency prediction scheme using rational modeling that predicts network latency based on features like frame size and link utilization without active probing. The model achieves high accuracy (MAE = 0.0115, R² = 0.9847) with competitive inference time, offering better precision-efficiency trade-offs than traditional regressors and neural networks for edge applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251105] 3D Point Cloud Object Detection on Edge Devices for Split Computing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [split computing, voxelization, 3D point cloud, edge devices, deep neural networks]</li>
<li class=""><strong>authors:</strong> Taisuke Noguchi, Takuya Azumi</li>
<li class=""><strong>institution:</strong> Saitama University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.02293" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.02293</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes using split computing to distribute 3D point cloud object detection processing between edge devices and servers. The method reduces computational burden on edge devices by transmitting intermediate data after voxelization or within the network. Experimental results show significant reductions in inference time (up to 70.8%) and edge device execution time (up to 90.0%).</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251105] Quantum-Enhanced Generative Models for Rare Event Prediction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [variational quantum circuits, hybrid classical-quantum framework, tail-aware likelihood, quantum randomness-driven noise injection, parameter-shift gradients]</li>
<li class=""><strong>authors:</strong> M.Z. Haider, M.U. Ghouri, Tayyaba Noreen, M. Salman</li>
<li class=""><strong>institution:</strong> Université du Québec, The University of Faisalabad, SZABIST University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.02042" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.02042</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a Quantum-Enhanced Generative Model (QEGM) that combines classical deep latent-variable models with variational quantum circuits to improve rare event prediction. The hybrid framework uses quantum randomness for noise injection and a tail-aware loss function to enhance sample diversity and capture rare events. Results show QEGM reduces tail KL-divergence by up to 50% compared to classical baselines, demonstrating superior performance for rare-event modeling.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251105] Fast Algorithms for Scheduling Many-body Correlation Functions on Accelerators</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [high-performance computing], [scheduling algorithms, tensor contraction, memory optimization, GPU accelerators, temporal locality]</li>
<li class=""><strong>authors:</strong> Oguz Selvitopi, Emin Ozturk, Jie Chen, Ponnuswamy Sadayappan, Robert G. Edwards, Aydın Buluç</li>
<li class=""><strong>institution:</strong> Lawrence Berkeley National Laboratory, University of Utah, University of California, Jefferson Lab</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.02257" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.02257</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes two novel scheduling algorithms that reorder tensor contractions to optimize memory usage and data traffic on GPU accelerators for lattice quantum chromodynamics simulations. The methods achieve up to 2.1x improvement in peak memory and up to 1.9x faster computation time by increasing temporal locality through input and intermediate tensor reuse. These schedulers were integrated into the Redstar software suite, demonstrating significant performance improvements for correlation function computations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251105] Federated Attention: A Distributed Paradigm for Collaborative LLM Inference over Edge Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [federated attention, KV matrix aggregation, sparse attention, adaptive KV aggregation]</li>
<li class=""><strong>authors:</strong> Xiumei Deng, Zehui Xiong, Binbin Chen, Dong In Kim, Merouane Debbah, H. Vincent Poor</li>
<li class=""><strong>institution:</strong> Singapore University of Technology and Design, Queen&#x27;s University Belfast, Sungkyunkwan University, Khalifa University, Princeton University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.02647" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.02647</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Federated Attention (FedAttn), a distributed LLM inference framework that integrates federated learning principles into self-attention mechanisms. It enables collaborative response generation through local self-attention computation and periodic KV matrix exchange while preserving privacy. Experimental results demonstrate significant optimization opportunities and validate the framework&#x27;s potential for scalable and efficient edge deployments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251105] Implementing Multi-GPU Scientific Computing Miniapps Across Performance Portable Frameworks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [high-performance computing], [Kokkos, OpenMP, RAJA, OCCA, multi-GPU, performance portability, N-body simulation, structured grid simulation]</li>
<li class=""><strong>authors:</strong> Johansell Villalobos, Josef Ruzicka, Silvio Rizzi</li>
<li class=""><strong>institution:</strong> National High Technology Center, Costa Rica Institute of Technology, Argonne National Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.02655" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.02655</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper compares four performance portability frameworks (Kokkos, OpenMP, RAJA, and OCCA) for implementing multi-GPU scientific computing applications including N-body and structured grid simulations. The study found significant performance variability among frameworks, with OCCA showing advantages for small problems but potential scalability limitations, while OpenMP exhibited communication inefficiencies. The results highlight the need for further optimization in reduction algorithms and data communication to maximize framework performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251105] Making Democracy Work: Fixing and Simplifying Egalitarian Paxos (Extended Version)</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed consensus], [Paxos, Egalitarian Paxos, state-machine replication, fault tolerance, leaderless consensus]</li>
<li class=""><strong>authors:</strong> Fedor Ryabinin, Alexey Gotsman, Pierre Sutra</li>
<li class=""><strong>institution:</strong> IMDEA Software Institute, Institut Polytechnique de Paris</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.02743" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.02743</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents EPaxos*, a simplified and corrected variant of Egalitarian Paxos that eliminates the leader dependency in distributed consensus protocols. The key contribution is a simpler failure-recovery algorithm that has been rigorously proven correct, while generalizing the protocol to cover optimal failure thresholds. The work addresses complexity and correctness issues in the original Egalitarian Paxos while maintaining its leaderless advantages.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 20</strong></p>
<ul>
<li class="">[arXiv251105] Tool Zero: Training Tool-Augmented LLMs via Pure RL from Scratch <a href="https://arxiv.org/pdf/2511.01934" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251105] Shorter but not Worse: Frugal Reasoning via Easy Samples as Length Regularizers in Math RLVR <a href="https://arxiv.org/pdf/2511.01937" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251105] Automated Reward Design for Gran Turismo <a href="https://arxiv.org/pdf/2511.02094" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251105] Training Proactive and Personalized LLM Agents <a href="https://arxiv.org/pdf/2511.02208" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251105] Adaptive Cooperative Transmission Design for Ultra-Reliable Low-Latency Communications via Deep Reinforcement Learning <a href="https://arxiv.org/pdf/2511.02216" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251105] Optimizing Multi-Lane Intersection Performance in Mixed Autonomy Environments <a href="https://arxiv.org/pdf/2511.02217" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251105] Structural Plasticity as Active Inference: A Biologically-Inspired Architecture for Homeostatic Control <a href="https://arxiv.org/pdf/2511.02241" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251105] Reinforcement learning based data assimilation for unknown state model <a href="https://arxiv.org/pdf/2511.02286" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251105] Unlocking the Power of Multi-Agent LLM for Reasoning: From Lazy Agents to Deliberation <a href="https://arxiv.org/pdf/2511.02303" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251105] Automata-Conditioned Cooperative Multi-Agent Reinforcement Learning <a href="https://arxiv.org/pdf/2511.02304" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251105] Large-scale automatic carbon ion treatment planning for head and neck cancers via parallel multi-agent reinforcement learning <a href="https://arxiv.org/pdf/2511.02314" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251105] Auditable-choice reframing unlocks RL-based verification for open-ended tasks <a href="https://arxiv.org/pdf/2511.02463" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251105] An End-to-End Learning Approach for Solving Capacitated Location-Routing Problems <a href="https://arxiv.org/pdf/2511.02525" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251105] Adaptive Neighborhood-Constrained Q Learning for Offline Reinforcement Learning <a href="https://arxiv.org/pdf/2511.02567" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251105] Directional-Clamp PPO <a href="https://arxiv.org/pdf/2511.02577" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251105] Adaptive GR(1) Specification Repair for Liveness-Preserving Shielding in Reinforcement Learning <a href="https://arxiv.org/pdf/2511.02605" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251105] Natural-gas storage modelling by deep reinforcement learning <a href="https://arxiv.org/pdf/2511.02646" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251105] Curriculum Design for Trajectory-Constrained Agent: Compressing Chain-of-Thought Tokens in LLMs <a href="https://arxiv.org/pdf/2511.02690" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251105] From Solo to Symphony: Orchestrating Multi-Agent Collaboration with Single-Agent Demos <a href="https://arxiv.org/pdf/2511.02762" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251105] RL-Aided Cognitive ISAC: Robust Detection and Sensing-Communication Trade-offs <a href="https://arxiv.org/pdf/2511.02672" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 15</strong></p>
<ul>
<li class="">[arXiv251105] Before the Clinic: Transparent and Operable Design Principles for Healthcare AI <a href="https://arxiv.org/pdf/2511.01902" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251105] DeepContour: A Hybrid Deep Learning Framework for Accelerating Generalized Eigenvalue Problem Solving via Efficient Contour Design <a href="https://arxiv.org/pdf/2511.01927" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251105] TapOut: A Bandit-Based Approach to Dynamic Speculative Decoding <a href="https://arxiv.org/pdf/2511.02017" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251105] Beyond Static Cutoffs: One-Shot Dynamic Thresholding for Diffusion Language Models <a href="https://arxiv.org/pdf/2511.02077" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251105] Optimal-Agent-Selection: State-Aware Routing Framework for Efficient Multi-Agent Collaboration <a href="https://arxiv.org/pdf/2511.02200" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251105] Gradient-Variation Online Adaptivity for Accelerated Optimization with Hölder Smoothness <a href="https://arxiv.org/pdf/2511.02276" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251105] FP8-Flow-MoE: A Casting-Free FP8 Recipe without Double Quantization Error <a href="https://arxiv.org/pdf/2511.02302" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251105] Large-scale automatic carbon ion treatment planning for head and neck cancers via parallel multi-agent reinforcement learning <a href="https://arxiv.org/pdf/2511.02314" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251105] RoME: Domain-Robust Mixture-of-Experts for MILP Solution Prediction across Domains <a href="https://arxiv.org/pdf/2511.02331" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251105] NOWS: Neural Operator Warm Starts for Accelerating Iterative Solvers <a href="https://arxiv.org/pdf/2511.02481" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251105] Nesterov-Accelerated Robust Federated Learning Over Byzantine Adversaries <a href="https://arxiv.org/pdf/2511.02657" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251105] Curriculum Design for Trajectory-Constrained Agent: Compressing Chain-of-Thought Tokens in LLMs <a href="https://arxiv.org/pdf/2511.02690" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251105] ConMeZO: Adaptive Descent-Direction Sampling for Gradient-Free Finetuning of Large Language Models <a href="https://arxiv.org/pdf/2511.02757" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251105] Improving Bayesian inference in PTA data analysis: importance nested sampling with Normalizing Flows <a href="https://arxiv.org/pdf/2511.01958" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251105] Accelerated Frank-Wolfe Algorithms: Complementarity Conditions and Sparsity <a href="https://arxiv.org/pdf/2511.02821" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-11-06">2025-11-06<a href="#2025-11-06" class="hash-link" aria-label="Direct link to 2025-11-06" title="Direct link to 2025-11-06" translate="no">​</a></h2>
<p><strong>cs.DC total: 7</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251106] SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [KV cache compression, static graphs, continuous batching, tensor parallelism, sparse KV attention]</li>
<li class=""><strong>authors:</strong> Jonathan Li, Nasim Farahini, Evgenii Iuliugin, Magnus Vesterlund, Christian Haggstrom, Guangtao Wang, Shubhangi Upasani, Ayush Sachdeva, Rui Li, Faline Fu, Chen Wu, Ayesha Siddiqua, John Long, Tuowen Zhao, Matheen Musaddiq, Hakan Zeffer, Yun Du, Mingran Wang, Qinghua Li, Bo Li, Urmish Thakker, Raghu Prabhakar</li>
<li class=""><strong>institution:</strong> SambaNova Systems, Cartesia AI</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.03092" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.03092</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SnapStream introduces a KV cache compression method that reduces on-chip memory usage by 4x while maintaining model accuracy in long sequence decoding. It addresses deployment challenges in production inference systems with static graphs and continuous batching. The method demonstrates minimal accuracy degradation on benchmarks while achieving high throughput in real production settings.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251106] Stone Duality Proofs for Colorless Distributed Computability Theorems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed computing], [spectral spaces, Stone duality, simplicial complexes, projective limit, Alexandrov topology]</li>
<li class=""><strong>authors:</strong> Cameron Calk, Emmanuel Godard</li>
<li class=""><strong>institution:</strong> Aix-Marseille University, CNRS</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.03609" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.03609</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a new topological approach using spectral spaces to model distributed computations with round-based full-information adversaries. The main result shows that colorless tasks are solvable against compact adversaries if and only if there exists a compatible spectral map between the limit object and output space. This provides a unified topological framework that explains why colored and uncolored distributed models have equivalent computability power.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251106] A General Input-Dependent Colorless Computability Theorem and Applications to Core-Dependent Adversaries</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed computing], [topological methods, geometric simplicial complex, colorless tasks, k-set agreement, condition-based adversaries, core-resilient adversaries]</li>
<li class=""><strong>authors:</strong> Yannis Coutouly, Emmanuel Godard</li>
<li class=""><strong>institution:</strong> Aix-Marseille University, CNRS LIS UMR7020</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.03662" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.03662</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper uses topological and geometric methods to extend computability theorems for distributed computing tasks. It generalizes previous results to input-dependent adversaries and shows that core-resilient adversaries have equivalent computability power regardless of crash timing. The main conclusion provides a complete characterization of when k-Set Agreement is solvable under condition-based, core-dependent adversaries.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251106] Investigating the Impact of Isolation on Synchronized Benchmarks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [cloud benchmarking], [duet benchmarking, cgroups, CPU pinning, Docker containers, Firecracker MicroVMs, process isolation]</li>
<li class=""><strong>authors:</strong> Nils Japke, Furat Hamdan, Diana Baumann, David Bermbach</li>
<li class=""><strong>institution:</strong> TU Berlin</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.03533" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.03533</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper evaluates different isolation strategies (cgroups/CPU pinning, Docker containers, and Firecracker MicroVMs) for synchronized duet benchmarking in cloud environments. The study found that process isolation generally reduces false positives in performance measurements, except for Docker containers which showed higher susceptibility to performance degradation despite using similar underlying isolation mechanisms. The authors recommend using process isolation for synchronized workloads but advise against Docker containers for this purpose.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251106] Harvesting energy consumption on European HPC systems: Sharing Experience from the CEEC project</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [HPC energy efficiency], [energy-to-solution, time-to-solution, mixed-precision, CFD applications, accelerators]</li>
<li class=""><strong>authors:</strong> Kajol Kulkarni, Samuel Kemmler, Anna Schwarz, Gulcin Gedik, Yanxiang Chen, Dimitrios Papageorgiou, Ioannis Kavroulakis, Roman Iakymchuk</li>
<li class=""><strong>institution:</strong> Friedrich-Alexander-University Erlangen-Nürnberg, Bundesanstalt für Materialforschung und -prüfung, University of Stuttgart, Umeå University, Aristotle University of Thessaloniki, Uppsala University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.03029" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.03029</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes energy consumption across European HPC systems using case studies with CFD applications and evaluates energy-to-solution and time-to-solution metrics. The study demonstrates that accelerators and mixed-precision techniques significantly reduce energy consumption while maintaining computational accuracy. The authors advocate for improved energy measurement practices to promote sustainable exascale computing.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251106] Characterising Global Platforms: Centralised, Decentralised, Federated, and Grassroots</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed systems], [multiagent systems, atomic transactions, essential agents, protocol specification]</li>
<li class=""><strong>authors:</strong> Ehud Shapiro</li>
<li class=""><strong>institution:</strong> London School of Economics and Political Science, Weizmann Institute of Science</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.03286" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.03286</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a formal framework using atomic transactions-based multiagent transition systems to characterize global digital platforms. It introduces the concept of essential agents and shows that their cardinality partitions platforms into four distinct classes: centralized, decentralized, federated, and grassroots. This provides the first mathematical framework for classifying any global platform based on its architectural dependencies.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251106] UMDAM: A Unified Data Layout and DRAM Address Mapping for Heterogenous NPU-PIM</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [unified data layout, DRAM address mapping, column-major tile-based layout, processing-in-memory, NPU-PIM co-execution]</li>
<li class=""><strong>authors:</strong> Hai Huang, Xuhong Qiang, Weisheng Zhao, Chenchen Liu</li>
<li class=""><strong>institution:</strong> Beihang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.03293" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.03293</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes UMDAM, a unified memory-affinity data layout and DRAM address mapping scheme for NPU-PIM co-execution systems. It uses a column-major tile-based layout and configurable DRAM mapping to resolve data layout mismatches without extra memory overhead. Evaluations show UMDAM improves LLM inference efficiency with up to 3.0× faster time-to-first-token and 2.18× faster time-to-last-token on edge devices.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 22</strong></p>
<ul>
<li class="">[arXiv251106] Shrinking the Variance: Shrinkage Baselines for Reinforcement Learning with Verifiable Rewards <a href="https://arxiv.org/pdf/2511.03710" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251106] Periodic Skill Discovery <a href="https://arxiv.org/pdf/2511.03187" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251106] Learning-based Cooperative Robotic Paper Wrapping: A Unified Control Policy with Residual Force Control <a href="https://arxiv.org/pdf/2511.03181" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251106] Adaptable Hindsight Experience Replay for Search-Based Learning <a href="https://arxiv.org/pdf/2511.03405" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251106] Scaling Multi-Agent Environment Co-Design with Diffusion Models <a href="https://arxiv.org/pdf/2511.03100" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251106] Tensor-Efficient High-Dimensional Q-learning <a href="https://arxiv.org/pdf/2511.03595" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251106] Value of Information-Enhanced Exploration in Bootstrapped DQN <a href="https://arxiv.org/pdf/2511.02969" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251106] Digital Twin-Driven Pavement Health Monitoring and Maintenance Optimization Using Graph Neural Networks <a href="https://arxiv.org/pdf/2511.02957" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251106] Optimizing Earth-Moon Transfer and Cislunar Navigation: Integrating Low-Energy Trajectories, AI Techniques and GNSS-R Technologies <a href="https://arxiv.org/pdf/2511.03173" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251106] Climate Adaptation with Reinforcement Learning: Economic vs. Quality of Life Adaptation Pathways <a href="https://arxiv.org/pdf/2511.03243" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251106] PerfDojo: Automated ML Library Generation for Heterogeneous Architectures <a href="https://arxiv.org/pdf/2511.03586" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251106] Reinforcement Learning Using known Invariances <a href="https://arxiv.org/pdf/2511.03473" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251106] Outbidding and Outbluffing Elite Humans: Mastering Liar&#x27;s Poker via Self-Play and Reinforcement Learning <a href="https://arxiv.org/pdf/2511.03724" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251106] Incorporating Quality of Life in Climate Adaptation Planning via Reinforcement Learning <a href="https://arxiv.org/pdf/2511.03238" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251106] Multi-Objective Adaptive Rate Limiting in Microservices Using Deep Reinforcement Learning <a href="https://arxiv.org/pdf/2511.03279" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251106] Learning Without Critics? Revisiting GRPO in Classical Reinforcement Learning Environments <a href="https://arxiv.org/pdf/2511.03527" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251106] Leveraging Discrete Function Decomposability for Scientific Design <a href="https://arxiv.org/pdf/2511.03032" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251106] Going Beyond Expert Performance via Deep Implicit Imitation Reinforcement Learning <a href="https://arxiv.org/pdf/2511.03616" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251106] Behavior-Adaptive Q-Learning: A Unifying Framework for Offline-to-Online RL <a href="https://arxiv.org/pdf/2511.03695" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251106] Towards Formalizing Reinforcement Learning Theory <a href="https://arxiv.org/pdf/2511.03618" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251106] AnaFlow: Agentic LLM-based Workflow for Reasoning-Driven Explainable and Sample-Efficient Analog Circuit Sizing <a href="https://arxiv.org/pdf/2511.03697" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251106] DQN Performance with Epsilon Greedy Policies and Prioritized Experience Replay <a href="https://arxiv.org/pdf/2511.03670" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 7</strong></p>
<ul>
<li class="">[arXiv251106] Provable Accelerated Bayesian Optimization with Knowledge Transfer <a href="https://arxiv.org/pdf/2511.03125" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251106] A unified physics-informed generative operator framework for general inverse problems <a href="https://arxiv.org/pdf/2511.03241" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251106] PerfDojo: Automated ML Library Generation for Heterogeneous Architectures <a href="https://arxiv.org/pdf/2511.03586" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251106] Online Learning to Rank under Corruption: A Robust Cascading Bandits Approach <a href="https://arxiv.org/pdf/2511.03074" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251106] Going Beyond Expert Performance via Deep Implicit Imitation Reinforcement Learning <a href="https://arxiv.org/pdf/2511.03616" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251106] Behavior-Adaptive Q-Learning: A Unifying Framework for Offline-to-Online RL <a href="https://arxiv.org/pdf/2511.03695" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251106] AnaFlow: Agentic LLM-based Workflow for Reasoning-Driven Explainable and Sample-Efficient Analog Circuit Sizing <a href="https://arxiv.org/pdf/2511.03697" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-11-07">2025-11-07<a href="#2025-11-07" class="hash-link" aria-label="Direct link to 2025-11-07" title="Direct link to 2025-11-07" translate="no">​</a></h2>
<p><strong>cs.DC total: 9</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251107] A Reinforced Evolution-Based Approach to Multi-Resource Load Balancing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [optimization algorithms], [genetic algorithms, migration operator, random genetic drift, combinatorial optimization, load balancing]</li>
<li class=""><strong>authors:</strong> Leszek Sliwko</li>
<li class=""><strong>institution:</strong> Wrocław University of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.04183" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.04183</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a reinforced genetic algorithm approach for multi-resource load balancing optimization problems. The authors developed modifications to classical genetic routines, including a migration operator inspired by biological random genetic drift, to handle strict feasibility constraints. The proposed method addresses NP-hard optimization problems where traditional evolutionary schemas were ineffective.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251107] Stochastic Modeling for Energy-Efficient Edge Infrastructure</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [Markov Chains, stochastic modeling, Monte Carlo simulation, predictive power scaling, sensitivity analysis]</li>
<li class=""><strong>authors:</strong> Fabio Diniz Rossi</li>
<li class=""><strong>institution:</strong> Federal Institute Farroupilha</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.03941" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.03941</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a stochastic modeling approach using Markov Chains to analyze power state transitions in Edge Computing. The research demonstrates that AI-driven predictive power scaling outperforms conventional reactive methods by minimizing unnecessary transitions and optimizing workload distribution. Experimental results show significant improvements in energy efficiency and system responsiveness across heterogeneous edge nodes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251107] Enabling Dynamic Sparsity in Quantized LLM Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [dynamic sparsity, quantization, zigzag-patterned layout, GEMV kernel, sparse indices gathering]</li>
<li class=""><strong>authors:</strong> Rongxiang Wang, Kangyuan Shu, Felix Xiaozhu Lin</li>
<li class=""><strong>institution:</strong> University of Virginia, Zoom Communications Inc</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.04477" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.04477</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes techniques to enable dynamic sparse inference under low-bit quantization for LLMs, featuring a zigzag-patterned quantization layout, specialized GEMV kernel, and efficient sparse indices gathering mechanism. The approach achieves up to 1.55× faster decoding throughput while maintaining comparable accuracy to dense quantized inference. This demonstrates that structured sparsity and quantization can effectively coexist on commodity GPUs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251107] A New Probabilistic Mobile Byzantine Failure Model for Self-Protecting Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [fault-tolerance], [Mobile Byzantine Failure, MAPE-K architecture, Markov process, self-protection, self-reconfiguration]</li>
<li class=""><strong>authors:</strong> Silvia Bonomi, Giovanni Farina, Roy Friedman, Eviatar B. Procaccia, Sebastien Tixeuil</li>
<li class=""><strong>institution:</strong> Sapienza University of Rome, Niccolò Cusano University, Technion, Sorbonne University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.04523" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.04523</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a new probabilistic Mobile Byzantine Failure model that integrates with the Analysis component of MAPE-K based self-protecting systems. The model mathematically analyzes system behavior under attack-recovery dynamics using Markov processes and provides simulation results. The approach enables distributed systems to autonomously manage security threats through dynamic self-protection and reconfiguration strategies.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251107] Resolving Conflicts with Grace: Dynamically Concurrent Universality</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed computing], [dynamic concurrency, universal construction, consensus, synchronization, conflict detection]</li>
<li class=""><strong>authors:</strong> Petr Kuznetsov, Nathan Josia Schrodt</li>
<li class=""><strong>institution:</strong> Télécom Paris, Institut Polytechnique de Paris, Technical University of Darmstadt</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.04631" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.04631</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces dynamic concurrency, a method that uses strong synchronization primitives only when operations must arbitrate based on the current system state. It presents a dynamically concurrent universal construction that avoids consensus-based synchronization when operations don&#x27;t conflict in the current state. The approach improves scalability by reducing unnecessary synchronization in distributed systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251107] Parallel Spawning Strategies for Dynamic-Aware MPI Applications</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [HPC resource management], [parallel spawning, dynamic resource management, MPI malleability, process redistribution]</li>
<li class=""><strong>authors:</strong> Iker Martín-Álvarez, José I. Aliaga, Maribel Castillo, Sergio Iserte</li>
<li class=""><strong>institution:</strong> Universitat Jaume I, Barcelona Supercomputing Center</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.04268" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.04268</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a novel parallel spawning strategy for MPI applications where all processes cooperate in spawning before redistribution, reducing execution time and enabling better resource adaptation. The method overcomes limitations of existing approaches by allowing full release of unneeded processes during shrink operations while maintaining competitive expansion performance. The strategy achieves up to 1.25× overhead for expansion and reduces shrink operation costs by at least 20×, validated on both homogeneous and heterogeneous systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251107] OMPILOT: Harnessing Transformer Models for Auto Parallelization to Shared Memory Computing Paradigms</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [transformer, encoder-decoder, OpenMP, parallelization, function-level translation, OMPBLEU]</li>
<li class=""><strong>authors:</strong> Arijit Bhattacharjee, Ali TehraniJamsaz, Le Chen, Niranjan Hasabnis, Mihai Capota, Nesreen Ahmed, Ali Jannesari</li>
<li class=""><strong>institution:</strong> Iowa State University, Mako AI, Argonne National Lab, Code Metal, Intel Labs, Cisco Outshift</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.03866" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.03866</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces OMPILOT, a domain-specific encoder-decoder transformer model that translates C++ code to OpenMP for shared-memory parallelization. It uses custom pre-training objectives combining unsupervised and supervised learning to operate at function level rather than just loop level. The approach demonstrates improved robustness in code translation and introduces OMPBLEU, a specialized metric for evaluating OpenMP parallel constructs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251107] Universal Quantum Simulation of 50 Qubits on Europe`s First Exascale Supercomputer Harnessing Its Heterogeneous CPU-GPU Architecture</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [quantum computing simulation], [CPU-GPU heterogeneous architecture, adaptive data encoding, network traffic optimization, LPDDR5 memory, high-bandwidth interconnects]</li>
<li class=""><strong>authors:</strong> Hans De Raedt, Jiri Kraus, Andreas Herten, Vrinda Mehta, Mathis Bode, Markus Hrywniak, Kristel Michielsen, Thomas Lippert</li>
<li class=""><strong>institution:</strong> Jülich Supercomputing Centre, NVIDIA</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.03359" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.03359</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents JUQCS-50, a quantum computer simulator that achieves 50-qubit simulations using three key innovations: extending memory beyond GPU limits through CPU-GPU interconnects and LPDDR5 memory, adaptive data encoding to reduce memory footprint, and network traffic optimization. These methods resulted in an 11.4-fold speedup over previous 48-qubit simulations, demonstrating efficient large-scale quantum circuit simulation on Europe&#x27;s first exascale supercomputer.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251107] Scalable Domain-decomposed Monte Carlo Neutral Transport for Nuclear Fusion</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [high-performance computing], [domain decomposition, Monte Carlo, parallel algorithms, strong scaling, weak scaling]</li>
<li class=""><strong>authors:</strong> Oskar Lappi, Huw Leggate, Yannick Marandet, Jan Åström, Keijo Heljanko, Dmitriy V. Borodin</li>
<li class=""><strong>institution:</strong> University of Helsinki, Dublin City University, Aix-Marseille University, CSC – IT Center for Science Ltd., Forschungszentrum Jülich GmbH, Helsinki Institute for Information Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.04489" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.04489</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a domain-decomposed Monte Carlo algorithm implemented in a new open-source code called Eiron for nuclear fusion simulations. The domain decomposition approach outperforms existing parallel algorithms in scaling tests and enables simulations that were previously impossible due to memory constraints. The method achieved superlinear strong scaling on grids exceeding L3 cache size and scaled efficiently to 16,384 cores.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 13</strong></p>
<ul>
<li class="">[arXiv251107] From Static to Dynamic: Enhancing Offline-to-Online Reinforcement Learning via Energy-Guided Diffusion Stratification <a href="https://arxiv.org/pdf/2511.03828" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251107] RLoop: An Self-Improving Framework for Reinforcement Learning with Iterative Policy Initialization <a href="https://arxiv.org/pdf/2511.04285" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251107] GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents <a href="https://arxiv.org/pdf/2511.04307" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251107] Scaling Agent Learning via Experience Synthesis <a href="https://arxiv.org/pdf/2511.03773" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251107] The Peril of Preference: Why GRPO fails on Ordinal Rewards <a href="https://arxiv.org/pdf/2511.04439" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251107] Fitting Reinforcement Learning Model to Behavioral Data under Bandits <a href="https://arxiv.org/pdf/2511.04454" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251107] Opus: A Quantitative Framework for Workflow Evaluation <a href="https://arxiv.org/pdf/2511.04220" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251107] Environment Agnostic Goal-Conditioning, A Study of Reward-Free Autonomous Learning <a href="https://arxiv.org/pdf/2511.04598" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251107] Shared Spatial Memory Through Predictive Coding <a href="https://arxiv.org/pdf/2511.04235" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251107] Forgetting is Everywhere <a href="https://arxiv.org/pdf/2511.04666" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251107] End-to-End Reinforcement Learning of Koopman Models for eNMPC of an Air Separation Unit <a href="https://arxiv.org/pdf/2511.04522" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251107] RLHF: A comprehensive Survey for Cultural, Multimodal and Low Latency Alignment Methods <a href="https://arxiv.org/pdf/2511.03939" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251107] Exchange Policy Optimization Algorithm for Semi-Infinite Safe Reinforcement Learning <a href="https://arxiv.org/pdf/2511.04147" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 10</strong></p>
<ul>
<li class="">[arXiv251107] Machine Learning for Electron-Scale Turbulence Modeling in W7-X <a href="https://arxiv.org/pdf/2511.04567" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251107] DecoHD: Decomposed Hyperdimensional Classification under Extreme Memory Budgets <a href="https://arxiv.org/pdf/2511.03911" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251107] Data-driven uncertainty-aware seakeeping prediction of the Delft 372 catamaran using ensemble Hankel dynamic mode decomposition <a href="https://arxiv.org/pdf/2511.04461" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251107] GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents <a href="https://arxiv.org/pdf/2511.04307" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251107] One Size Does Not Fit All: Architecture-Aware Adaptive Batch Scheduling with DEBA <a href="https://arxiv.org/pdf/2511.03809" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251107] ARETE: an R package for Automated REtrieval from TExt with large language models <a href="https://arxiv.org/pdf/2511.04573" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251107] OpenMENA: An Open-Source Memristor Interfacing and Compute Board for Neuromorphic Edge-AI Applications <a href="https://arxiv.org/pdf/2511.03747" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251107] A convolutional neural network deep learning method for model class selection <a href="https://arxiv.org/pdf/2511.03743" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251107] Hybrid Fuzzing with LLM-Guided Input Mutation and Semantic Feedback <a href="https://arxiv.org/pdf/2511.03995" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251107] DartQuant: Efficient Rotational Distribution Calibration for LLM Quantization <a href="https://arxiv.org/pdf/2511.04063" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-11-11T15:48:31.000Z" itemprop="dateModified">Nov 11, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/Recommend-System-Note/daily/20251027-20251102"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20251027-20251102</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/Recommend-System-Note/category/paper"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Paper</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-11-03" class="table-of-contents__link toc-highlight">2025-11-03</a></li><li><a href="#2025-11-04" class="table-of-contents__link toc-highlight">2025-11-04</a></li><li><a href="#2025-11-05" class="table-of-contents__link toc-highlight">2025-11-05</a></li><li><a href="#2025-11-06" class="table-of-contents__link toc-highlight">2025-11-06</a></li><li><a href="#2025-11-07" class="table-of-contents__link toc-highlight">2025-11-07</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 xiezilailai, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>