<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/20251110-20251116" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251110-20251116 | Recommend System Note</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://xiezilailai.github.io/Recommend-System-Note/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://xiezilailai.github.io/Recommend-System-Note/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://xiezilailai.github.io/Recommend-System-Note/daily/20251110-20251116"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251110-20251116 | Recommend System Note"><meta data-rh="true" name="description" content="2025-11-12"><meta data-rh="true" property="og:description" content="2025-11-12"><link data-rh="true" rel="icon" href="/Recommend-System-Note/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://xiezilailai.github.io/Recommend-System-Note/daily/20251110-20251116"><link data-rh="true" rel="alternate" href="https://xiezilailai.github.io/Recommend-System-Note/daily/20251110-20251116" hreflang="en"><link data-rh="true" rel="alternate" href="https://xiezilailai.github.io/Recommend-System-Note/daily/20251110-20251116" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"20251110-20251116","item":"https://xiezilailai.github.io/Recommend-System-Note/daily/20251110-20251116"}]}</script><link rel="stylesheet" href="/Recommend-System-Note/assets/css/styles.75e1961b.css">
<script src="/Recommend-System-Note/assets/js/runtime~main.9edb3461.js" defer="defer"></script>
<script src="/Recommend-System-Note/assets/js/main.d625817a.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/Recommend-System-Note/"><div class="navbar__logo"><img src="/Recommend-System-Note/img/favicon.ico" alt="Recommend System Note" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/Recommend-System-Note/img/favicon.ico" alt="Recommend System Note" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Recommend System Note</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/xiezilailai" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Recommend-System-Note/intro"><span title="Introduction" class="linkLabel_WmDU">Introduction</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/Recommend-System-Note/daily/20251110-20251116"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/Recommend-System-Note/daily/20251110-20251116"><span title="20251110-20251116" class="linkLabel_WmDU">20251110-20251116</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/Recommend-System-Note/daily/20251117-20251123"><span title="20251117-20251123" class="linkLabel_WmDU">20251117-20251123</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/Recommend-System-Note/paper/Algorithm"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/Recommend-System-Note/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Daily</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251110-20251116</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251110-20251116</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-11-12">2025-11-12<a href="#2025-11-12" class="hash-link" aria-label="Direct link to 2025-11-12" title="Direct link to 2025-11-12" translate="no">​</a></h2>
<p><strong>cs.DC total: 21</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251112] HyProv: Hybrid Provenance Management for Scientific Workflows</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [hybrid provenance management, centralized component, federated querying, workflow-aware queries, Airflow, Kubernetes]</li>
<li class=""><strong>authors:</strong> Vasilis Bountris, Lauritz Thamsen, Ulf Leser</li>
<li class=""><strong>institution:</strong> Humboldt-Universität zu Berlin, University of Glasgow</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.07574" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.07574</a></li>
<li class=""><strong>Simple LLM Summary:</strong> HyProv introduces a hybrid provenance management system that combines centralized and federated approaches to handle workflow provenance data. The system uses a centralized component for workflow-specific provenance and federated querying for large-scale execution logs. Experiments show that HyProv scales to large workflows, provides sub-second query latencies, and adds minimal overhead to cluster resources.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] Network and Systems Performance Characterization of MCP-Enabled LLM Agents</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [Model Context Protocol, token efficiency, parallel tool calls, task abort mechanisms, performance characterization]</li>
<li class=""><strong>authors:</strong> Zihao Ding, Mufeng Zhu, Yao Liu</li>
<li class=""><strong>institution:</strong> Rutgers University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.07426" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.07426</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper conducts a measurement-based analysis of MCP-enabled LLM interactions, examining how different models and configurations affect token usage, costs, and performance. The study reveals significant trade-offs between capability enhancement and increased computational overhead in MCP workflows. The findings suggest optimizations like parallel tool calls to develop more efficient and cost-effective agent systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] Enhancing reliability in AI inference services: An empirical study on real production incidents</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [incident taxonomy, traffic routing, GPU capacity-aware routing, connection liveness, endpoint isolation, auto-detection, hotfix]</li>
<li class=""><strong>authors:</strong> Bhala Ranganathan, Mickey Zhang, Kai Wu</li>
<li class=""><strong>institution:</strong> Microsoft</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.07424" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.07424</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents an empirical study of 156 high-severity production incidents in large language model inference services, developing a taxonomy and methodology grounded in operational experience. The research identifies dominant failure modes and mitigation strategies, showing that systematic analysis can drive more reliable and cost-efficient LLM serving at scale. The study also provides a practitioner-oriented adoption checklist to help others replicate their approach.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] DynaKV: Enabling Accurate and Efficient Long-Sequence LLM Decoding on Smartphones</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [KVCache management, cluster adaptation, flash management, memory virtualization, retrieval-based methods]</li>
<li class=""><strong>authors:</strong> Tuowei Wang, Minxing Huang, Fengzu Li, Ligeng Chen, Jinrui Zhang, Ju Ren</li>
<li class=""><strong>institution:</strong> Tsinghua University, Honor Device Co., Ltd.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.07427" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.07427</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DynaKV is an adaptive KVCache management system for smartphones that uses migration-free cluster adaptation, continuity-centric flash management, and memory-efficient cache design to handle long-sequence LLM decoding. It improves retrieval accuracy by 1.38× and reduces latency by 1.47× compared to state-of-the-art solutions while addressing smartphone-specific memory and bandwidth constraints.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] An Evaluation of LLMs Inference on Popular Single-board Computers</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [quantization, on-device inference, edge computing, benchmarking, single-board computers]</li>
<li class=""><strong>authors:</strong> Tung, Nguyen, Tuyen Nguyen</li>
<li class=""><strong>institution:</strong> BillulloNex, University of Technology Sydney</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.07425" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.07425</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper benchmarks the performance of 25 quantized LLMs across three single-board computers using Ollama and Llamafile runtimes. The study evaluates generation throughput, memory usage, and power consumption under realistic workloads. Results show SBCs can reliably support models up to 1.5B parameters, with Llamafile achieving 4x higher throughput and 30-40% lower power usage than Ollama.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] From Attention to Disaggregation: Tracing the Evolution of LLM Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [disaggregated inference, distributed systems, service decomposition, resource disaggregation, workload partitioning, prefill phase, decode phase]</li>
<li class=""><strong>authors:</strong> Madabattula Rajesh Kumar, Srinivasa Rao Aravilli, Mustafa Saify, Shashank Srivastava</li>
<li class=""><strong>institution:</strong> Capital One</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.07422" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.07422</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes disaggregated inference as an architectural shift for LLM deployment, separating compute-intensive prefill from memory-intensive decode phases into independently scalable components. This approach addresses the multi-objective optimization challenge of minimizing latency while maximizing throughput and reducing costs. The main conclusion is that disaggregation mitigates resource contention and enables independent optimization of key inference metrics like Time to First Token and Inter Token Latency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] SemanticForge: Repository-Level Code Generation through Semantic Knowledge Graphs and Constraint Satisfaction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [semantic knowledge graphs, constraint satisfaction, SMT solving, beam search, dual static-dynamic graphs]</li>
<li class=""><strong>authors:</strong> Wuyang Zhang, Chenkai Zhang, Zhen Luo, Jianming Ma, Wangming Yuan, Chuqiao Gu, Chenwei Feng</li>
<li class=""><strong>institution:</strong> University of Massachusetts Amherst, Northeastern University, George Mason University, Carnegie Mellon University, Auckland University of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.07584" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.07584</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SemanticForge introduces a repository-level code generation system that combines semantic knowledge graphs with constraint satisfaction to address LLM hallucinations. The system uses dual static-dynamic knowledge graphs and integrated SMT solving during beam search to ensure semantic correctness. Evaluation shows 49.8% Pass@1 performance with significant reductions in logical and schematic hallucinations while maintaining sub-3s latency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] Towards Affordable, Adaptive and Automatic GNN Training on CPU-GPU Heterogeneous Platforms</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [locality-aware sampling, fine-grained parallelism scheduling, reinforcement learning, CPU-GPU heterogeneous platforms]</li>
<li class=""><strong>authors:</strong> Tong Qiao, Ao Zhou, Yingjie Qi, Yiou Wang, Han Wan, Jianlei Yang, Chunming Hu</li>
<li class=""><strong>institution:</strong> Beihang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.07421" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.07421</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces A3GNN, a framework that optimizes GNN training on CPU-GPU platforms through locality-aware sampling and fine-grained parallelism scheduling, using reinforcement learning to achieve optimal trade-offs. The system enables affordable GNN training by efficiently utilizing resource-constrained hardware. Experiments show it can outperform high-end GPUs, with seven 2080Ti GPUs achieving up to 1.8× higher throughput than two A100 GPUs with minimal accuracy loss.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] Synera: Synergistic LLM Serving across Device and Cloud at Scale</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [device-cloud synergy, selective offloading, parallel inference, scalable batching, SLM-LLM synergy]</li>
<li class=""><strong>authors:</strong> Genglin Wang, Liekang Zeng, Bufang Yang, Kaiwei Liu, Guoliang Xing, Chumin Sun, Li Zhou, Jie Sun, Zhenyu Yan</li>
<li class=""><strong>institution:</strong> The Chinese University of Hong Kong, Huawei Technologies Co. Ltd.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.07423" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.07423</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Synera is a device-cloud synergistic LLM serving system that uses an efficient SLM-LLM synergistic mechanism with communication-efficient selective offloading, stall-free parallel inference, and scalable cloud batching. The system achieves 1.20-5.47× better generation quality compared to baselines while maintaining similar latency, and reduces cloud serving costs by 8.2-16.5% compared to existing cloud serving approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] Parallel Sampling via Autospeculation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [speculative rejection sampling, autospeculation, parallel sampling]</li>
<li class=""><strong>authors:</strong> Nima Anari, Carlo Baronio, CJ Chen, Alireza Haqi, Frederic Koehler, Anqi Li, Thuy-Duong Vuong</li>
<li class=""><strong>institution:</strong> Stanford University, University of Arizona, University of Chicago, UC Berkeley</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.07869" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.07869</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces speculative rejection sampling, a novel technique that uses autospeculation to accelerate sampling from autoregressive and diffusion models. By building speculative distributions from the same oracle that defines the target distribution and making sequence-level speculations, the method achieves parallel sampling. This reduces expected sampling time from O(n) to O(n^{1/2}), improving previous bounds and providing the first parallel speedup for diffusion models in high-accuracy regimes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] BIPPO: Budget-Aware Independent PPO for Energy-Efficient Federated Learning Services</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, reinforcement learning, proximal policy optimization, client selection, energy efficiency, multi-agent RL]</li>
<li class=""><strong>authors:</strong> Anna Lackinger, Andrea Morichetta, Pantelis A. Frangoudis, Schahram Dustdar</li>
<li class=""><strong>institution:</strong> TU Wien</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.08142" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.08142</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes BIPPO, a budget-aware independent proximal policy optimization method for energy-efficient client selection in federated learning. This multi-agent reinforcement learning approach improves performance while consuming minimal budget in resource-constrained IoT environments. Experimental results show BIPPO achieves higher accuracy than traditional methods while maintaining scalability and sustainability.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] UniFormer: Unified and Efficient Transformer for Reasoning Across General and Custom Computing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [transformer architecture, parallelism, compute-storage fusion, FPGA optimization, GPU acceleration]</li>
<li class=""><strong>authors:</strong> Zhuoheng Ran, Chong Wu, Renjie Xu, Maolin Che, Hong Yan</li>
<li class=""><strong>institution:</strong> City University of Hong Kong, Guizhou University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.08135" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.08135</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces UniFormer, a unified Transformer architecture designed for both general-purpose and custom computing platforms. It achieves higher parallelism and compute-storage fusion to optimize performance across different hardware. The method demonstrates state-of-the-art accuracy and latency on GPUs while maintaining strong adaptability on FPGAs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] Intelligence per Watt: Measuring Intelligence Efficiency of Local AI</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [intelligence per watt, local inference, power efficiency, model-accelerator pairs, empirical benchmarking]</li>
<li class=""><strong>authors:</strong> Jon Saad-Falcon, Avanika Narayan, Hakki Orhun Akengin, J. Wes Griffin, Herumb Shandilya, Adrian Gamarra Lafuente, Medhya Goel, Rebecca Joseph, Shlok Natarajan, Etash Kumar Guha, Shang Zhu, Ben Athiwaratkun, John Hennessy, Azalia Mirhoseini, Christopher Ré</li>
<li class=""><strong>institution:</strong> Stanford University, Together AI</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.07885" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.07885</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Intelligence Per Watt (IPW) as a metric to evaluate the efficiency of local AI inference across model-accelerator pairs. Through large-scale empirical analysis of local language models and hardware accelerators, the study demonstrates that local inference can accurately handle most single-turn queries and meaningfully redistribute demand from centralized cloud infrastructure. The findings show significant improvements in IPW over time and reveal optimization opportunities for local accelerators.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] Generic Algorithm for Universal TDM Communication Over Inter Satellite Links</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [TDM communication, federated learning, inter satellite links, time division multiplexing, peer data exchange, satellite constellations]</li>
<li class=""><strong>authors:</strong> Miroslav Popovic, Marko Popovic, Pavle Vasiljevic, Ilija Basicevic</li>
<li class=""><strong>institution:</strong> University of Novi Sad, RT-RK Institute for Computer Based Systems</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.08034" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.08034</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a new generic algorithm for universal TDM communication that extends beyond pairwise node communication, allowing nodes to communicate with multiple peers simultaneously. The algorithm was developed within a federated learning framework and specifically addresses communication needs for satellite constellations with multiple antennas. The main advantage is enabling real-world TDM communications over inter satellite links for applications like orbit determination and time synchronization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] ProbSelect: Stochastic Client Selection for GPU-Accelerated Compute Devices in the 3D Continuum</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [probabilistic forecasting, analytical modeling, GPU-accelerated training, federated learning, client selection]</li>
<li class=""><strong>authors:</strong> Andrija Stanisic, Stefan Nastic</li>
<li class=""><strong>institution:</strong> TU Wien</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.08147" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.08147</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces ProbSelect, a novel client selection approach for federated learning that uses analytical modeling and probabilistic forecasting to select GPU-accelerated devices in the 3D compute continuum. The method operates without requiring historical data or continuous monitoring and models client selection within user-defined SLOs. Evaluation shows ProbSelect improves SLO compliance by 13.77% on average while reducing computational waste by 72.5% compared to baseline approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] ACGraph: An Efficient Asynchronous Out-of-Core Graph Processing Framework</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [graph processing systems], [asynchronous execution, out-of-core processing, block-centric scheduling, hybrid storage format, SSD optimization]</li>
<li class=""><strong>authors:</strong> Dechuang Chen, Sibo Wang, Qintian Guo</li>
<li class=""><strong>institution:</strong> The Chinese University of Hong Kong, The Hong Kong University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.07886" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.07886</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ACGraph introduces an asynchronous out-of-core graph processing framework that uses dynamic block-centric scheduling and pipelined I/O-computation execution to overcome limitations of synchronous systems. It employs an online asynchronous worklist and hybrid storage format to minimize redundant disk accesses and optimize memory usage. Experimental results show ACGraph significantly outperforms state-of-the-art out-of-core graph processing systems in both runtime and I/O efficiency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] Forgetting Alternation and Blossoms: A New Framework for Fast Matching Augmentation and Its Applications to Sequential/Distributed/Streaming Computation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [graph algorithms], [alternating base trees, maximum cardinality matching, shortest alternating paths, matching augmentation]</li>
<li class=""><strong>authors:</strong> Taisuke Izumi, Naoki Kitamura, Yutaro Yamaguchi</li>
<li class=""><strong>institution:</strong> Osaka University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.08210" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.08210</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a new framework for maximum matching that simplifies the complex structure of shortest alternating paths by &quot;forgetting alternation&quot; and using alternating base trees. This approach yields a more implementable algorithm that is easier to verify than the classical Micali-Vazirani algorithm. The framework also enables improved deterministic approximation algorithms for distributed and streaming settings with substantially better time bounds.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] Foam Segmentation in Wastewater Treatment Plants: A Federated Learning Approach with Segment Anything Model 2</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [Federated Learning, Segment Anything Model 2, Image Segmentation, Flower framework, Fog computing]</li>
<li class=""><strong>authors:</strong> Mehmet Batuhan Duman, Alejandro Carnero, Cristian Martín, Daniel Garrido, Manuel Díaz</li>
<li class=""><strong>institution:</strong> ITIS Software, University of Malaga</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.08130" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.08130</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a framework combining Federated Learning with Segment Anything Model 2 (SAM2) for foam segmentation in wastewater treatment plants. The approach enables privacy-preserving collaborative training across multiple plants without sharing sensitive data while leveraging SAM2&#x27;s pre-trained weights for improved performance. The research demonstrates that integrating large-scale foundational models with federated learning provides a practical solution for industrial applications with distributed and sensitive data.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] Gathering in Vertex- and Edge-Transitive Graphs without Multiplicity Detection under Round Robin</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed robotics algorithms], [round-robin scheduling, vertex-transitive graphs, edge-transitive graphs, OBLOT model, multiplicity detection]</li>
<li class=""><strong>authors:</strong> Serafino Cicerone, Alessia Di Fonso, Gabriele Di Stefano, Alfredo Navarra</li>
<li class=""><strong>institution:</strong> Università degli Studi dell&#x27;Aquila, Università degli Studi di Perugia</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.08222" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.08222</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes distributed algorithms for solving the Gathering problem in swarm robotics on vertex- and edge-transitive graphs under hostile conditions where robots cannot detect multiplicities. The algorithms specifically target infinite grids and hypercubes, exploiting their topological properties to achieve time-optimal performance. The authors conclude that no general algorithm likely exists for all solvable cases due to the heavy reliance on specific graph properties.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] LOw-cost yet High-Performant Sparse Matrix-Matrix Multiplication on Arm SME Architectures</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [SME, NEON, CSR, BCSR, two-level parallelization, performance modeling]</li>
<li class=""><strong>authors:</strong> Kelun Lei, Hailong Yang, Kaige Zhang, Kejie Ma, Yiqing Wang, Xin You, Yufan Xu, Enrique S. Quintana-Orti, Zhongzhi Luan, Yi Liu, Depei Qian</li>
<li class=""><strong>institution:</strong> Beihang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.08158" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.08158</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes LOOPS, a hybrid execution framework that combines row-wise CSR and vector-wise BCSR layouts to cooperatively utilize both NEON vector instructions and Arm SME matrix extensions for sparse matrix multiplication. Experimental results show LOOPS achieves significant speedups over CPU baselines and GPU methods while delivering better energy efficiency on Apple M4Pro CPUs compared to NVIDIA A100 GPU implementations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] Priority Matters: Optimising Kubernetes Clusters Usage with Constraint-Based Pod Packing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [constraint programming, pod packing, OR-Tools, Kubernetes scheduler, resource optimization]</li>
<li class=""><strong>authors:</strong> Henrik Daniel Christensen, Saverio Giallorenzo, Jacopo Mauro</li>
<li class=""><strong>institution:</strong> University of Southern Denmark, University of Bologna</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.08373" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.08373</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes using constraint programming with OR-Tools to optimize Kubernetes pod scheduling, implemented as a fallback plugin when the default scheduler fails. The method improves pod placement by finding optimal allocations that satisfy priorities and resource requirements. Experimental results show the approach places more high-priority pods in over 44% of problematic scenarios within 1 second, and over 73% within 10 seconds, while certifying default scheduler optimality in 19% of cases.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 31</strong></p>
<ul>
<li class="">[arXiv251112] A Negotiation-Based Multi-Agent Reinforcement Learning Approach for Dynamic Scheduling of Reconfigurable Manufacturing Systems <a href="https://arxiv.org/pdf/2511.07707" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Think Before You Retrieve: Learning Test-Time Adaptive Search with Small Language Models <a href="https://arxiv.org/pdf/2511.07581" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Diffusion Guided Adversarial State Perturbations in Reinforcement Learning <a href="https://arxiv.org/pdf/2511.07701" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Partial Action Replacement: Tackling Distribution Shift in Offline MARL <a href="https://arxiv.org/pdf/2511.07629" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] The Polite Liar: Epistemic Pathology in Language Models <a href="https://arxiv.org/pdf/2511.07477" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] RELEAP: Reinforcement-Enhanced Label-Efficient Active Phenotyping for Electronic Health Records <a href="https://arxiv.org/pdf/2511.07473" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] MURPHY: Multi-Turn GRPO for Self Correcting Code Generation <a href="https://arxiv.org/pdf/2511.07833" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] From Exploration to Exploitation: A Two-Stage Entropy RLVR Approach for Noise-Tolerant MLLM Training <a href="https://arxiv.org/pdf/2511.07738" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Beyond Correctness: Confidence-Aware Reward Modeling for Enhancing Large Language Model Reasoning <a href="https://arxiv.org/pdf/2511.07483" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Intelligent Optimization of Multi-Parameter Micromixers Using a Scientific Machine Learning Framework <a href="https://arxiv.org/pdf/2511.07702" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] ZeroSim: Zero-Shot Analog Circuit Evaluation with Unified Transformer Embeddings <a href="https://arxiv.org/pdf/2511.07658" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Statistically Assuring Safety of Control Systems using Ensembles of Safety Filters and Conformal Prediction <a href="https://arxiv.org/pdf/2511.07899" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Test-driven Reinforcement Learning <a href="https://arxiv.org/pdf/2511.07904" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Feedback Descent: Open-Ended Text Optimization via Pairwise Comparison <a href="https://arxiv.org/pdf/2511.07919" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] SERL: Self-Examining Reinforcement Learning on Open-Domain <a href="https://arxiv.org/pdf/2511.07922" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] SpeechJudge: Towards Human-Level Judgment for Speech Naturalness <a href="https://arxiv.org/pdf/2511.07931" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Thinker: Training LLMs in Hierarchical Thinking for Deep Search via Multi-Turn Interaction <a href="https://arxiv.org/pdf/2511.07943" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Knowledge-Augmented Long-CoT Generation for Complex Biomolecular Reasoning <a href="https://arxiv.org/pdf/2511.08024" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Dynamic Sparsity: Challenging Common Sparsity Assumptions for Learning World Models in Robotic Reinforcement Learning Benchmarks <a href="https://arxiv.org/pdf/2511.08086" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] An Efficient Training Pipeline for Reasoning Graphical User Interface Agents <a href="https://arxiv.org/pdf/2511.08172" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Beyond Distributions: Geometric Action Control for Continuous Reinforcement Learning <a href="https://arxiv.org/pdf/2511.08234" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] PrefPoE: Advantage-Guided Preference Fusion for Learning Where to Explore <a href="https://arxiv.org/pdf/2511.08241" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Where and What Matters: Sensitivity-Aware Task Vectors for Many-Shot Multimodal In-Context Learning <a href="https://arxiv.org/pdf/2511.08246" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] AgentPRM: Process Reward Models for LLM Agents via Step-Wise Promise and Progress <a href="https://arxiv.org/pdf/2511.08325" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] LPPG-RL: Lexicographically Projected Policy Gradient Reinforcement Learning with Subproblem Exploration <a href="https://arxiv.org/pdf/2511.08339" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] ARAC: Adaptive Regularized Multi-Agent Soft Actor-Critic in Graph-Structured Adversarial Games <a href="https://arxiv.org/pdf/2511.08412" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Understanding Electro-communication and Electro-sensing in Weakly Electric Fish using Multi-Agent Deep Reinforcement Learning <a href="https://arxiv.org/pdf/2511.08436" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] The Path Not Taken: RLVR Provably Learns Off the Principals <a href="https://arxiv.org/pdf/2511.08567" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] DeepProofLog: Efficient Proving in Deep Stochastic Logic Programs <a href="https://arxiv.org/pdf/2511.08581" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Shocks Under Control: Taming Transonic Compressible Flow over an RAE2822 Airfoil with Deep Reinforcement Learning <a href="https://arxiv.org/pdf/2511.07564" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Distributionally Robust Online Markov Game with Linear Function Approximation <a href="https://arxiv.org/pdf/2511.07831" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 16</strong></p>
<ul>
<li class="">[arXiv251112] TurboSAT: Gradient-Guided Boolean Satisfiability Accelerated on GPU-CPU Hybrid System <a href="https://arxiv.org/pdf/2511.07737" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] A Negotiation-Based Multi-Agent Reinforcement Learning Approach for Dynamic Scheduling of Reconfigurable Manufacturing Systems <a href="https://arxiv.org/pdf/2511.07707" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Operational machine learning for remote spectroscopic detection of CH<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mn>4</mn></msub></mrow><annotation encoding="application/x-tex">_{4}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4511em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> point sources <a href="https://arxiv.org/pdf/2511.07719" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] FractalCloud: A Fractal-Inspired Architecture for Efficient Large-Scale Point Cloud Processing <a href="https://arxiv.org/pdf/2511.07665" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Hyperellipsoid Density Sampling: Exploitative Sequences to Accelerate High-Dimensional Optimization <a href="https://arxiv.org/pdf/2511.07836" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] A Self-Improving Architecture for Dynamic Safety in Large Language Models <a href="https://arxiv.org/pdf/2511.07645" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Streaming Tensor Program: A streaming abstraction for dynamic parallelism <a href="https://arxiv.org/pdf/2511.07776" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Sharp Eyes and Memory for VideoLLMs: Information-Aware Visual Token Pruning for Efficient and Reliable VideoLLM Reasoning <a href="https://arxiv.org/pdf/2511.08003" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Improving Long-Range Interactions in Graph Neural Simulators via Hamiltonian Dynamics <a href="https://arxiv.org/pdf/2511.08185" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Dual-Kernel Graph Community Contrastive Learning <a href="https://arxiv.org/pdf/2511.08287" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] LPPG-RL: Lexicographically Projected Policy Gradient Reinforcement Learning with Subproblem Exploration <a href="https://arxiv.org/pdf/2511.08339" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] NeuCLIP: Efficient Large-Scale CLIP Training with Neural Normalizer Optimization <a href="https://arxiv.org/pdf/2511.08417" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Benchmarking Simulacra AI&#x27;s Quantum Accurate Synthetic Data Generation for Chemical Sciences <a href="https://arxiv.org/pdf/2511.07433" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Emulating Radiative Transfer in Astrophysical Environments <a href="https://arxiv.org/pdf/2511.08219" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Generative AI Meets 6G and Beyond: Diffusion Models for Semantic Communications <a href="https://arxiv.org/pdf/2511.08416" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Galactification: painting galaxies onto dark matter only simulations using a transformer-based model <a href="https://arxiv.org/pdf/2511.08438" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-11-13">2025-11-13<a href="#2025-11-13" class="hash-link" aria-label="Direct link to 2025-11-13" title="Direct link to 2025-11-13" translate="no">​</a></h2>
<p><strong>cs.DC total: 13</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251113] FedPM: Federated Learning Using Second-order Optimization with Preconditioned Mixing of Local Parameters</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, second-order optimization, preconditioned mixing, local parameter updates, convergence analysis]</li>
<li class=""><strong>authors:</strong> Hiro Ishii, Kenta Niwa, Hiroshi Sawada, Akinori Fujino, Noboru Harada, Rio Yokota</li>
<li class=""><strong>institution:</strong> Institute of Science Tokyo, NTT Communication Science Laboratories</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.09100" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.09100</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FedPM introduces a novel federated learning method that uses second-order optimization with preconditioned mixing of local parameters to address drift issues in local preconditioners. The approach decomposes ideal second-order updates into server-side parameter mixing and client-side local updates. Experimental results show significant improvements in test accuracy compared to conventional methods with simple mixing.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251113] Attack-Centric by Design: A Program-Structure Taxonomy of Smart Contract Vulnerabilities</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [blockchain security], [vulnerability taxonomy, static analysis, dynamic analysis, learning-based tools, root-cause analysis, program structure]</li>
<li class=""><strong>authors:</strong> Parsa Hedayatnia, Tina Tavakkoli, Hadi Amini, Mohammad Allahbakhsh, Haleh Amintoosi</li>
<li class=""><strong>institution:</strong> Ferdowsi University of Mashhad</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.09051" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.09051</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces an attack-centric program-structure taxonomy that organizes smart contract vulnerabilities into eight root-cause families. The taxonomy provides a unified framework for vulnerability detection, audit reproducibility, and security education. It reveals coverage gaps in existing datasets and enables more interpretable security analysis through structural classification.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251113] Minimize Your Critical Path with Combine-and-Exchange Locks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [concurrency and synchronization], [Combine-and-Exchange Scheduling, coroutines, userspace scheduling, cooperative multitasking, critical sections]</li>
<li class=""><strong>authors:</strong> Simon König, Lukas Epple, Christian Becker</li>
<li class=""><strong>institution:</strong> University of Stuttgart</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.09194" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.09194</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Combine-and-Exchange Scheduling (CES), a novel scheduling approach for userspace tasks like coroutines that keeps contended critical sections on the same thread while distributing parallelizable work across other threads. The method addresses limitations in existing userspace synchronization primitives that introduce unnecessary delays. The approach achieves 3-fold performance improvements in application benchmarks and 8-fold improvements in microbenchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251113] An MLIR pipeline for offloading Fortran to FPGAs via OpenMP</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [compiler systems], [MLIR, OpenMP, FPGA offloading, High-Level Synthesis, Flang]</li>
<li class=""><strong>authors:</strong> Gabriel Rodriguez-Canal, David Katz, Nick Brown</li>
<li class=""><strong>institution:</strong> EPCC, The University of Edinburgh</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.08713" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.08713</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents an MLIR-based compilation pipeline that enables Fortran code offloading to FPGAs using OpenMP target directives. The approach combines MLIR&#x27;s OpenMP dialect with a High-Level Synthesis dialect to create a portable FPGA compilation flow. The work demonstrates that building upon existing MLIR components significantly reduces development effort and provides a flexible path for directive-based FPGA acceleration within the MLIR ecosystem.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251113] CheetahGIS: Architecting a Scalable and Efficient Streaming Spatial Query Processing System</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [spatial data processing], [Apache Flink Stateful Functions, grid-based indexing, load balancing, streaming spatial queries]</li>
<li class=""><strong>authors:</strong> Jiaping Cao, Ting Sun, Man Lung Yiu, Xiao Yan, Bo Tang</li>
<li class=""><strong>institution:</strong> Hong Kong Polytechnic University, Southern University of Science and Technology, Wuhan University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.09262" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.09262</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CheetahGIS is a scalable streaming spatial query processing system built on Apache Flink Stateful Functions with optimizations including lightweight grid indexing and load balancing. The system efficiently handles massive moving objects and real-time spatial queries through its modular architecture. Experimental results demonstrate its excellent scalability and performance for various streaming spatial query types.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251113] Evaluating HPC-Style CPU Performance and Cost in Virtualized Cloud Infrastructures</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [cloud computing performance evaluation], [SPEC ACCEL, OpenMP workloads, virtualized cloud infrastructure, CPU performance benchmarking]</li>
<li class=""><strong>authors:</strong> Jay Tharwani, Shobhit Aggarwal, Arnab A Purkayastha</li>
<li class=""><strong>institution:</strong> University of North Carolina at Charlotte, The Citadel, Western New England University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.08948" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.08948</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper evaluates HPC-style CPU performance and cost across four major cloud providers using SPEC ACCEL OpenMP workloads on Intel, AMD, and ARM instances. The study found that AWS delivers the fastest performance but at a premium cost, while OCI emerges as the most economical option despite slower runtimes. The research demonstrates that instance selection and provider choice significantly impact both runtime and pricing, suggesting workload priorities should guide deployment decisions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251113] No Cords Attached: Coordination-Free Concurrent Lock-Free Queues</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [concurrent data structures], [Cyclic Memory Protection, lock-free queues, coordination-free, linearizability, bounded reclamation]</li>
<li class=""><strong>authors:</strong> Yusuf Motiwala</li>
<li class=""><strong>institution:</strong> mesibo, PatANN</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.09410" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.09410</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Cyclic Memory Protection (CMP), a coordination-free concurrent lock-free queue that uses bounded protection windows to provide practical reclamation guarantees. The method preserves strict FIFO ordering, unbounded capacity, and lock-free progress while significantly outperforming state-of-the-art queues by up to 4x under high contention. The work demonstrates that highly concurrent queues can maintain fundamental simplicity without compromising queue semantics.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251113] Flex-MIG: Enabling Distributed Execution on MIG</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [GPU sharing, MIG, fragmentation reduction, host-shared-memory collectives, one-to-many allocation]</li>
<li class=""><strong>authors:</strong> Myungsu Kim, Ikjun Yeom, Younghoon Kim</li>
<li class=""><strong>institution:</strong> SungKyunKwan University, Ajou University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.09143" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.09143</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Flex-MIG is a software framework that replaces the conventional one-to-one allocation model with a one-to-many model for NVIDIA MIG GPUs, enabling distributed execution across MIG instances using host-shared-memory collectives. This approach eliminates drain-required reconfigurations and reduces fragmentation without hardware modifications. The system improves cluster efficiency by up to 17% in makespan across diverse workload traces.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251113] Distribution and Management of Datacenter Load Decoupling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [datacenter energy management], [load adaptation, energy resources, datacenter-grid cooperation, power capacity decoupling]</li>
<li class=""><strong>authors:</strong> Liuzixuan Lin, Andrew A. Chien</li>
<li class=""><strong>institution:</strong> University of Chicago, Argonne National Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.08936" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.08936</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes datacenter load decoupling using energy resources to separate power capacity from grid load, enabling datacenter flexibility without compromising capacity. The study shows optimized distribution achieves 98% grid carbon reduction with 70% decoupling needs, and DC-grid cooperation provides 1.4x greater carbon reduction. Economic analysis suggests decoupling is viable on average but may require grid intervention due to site variations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251113] Experiences Building Enterprise-Level Privacy-Preserving Federated Learning to Power AI for Science</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [federated learning, differential privacy, secure aggregation, confidential computing, hybrid cloud-HPC architecture]</li>
<li class=""><strong>authors:</strong> Zilinghan Li, Aditya Sinha, Yijiang Li, Kyle Chard, Kibaek Kim, Ravi Madduri</li>
<li class=""><strong>institution:</strong> Argonne National Laboratory, University of Illinois at Urbana-Champaign, University of Chicago</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.08998" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.08998</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents the Advanced Privacy-Preserving Federated Learning (APPFL) framework designed to enable collaborative AI model training without centralized data sharing. It focuses on building an enterprise-level system that bridges local prototyping with distributed deployment across heterogeneous infrastructures while ensuring privacy through techniques like differential privacy and secure aggregation. The framework aims to provide scalable, reliable, and privacy-preserving AI for scientific applications by addressing challenges in distributed execution across diverse computing environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251113] Formal Verification of a Generic Algorithm for TDM Communication Over Inter Satellite Links</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [formal verification, CSP process algebra, model checking, deadlock freeness, successful termination]</li>
<li class=""><strong>authors:</strong> Miroslav Popovic, Marko Popovic, Pavle Vasiljevic, Miodrag Djukic</li>
<li class=""><strong>institution:</strong> University of Novi Sad, RT-RK Institute for Computer Based Systems</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.09485" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.09485</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper formally verifies a TDM communication algorithm for satellite constellations using CSP process algebra and the PAT model checker. The verification process involves constructing a CSP model from Python code and proving deadlock freeness and successful termination properties. The main conclusion is that the algorithm&#x27;s correctness was automatically verified, ensuring its reliability for safety-critical edge systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251113] SPADA: A Spatial Dataflow Architecture Programming Language</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [spatial dataflow architectures, programming language, dataflow semantics, network-on-chip, compiler design, stencil DSL]</li>
<li class=""><strong>authors:</strong> Lukas Gianinazzi, Tal Ben-Nun, Torsten Hoefler</li>
<li class=""><strong>institution:</strong> ETH Zurich, Lawrence Livermore National Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.09447" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.09447</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SPADA is a programming language that provides precise control over data placement, dataflow patterns, and asynchronous operations for spatial dataflow architectures while abstracting low-level details. It enables developers to express complex parallel patterns in 6-8x less code than existing approaches while achieving near-ideal weak scaling. The language serves as both a high-level programming interface and intermediate representation for domain-specific languages, advancing both theoretical foundations and practical usability of spatial dataflow architectures.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251113] LLM Inference Beyond a Single Node: From Bottlenecks to Mitigations with Fast All-Reduce Communication</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [all-reduce, tensor parallelism, NVSHMEM, model parallelism, distributed inference]</li>
<li class=""><strong>authors:</strong> Prajwal Singhania, Siddharth Singh, Lannie Dalton Hough, Akarsh Srivastava, Harshitha Menon, Charles Fredrick Jekel, Abhinav Bhatele</li>
<li class=""><strong>institution:</strong> University of Maryland, Lawrence Livermore National Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.09557" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.09557</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces NVRAR, a hierarchical all-reduce algorithm using NVSHMEM to optimize communication bottlenecks in multi-node LLM inference. The method achieves up to 3.6x lower latency than NCCL and reduces end-to-end batch latency by 1.72x for large models like Llama 3.1 405B. The research demonstrates that optimized collective communication significantly improves performance in distributed inference workloads.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 21</strong></p>
<ul>
<li class="">[arXiv251113] TIGER-MARL: Enhancing Multi-Agent Reinforcement Learning with Temporal Information through Graph-based Embeddings and Representations <a href="https://arxiv.org/pdf/2511.08832" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Planning in Branch-and-Bound: Model-Based Reinforcement Learning for Exact Combinatorial Optimization <a href="https://arxiv.org/pdf/2511.09219" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] A Distributed Training Architecture For Combinatorial Optimization <a href="https://arxiv.org/pdf/2511.09261" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Interpretable by Design: Query-Specific Neural Modules for Explainable Reinforcement Learning <a href="https://arxiv.org/pdf/2511.08749" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Efficient Reasoning via Reward Model <a href="https://arxiv.org/pdf/2511.09158" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Iterated Population Based Training with Task-Agnostic Restarts <a href="https://arxiv.org/pdf/2511.09190" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Structured Uncertainty guided Clarification for LLM Agents <a href="https://arxiv.org/pdf/2511.08798" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Achieving Equilibrium under Utility Heterogeneity: An Agent-Attention Framework for Multi-Agent Multi-Objective Reinforcement Learning <a href="https://arxiv.org/pdf/2511.08926" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Potent but Stealthy: Rethink Profile Pollution against Sequential Recommendation via Bi-level Constrained Reinforcement Paradigm <a href="https://arxiv.org/pdf/2511.09392" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Diffusion Policies with Value-Conditional Optimization for Offline Reinforcement Learning <a href="https://arxiv.org/pdf/2511.08922" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning <a href="https://arxiv.org/pdf/2511.09109" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Advancing Autonomous Emergency Response Systems: A Generative AI Perspective <a href="https://arxiv.org/pdf/2511.09044" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Towards a Generalisable Cyber Defence Agent for Real-World Computer Networks <a href="https://arxiv.org/pdf/2511.09114" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] History-Aware Reasoning for GUI Agents <a href="https://arxiv.org/pdf/2511.09127" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] UCO: A Multi-Turn Interactive Reinforcement Learning Method for Adaptive Teaching with Large Language Models <a href="https://arxiv.org/pdf/2511.08873" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation and Historical Revisiting <a href="https://arxiv.org/pdf/2511.09478" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Quasi-Newton Compatible Actor-Critic for Deterministic Policies <a href="https://arxiv.org/pdf/2511.09509" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] WMPO: World Model-based Policy Optimization for Vision-Language-Action Models <a href="https://arxiv.org/pdf/2511.09515" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Optimal Control of the Future via Prospective Foraging <a href="https://arxiv.org/pdf/2511.08717" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Practical considerations when designing an online learning algorithm for an app-based mHealth intervention <a href="https://arxiv.org/pdf/2511.08719" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] DRL-Based Beam Positioning for LEO Satellite Constellations with Weighted Least Squares <a href="https://arxiv.org/pdf/2511.08852" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 9</strong></p>
<ul>
<li class="">[arXiv251113] Data reuse enables cost-efficient randomized trials of medical AI models <a href="https://arxiv.org/pdf/2511.08986" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Leveraging Large Language Models for Use Case Model Generation from Software Requirements <a href="https://arxiv.org/pdf/2511.09231" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] CoCo-MILP: Inter-Variable Contrastive and Intra-Constraint Competitive MILP Solution Prediction <a href="https://arxiv.org/pdf/2511.09209" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] FSampler: Training Free Acceleration of Diffusion Sampling via Epsilon Extrapolation <a href="https://arxiv.org/pdf/2511.09180" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Enabling Agents to Communicate Entirely in Latent Space <a href="https://arxiv.org/pdf/2511.09149" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Tele-LLM-Hub: Building Context-Aware Multi-Agent LLM Systems for Telecom Networks <a href="https://arxiv.org/pdf/2511.09087" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Enhancing Explainability in Solar Energetic Particle Event Prediction: A Global Feature Mapping Approach <a href="https://arxiv.org/pdf/2511.09475" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Fluence Map Prediction with Deep Learning: A Transformer-based Approach <a href="https://arxiv.org/pdf/2511.08645" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Bio AI Agent: A Multi-Agent Artificial Intelligence System for Autonomous CAR-T Cell Therapy Development with Integrated Target Discovery, Toxicity Prediction, and Rational Molecular Design <a href="https://arxiv.org/pdf/2511.08649" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-11-14">2025-11-14<a href="#2025-11-14" class="hash-link" aria-label="Direct link to 2025-11-14" title="Direct link to 2025-11-14" translate="no">​</a></h2>
<p><strong>cs.DC total: 18</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251114] Dynamic Edge Server Selection in Time-Varying Environments: A Reliability-Aware Predictive Approach</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [edge computing], [latency prediction, reliability-aware selection, hysteresis-based handover, passive measurements, exponentially modulated rational delay model]</li>
<li class=""><strong>authors:</strong> Jaime Sebastian Burbano, Arnova Abdullah, Eldiyar Zhantileuov, Mohan Liyanage, Rolf Schuster</li>
<li class=""><strong>institution:</strong> University of Applied Sciences and Arts, Dortmund</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.10146" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.10146</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes MO-HAN, a lightweight server selection method that combines latency prediction with adaptive reliability and hysteresis-based handover to dynamically select edge servers. Results show the approach reduces mean and tail latencies while cutting handovers by nearly 50% compared to opportunistic selection, making it practical for resource-constrained embedded devices.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251114] MoFa: A Unified Performance Modeling Framework for LLM Pretraining</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [performance modeling, hybrid parallelization, fault tolerance, checkpoint recovery, distributed pretraining]</li>
<li class=""><strong>authors:</strong> Lu Zhao, Rong Shi, Shaoqing Zhang, Shangchao Su, Ziqing Yin, Zhiyan Cui, Hongfeng Sun, Baoguo He, Yueqiang Chen, Liang Dong, Xiyuan Li, Lingbin Wang, Lijun Ma, Qiang Huang, Ting Liu, Chong Wang, Can Wei</li>
<li class=""><strong>institution:</strong> AIH Training Team</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.09837" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.09837</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MoFa is a unified performance modeling framework for LLM pretraining that incorporates multi-dimensional optimization features and fault tolerance mechanisms. It uses an enhanced cost model and historical cluster reliability data to accurately predict pretraining performance. The framework provides systematic guidance for optimizing distributed training configurations and reveals key factors influencing pretraining efficiency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251114] TawPipe: Topology-Aware Weight Pipeline Parallelism for Accelerating Long-Context Large Models Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [pipeline parallelism, weight passing, topology-aware communication, hierarchical bandwidth, collective operations, peer-to-peer transfers]</li>
<li class=""><strong>authors:</strong> Houming Wu, Ling Chen</li>
<li class=""><strong>institution:</strong> Zhejiang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.09741" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.09741</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TawPipe proposes a topology-aware weight pipeline parallelism method that optimizes communication by grouping devices based on cluster topology and transmitting weights instead of activations. The approach reduces cross-node traffic through intra-node collective operations and overlaps communication with computation. Experiments show TawPipe achieves superior throughput and scalability compared to existing methods for long-context large model training.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251114] Ksurf-Drone: Attention Kalman Filter for Contextual Bandit Optimization in Cloud Resource Allocation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [Kalman Filter, Contextual Bandits, Gaussian Process Regression, Long Short Term Memory, Transformer Attention]</li>
<li class=""><strong>authors:</strong> Michael Dang&#x27;ana, Yuqiu Zhang, Hans-Arno Jacobsen</li>
<li class=""><strong>institution:</strong> University of Toronto</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.09766" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.09766</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Ksurf-Drone, which combines attention Kalman filters with contextual bandit optimization for cloud resource allocation. The method addresses high variability in cloud workloads by using variance-minimizing estimation techniques. Results show significant improvements in latency variance reduction, resource usage efficiency, and cost savings on Kubernetes benchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251114] Lit Silicon: A Case Where Thermal Imbalance Couples Concurrent Execution in Multiple GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [concurrent computation communication, thermal imbalance, power management, node-level power capping, performance modeling]</li>
<li class=""><strong>authors:</strong> Marco Kurzynski, Shaizeen Aga, Di Wu</li>
<li class=""><strong>institution:</strong> University of Central Florida, Advanced Micro Devices, Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.09861" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.09861</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper identifies the &quot;Lit Silicon&quot; effect where thermal imbalance in multi-GPU systems causes performance variation during LLM training through concurrent computation communication. The authors propose detection methods and power management solutions including GPU power capping and CPU power sloshing. Their approach achieves up to 6% performance improvement and can save significant costs in datacenters with minimal implementation overhead.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251114] A Poly-Log Approximation for Transaction Scheduling in Fog-Cloud Computing and Beyond</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed systems], [transaction scheduling, approximation algorithms, doubling dimension graphs, distributed algorithms, fog-cloud computing]</li>
<li class=""><strong>authors:</strong> Ramesh Adhikari, Costas Busch, Pavan Poudel</li>
<li class=""><strong>institution:</strong> Augusta University, University of Houston-Clear Lake</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.09776" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.09776</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents poly-logarithmic approximation algorithms for transaction scheduling in fog-cloud computing networks, where both transactions and shared objects can move to meet at optimal locations. The algorithms achieve O(log n · log D) approximation for single-object transactions and O(k · log n · log D) for multi-object transactions in constant doubling dimension networks. The authors also develop fully distributed versions that operate without global transaction knowledge.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251114] Selection of Supervised Learning-based Sparse Matrix Reordering Algorithms</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [supervised learning, sparse matrix reordering, automatic tuning, matrix bandwidth minimization]</li>
<li class=""><strong>authors:</strong> Tao Tang, Youfu Jiang, Yingbo Cui, Jianbin Fang, Peng Zhang, Lin Peng, Chun Huang</li>
<li class=""><strong>institution:</strong> National University of Defense Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.10180" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.10180</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a supervised learning model that automatically selects optimal sparse matrix reordering algorithms by learning the relationship between matrix characteristics and algorithm performance. Experiments on the Florida sparse matrix dataset show the model can accurately predict the best reordering algorithm, achieving a 55.37% reduction in solution time compared to using only the AMD algorithm, with an average speedup ratio of 1.45.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251114] SMoFi: Step-wise Momentum Fusion for Split Federated Learning on Heterogeneous Data</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [split federated learning, momentum fusion, staleness-aware alignment, gradient divergence control]</li>
<li class=""><strong>authors:</strong> Mingkun Yang, Ran Zhu, Qing Wang, Jie Yang</li>
<li class=""><strong>institution:</strong> Delft University of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.09828" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.09828</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces SMoFi, a framework that synchronizes momentum buffers across server-side optimizers in split federated learning to counteract gradient divergence from data heterogeneity. It uses a staleness-aware alignment mechanism to constrain gradient updates during training. Experimental results show SMoFi significantly improves global model accuracy (up to 7.1%) and convergence speed (up to 10.25×), particularly with more clients and deeper models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251114] Revisit to the Bai-Galbraith signature scheme</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [cryptography], [lattice-based cryptography, Fiat-Shamir paradigm, Learning with Errors, signature scheme]</li>
<li class=""><strong>authors:</strong> Banhirup Sengupta, Peenal Gupta, Souvik Sengupta</li>
<li class=""><strong>institution:</strong> PinakashieldTech OÜ, Tata Institute Of Fundamental Research, IONOS SE</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.09582" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.09582</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper revisits the Bai-Galbraith lattice-based signature scheme which uses the Fiat-Shamir paradigm and Learning with Errors (LWE) to create digital signatures. The scheme differs from Dilithium by avoiding public key compression and focuses on reducing signature sizes. The authors present this as an alternative approach to practical lattice-based signature schemes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251114] Optimizing CPU Cache Utilization in Cloud VMs with Accurate Cache Abstraction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [cloud computing], [cache abstraction, eviction sets, LLC contention-aware scheduling, virtual color-aware page management]</li>
<li class=""><strong>authors:</strong> Mani Tofigh, Edward Guo, Weiwei Jia, Xiaoning Ding, Jianchen Shan</li>
<li class=""><strong>institution:</strong> Hofstra University, Columbia University, University of Rhode Island, New Jersey Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.09956" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.09956</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes CacheX, a solution that probes accurate cache abstraction within cloud VMs using eviction sets without requiring hardware or hypervisor support. The system implements LLC contention-aware task scheduling and virtual color-aware page cache management to optimize cache utilization. Evaluation shows CacheX effectively improves cache performance for various workloads in public cloud VMs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251114] Pk-IOTA: Blockchain empowered Programmable Data Plane to secure OPC UA communications in Industry 4.0</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [industrial control system security], [programmable data plane, IOTA Tangle, certificate validation, OPC UA security, blockchain]</li>
<li class=""><strong>authors:</strong> Rinieri Lorenzo, Gori Giacomo, Melis Andrea, Girau Roberto, Prandini Marco, Callegati Franco</li>
<li class=""><strong>institution:</strong> University of Bologna</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.10248" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.10248</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Pk-IOTA integrates programmable data plane switches for in-network certificate validation and uses the IOTA Tangle blockchain for decentralized certificate distribution to secure OPC UA communications in Industry 4.0. The system was evaluated on a physical testbed and demonstrated minimal overhead while providing a scalable, tamper-proof mechanism for automated certificate management in industrial control systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251114] Workload Schedulers -- Genesis, Algorithms and Differences</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [scheduling algorithms], [list scheduling, largest processing time first, highest level first, round robin, weighted round robin, largest remaining processing time on fastest machine, fixed-priority pre-emptive scheduling]</li>
<li class=""><strong>authors:</strong> Leszek Sliwko, Vladimir Getov</li>
<li class=""><strong>institution:</strong> University of Westminster</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.10258" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.10258</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a novel categorization approach for workload schedulers across operating systems, cluster systems, and big data environments. It examines the evolution of scheduling algorithms from simple to complex implementations and identifies similarities in scheduling strategy design that apply to both local and distributed systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251114] On The Performance of Prefix-Sum Parallel Kalman Filters and Smoothers on GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [parallel scan algorithms, prefix-sum algorithms, Kalman filtering, smoothing, temporal parallelization, GPU implementation]</li>
<li class=""><strong>authors:</strong> Simo Särkkä, Ángel F. García-Fernández</li>
<li class=""><strong>institution:</strong> Aalto University, Universidad Politécnica de Madrid</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.10363" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.10363</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper evaluates parallel-in-time Kalman filters and smoothers using prefix-sum algorithms on GPUs. It compares different parallel scan algorithms through operation counts and runtime measurements on real GPU hardware. The experimental results demonstrate that temporal parallelization enables more efficient execution of Kalman filters and smoothers on GPU parallel architectures.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251114] Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [symbolic tensor graphs, parallelization strategies, execution traces, distributed workload modeling]</li>
<li class=""><strong>authors:</strong> Changhai Man, Joongun Park, Hanjiang Wu, Huan Xu, Srinivas Sridharan, Tushar Krishna</li>
<li class=""><strong>institution:</strong> Georgia Institute of Technology, Nvidia Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.10480" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.10480</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces STAGE, a framework that synthesizes high-fidelity execution traces to model distributed LLM workloads using symbolic tensor graphs. It enables systematic exploration of parallelization strategies and system configurations without requiring access to large-scale infrastructure. The framework demonstrates scalability by accurately modeling LLM workloads across over 32K GPUs while preserving tensor-level compute, memory, and communication accuracy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251114] Massively Parallel Proof-Number Search for Impartial Games and Beyond</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [game solving], [Proof-Number Search, parallelization, Grundy numbers, game tree reduction]</li>
<li class=""><strong>authors:</strong> Tomáš Čížek, Martin Balko, Martin Schmid</li>
<li class=""><strong>institution:</strong> Charles University, EquiLibre Technologies, Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.10339" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.10339</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a massively parallel version of Proof-Number Search that uses two parallelized levels and shared information to achieve efficient scaling on many CPU cores. The enhanced solver, incorporating Grundy numbers for game tree reduction, achieved a 332.9× speedup on 1024 cores and verified the Sprouts Conjecture for 42 new positions, nearly doubling known outcomes while generating proofs 1,000× more complex than previous state-of-the-art methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251114] FastGraph: Optimized GPU-Enabled Algorithms for Fast Graph Building and Message Passing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [k-nearest neighbor, bin-partitioned approach, gradient-flow support, adaptive parameter tuning, CUDA]</li>
<li class=""><strong>authors:</strong> Aarush Agarwal, Raymond He, Jan Kieseler, Matteo Cremonesi, Shah Rukh Qasim</li>
<li class=""><strong>institution:</strong> Carnegie Mellon University, Karlsruhe Institute of Technology, University of Zurich</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.10442" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.10442</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FastGraph introduces a GPU-optimized k-nearest neighbor algorithm using a bin-partitioned approach with gradient-flow support for efficient graph construction in low-dimensional spaces. The method achieves 20-40x speedup over state-of-the-art libraries like FAISS and SCANN with minimal memory overhead. These improvements significantly accelerate GNN workflows in applications such as particle physics and visual object tracking.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251114] dHPR: A Distributed Halpern Peaceman--Rachford Method for Non-smooth Distributed Optimization Problems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [distributed optimization], [Halpern Peaceman-Rachford, symmetric Gauss-Seidel decomposition, proximal operators, non-smooth optimization]</li>
<li class=""><strong>authors:</strong> Zhangcheng Feng, Defeng Sun, Yancheng Yuan, Guojun Zhang</li>
<li class=""><strong>institution:</strong> The Hong Kong Polytechnic University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.10069" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.10069</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces the distributed Halpern Peaceman-Rachford (dHPR) method for solving non-smooth distributed optimization problems. The method achieves O(1/k) iteration complexity for KKT residuals while maintaining parallelizability through symmetric Gauss-Seidel decomposition. Numerical experiments demonstrate superior performance on distributed LASSO, group LASSO, and L1-regularized logistic regression problems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251114] Unlocking Dynamic Inter-Client Spatial Dependencies: A Federated Spatio-Temporal Graph Learning Method for Traffic Flow Forecasting</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, spatio-temporal graphs, dynamic dependencies, graph neural networks, client-server protocol, nonlinear computation decomposition, node embedding augmentation]</li>
<li class=""><strong>authors:</strong> Feng Wang, Tianxiang Chen, Shuyue Wei, Qian Chu, Yi Zhang, Yifan Sun, Zhiming Zheng</li>
<li class=""><strong>institution:</strong> Beihang University, Renmin University of China</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.10434" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.10434</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes FedSTGD, a federated learning framework that models dynamic inter-client spatial dependencies for traffic flow forecasting using nonlinear computation decomposition and node embedding augmentation. The method coordinates learning through a client-server protocol that breaks down dependency learning into parallelizable subtasks. Experimental results show FedSTGD achieves superior performance approaching centralized baselines while maintaining data privacy.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 28</strong></p>
<ul>
<li class="">[arXiv251114] SEBA: Sample-Efficient Black-Box Attacks on Visual Reinforcement Learning <a href="https://arxiv.org/pdf/2511.09681" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Opinion: Towards Unified Expressive Policy Optimization for Robust Robot Learning <a href="https://arxiv.org/pdf/2511.10087" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Multi-agent In-context Coordination via Decentralized Memory Retrieval <a href="https://arxiv.org/pdf/2511.10030" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] DemoTuner: Efficient DBMS Knobs Tuning via LLM-Assisted Demonstration Reinforcement Learning <a href="https://arxiv.org/pdf/2511.09998" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Uncertainty-Guided Checkpoint Selection for Reinforcement Finetuning of Large Language Models <a href="https://arxiv.org/pdf/2511.09864" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] HierRouter: Coordinated Routing of Specialized Large Language Models via Reinforcement Learning <a href="https://arxiv.org/pdf/2511.09873" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy <a href="https://arxiv.org/pdf/2511.09737" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Scaling Environments for LLM Agents in the Era of Learning from Interaction: A Survey <a href="https://arxiv.org/pdf/2511.09586" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Tree-Based Stochastic Optimization for Solving Large-Scale Urban Network Security Games <a href="https://arxiv.org/pdf/2511.10072" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Optimistic Reinforcement Learning with Quantile Objectives <a href="https://arxiv.org/pdf/2511.09652" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Baby Sophia: A Developmental Approach to Self-Exploration through Self-Touch and Hand Regard <a href="https://arxiv.org/pdf/2511.09727" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Hail to the Thief: Exploring Attacks and Defenses in Decentralised GRPO <a href="https://arxiv.org/pdf/2511.09780" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Beyond Monotonicity: Revisiting Factorization Principles in Multi-Agent Q-Learning <a href="https://arxiv.org/pdf/2511.09792" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] ConstrainedSQL: Training LLMs for Text2SQL via Constrained Reinforcement Learning <a href="https://arxiv.org/pdf/2511.09693" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Improved Offline Reinforcement Learning via Quantum Metric Encoding <a href="https://arxiv.org/pdf/2511.10187" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Heuristic Transformer: Belief Augmented In-Context Reinforcement Learning <a href="https://arxiv.org/pdf/2511.10251" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Beyond Single-Step Updates: Reinforcement Learning of Heuristics with Limited-Horizon Search <a href="https://arxiv.org/pdf/2511.10264" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Causal Model-Based Reinforcement Learning for Sample-Efficient IoT Channel Access <a href="https://arxiv.org/pdf/2511.10291" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] MonkeyOCR v1.5 Technical Report: Unlocking Robust Document Parsing for Complex Patterns <a href="https://arxiv.org/pdf/2511.10390" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] AgentEvolver: Towards Efficient Self-Evolving Agent System <a href="https://arxiv.org/pdf/2511.10395" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Explaining Decentralized Multi-Agent Reinforcement Learning Policies <a href="https://arxiv.org/pdf/2511.10409" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Reasoning About Intent for Ambiguous Requests <a href="https://arxiv.org/pdf/2511.10453" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Strategic Opponent Modeling with Graph Neural Networks, Deep Reinforcement Learning and Probabilistic Topic Modeling <a href="https://arxiv.org/pdf/2511.10501" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Towards Emotionally Intelligent and Responsible Reinforcement Learning <a href="https://arxiv.org/pdf/2511.10573" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Instella: Fully Open Language Models with Stellar Performance <a href="https://arxiv.org/pdf/2511.10628" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Robot Crash Course: Learning Soft and Stylized Falling <a href="https://arxiv.org/pdf/2511.10635" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Operator Models for Continuous-Time Offline Reinforcement Learning <a href="https://arxiv.org/pdf/2511.10383" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Global Solutions to Non-Convex Functional Constrained Problems with Hidden Convexity <a href="https://arxiv.org/pdf/2511.10626" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 11</strong></p>
<ul>
<li class="">[arXiv251114] A General Anchor-Based Framework for Scalable Fair Clustering <a href="https://arxiv.org/pdf/2511.09889" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] The Role of Advanced Computer Architectures in Accelerating Artificial Intelligence Workloads <a href="https://arxiv.org/pdf/2511.10010" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Efficient Hyperdimensional Computing with Modular Composite Representations <a href="https://arxiv.org/pdf/2511.09708" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Steering Pretrained Drafters during Speculative Decoding <a href="https://arxiv.org/pdf/2511.09844" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] On the Convergence of Overparameterized Problems: Inherent Properties of the Compositional Structure of Neural Networks <a href="https://arxiv.org/pdf/2511.09810" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Quality Assurance of LLM-generated Code: Addressing Non-Functional Quality Characteristics <a href="https://arxiv.org/pdf/2511.10271" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] EDGC: Entropy-driven Dynamic Gradient Compression for Efficient LLM Training <a href="https://arxiv.org/pdf/2511.10333" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Simulating Misinformation Propagation in Social Networks using Large Language Models <a href="https://arxiv.org/pdf/2511.10384" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Pretrained Joint Predictions for Scalable Batch Bayesian Optimization of Molecular Designs <a href="https://arxiv.org/pdf/2511.10590" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Solvaformer: an SE(3)-equivariant graph transformer for small molecule solubility prediction <a href="https://arxiv.org/pdf/2511.09774" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] MATAI: A Generalist Machine Learning Framework for Property Prediction and Inverse Design of Advanced Alloys <a href="https://arxiv.org/pdf/2511.10108" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-11-18T14:21:45.000Z" itemprop="dateModified">Nov 18, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/Recommend-System-Note/intro"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Introduction</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/Recommend-System-Note/daily/20251117-20251123"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">20251117-20251123</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-11-12" class="table-of-contents__link toc-highlight">2025-11-12</a></li><li><a href="#2025-11-13" class="table-of-contents__link toc-highlight">2025-11-13</a></li><li><a href="#2025-11-14" class="table-of-contents__link toc-highlight">2025-11-14</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 xiezilailai, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>