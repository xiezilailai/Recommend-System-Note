<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/20251117-20251123" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251117-20251123 | Recommend System Note</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://xiezilailai.github.io/Recommend-System-Note/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://xiezilailai.github.io/Recommend-System-Note/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://xiezilailai.github.io/Recommend-System-Note/daily/20251117-20251123"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251117-20251123 | Recommend System Note"><meta data-rh="true" name="description" content="2025-11-17"><meta data-rh="true" property="og:description" content="2025-11-17"><link data-rh="true" rel="icon" href="/Recommend-System-Note/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://xiezilailai.github.io/Recommend-System-Note/daily/20251117-20251123"><link data-rh="true" rel="alternate" href="https://xiezilailai.github.io/Recommend-System-Note/daily/20251117-20251123" hreflang="en"><link data-rh="true" rel="alternate" href="https://xiezilailai.github.io/Recommend-System-Note/daily/20251117-20251123" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"20251117-20251123","item":"https://xiezilailai.github.io/Recommend-System-Note/daily/20251117-20251123"}]}</script><link rel="stylesheet" href="/Recommend-System-Note/assets/css/styles.75e1961b.css">
<script src="/Recommend-System-Note/assets/js/runtime~main.9edb3461.js" defer="defer"></script>
<script src="/Recommend-System-Note/assets/js/main.d625817a.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/Recommend-System-Note/"><div class="navbar__logo"><img src="/Recommend-System-Note/img/favicon.ico" alt="Recommend System Note" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/Recommend-System-Note/img/favicon.ico" alt="Recommend System Note" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Recommend System Note</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/xiezilailai" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Recommend-System-Note/intro"><span title="Introduction" class="linkLabel_WmDU">Introduction</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/Recommend-System-Note/daily/20251110-20251116"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/Recommend-System-Note/daily/20251110-20251116"><span title="20251110-20251116" class="linkLabel_WmDU">20251110-20251116</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/Recommend-System-Note/daily/20251117-20251123"><span title="20251117-20251123" class="linkLabel_WmDU">20251117-20251123</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/Recommend-System-Note/paper/Algorithm"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/Recommend-System-Note/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Daily</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251117-20251123</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251117-20251123</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-11-17">2025-11-17<a href="#2025-11-17" class="hash-link" aria-label="Direct link to 2025-11-17" title="Direct link to 2025-11-17" translate="no">​</a></h2>
<p><strong>cs.DC total: 9</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251117] SMART: A Surrogate Model for Predicting Application Runtime in Dragonfly Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [graph neural networks, large language models, parallel discrete event simulation, hybrid simulation, surrogate modeling]</li>
<li class=""><strong>authors:</strong> Xin Wang, Pietro Lodi Rizzini, Sourav Medya, Zhiling Lan</li>
<li class=""><strong>institution:</strong> University of Illinois Chicago, Argonne National Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11111" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11111</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents SMART, a surrogate model that combines graph neural networks and large language models to predict application runtime in Dragonfly networks. The model captures both spatial and temporal patterns from router data to address workload interference challenges. SMART outperforms existing baselines and enables efficient hybrid simulation of Dragonfly network systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251117] Beyond Exascale: Dataflow Domain Translation on a Cerebras Cluster</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [high performance computing], [Domain Translation algorithm, shallow-water equations, finite difference methods, stencil computations, cluster computing]</li>
<li class=""><strong>authors:</strong> Tomas Oppelstrup, Nicholas Giamblanco, Delyan Z. Kalchev, Ilya Sharapov, Mark Taylor, Dirk Van Essendelft, Sivasankaran Rajamanickam, Michael James</li>
<li class=""><strong>institution:</strong> Cerebras Systems, Sandia National Laboratories, National Energy Technology Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11542" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11542</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a novel Domain Translation algorithm that overcomes limitations of traditional domain decomposition methods for physical simulations. The method achieves 1.6 million time steps per second and 84 PFLOP/s performance on a 64-node Cerebras CS-3 cluster, demonstrating 90% of peak performance for planetary-scale tsunami modeling using shallow-water equations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251117] FengHuang: Next-Generation Memory Orchestration for AI Inferencing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [memory disaggregation, multi-tier shared-memory, active tensor paging, near-memory compute]</li>
<li class=""><strong>authors:</strong> Jiamin Li, Lei Qu, Tao Zhang, Grigory Chirkov, Shuotao Xu, Peng Cheng, Lidong Zhou</li>
<li class=""><strong>institution:</strong> Microsoft Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.10753" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.10753</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes FengHuang, a disaggregated AI infrastructure platform that uses multi-tier shared-memory architecture with active tensor paging and near-memory compute to overcome memory and communication bottlenecks. Simulation results show the system achieves significant memory capacity reduction, GPU compute savings, and faster inter-GPU communication while maintaining performance for large language model inference workloads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251117] Cascading Bandits With Feedback</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [cascade bandits, explore-then-commit, action elimination, lower confidence bound, thompson sampling]</li>
<li class=""><strong>authors:</strong> R Sri Prakash, Nikhil Karamchandani, Sharayu Moharir</li>
<li class=""><strong>institution:</strong> IIITDM Kancheepuram, IIT Bombay</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.10938" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.10938</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes four decision-making policies for cascade bandits in edge inference systems. The study shows that LCB and Thompson Sampling achieve constant regret by continuously adapting based on feedback, while Explore-then-Commit and Action Elimination incur suboptimal regret due to fixed ordering commitments. Adaptive policies are crucial for efficient edge inference under uncertainty.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251117] SemanticNN: Compressive and Error-Resilient Semantic Offloading for Extremely Weak Devices</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [semantic codec, bit error rate-aware decoder, soft quantization, feature-augmentation learning, XAI-based asymmetry compensation]</li>
<li class=""><strong>authors:</strong> Jiaming Huang, Yi Gao, Fuchang Pan, Renjie Li, Wei Dong</li>
<li class=""><strong>institution:</strong> Zhejiang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11038" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11038</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SemanticNN is a semantic codec system that tolerates bit-level transmission errors while maintaining semantic-level correctness for device-edge collaborative inference. It uses BER-aware decoders, soft quantization encoders, and novel training strategies to handle asymmetric resources. Experimental results show it reduces feature transmission volume by 56.82-344.83x while maintaining superior inference accuracy under varying error rates.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251117] HPCAgentTester: A Multi-Agent LLM Approach for Enhanced HPC Unit Test Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [multi-agent LLM, OpenMP, MPI, unit test generation, parallel computing, critique loop]</li>
<li class=""><strong>authors:</strong> Rabimba Karanjai, Lei Xu, Weidong Shi</li>
<li class=""><strong>institution:</strong> University of Houston, Kent State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.10860" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.10860</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces HPCAgentTester, a multi-agent LLM framework that uses specialized agents (Recipe Agent and Test Agent) working collaboratively through a critique loop to generate unit tests for HPC software. The system produces compilable and functionally correct tests targeting parallel execution constructs in OpenMP and MPI. Evaluation shows it significantly improves test compilation rates and correctness compared to standalone LLMs, effectively identifying subtle bugs in parallel software systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251117] A Unified Convergence Analysis for Semi-Decentralized Learning: Sampled-to-Sampled vs. Sampled-to-All Communication</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, semi-decentralized learning, local SGD, device-to-device communication, model aggregation, convergence analysis]</li>
<li class=""><strong>authors:</strong> Angelo Rodio, Giovanni Neglia, Zheng Chen, Erik G. Larsson</li>
<li class=""><strong>institution:</strong> Linköping University, Inria</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11560" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11560</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes two communication strategies (sampled-to-sampled vs sampled-to-all) in semi-decentralized federated learning where devices primarily use device-to-device communication with periodic server interactions. The authors develop a unified convergence framework that accounts for sampling rate, server aggregation frequency, and network connectivity. Their analysis reveals that the optimal strategy depends primarily on data heterogeneity across devices, providing practical design guidelines for semi-decentralized FL deployments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251117] UFO<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">^3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span>: Weaving the Digital Agent Galaxy</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [TaskConstellation, distributed DAG, asynchronous execution, adaptive recovery, dynamic optimization, Agent Interaction Protocol]</li>
<li class=""><strong>authors:</strong> Chaoyun Zhang, Liqun Li, He Huang, Chiming Ni, Bo Qiao, Si Qin, Yu Kang, Minghua Ma, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang</li>
<li class=""><strong>institution:</strong> Microsoft, ZJU-UIUC Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11332" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11332</a></li>
<li class=""><strong>Simple LLM Summary:</strong> UFO³ introduces a system that models user requests as mutable TaskConstellations - distributed DAGs of atomic subtasks executed across heterogeneous devices. The system enables asynchronous execution, adaptive recovery, and dynamic optimization through its Constellation Orchestrator and Agent Interaction Protocol. Evaluation shows it achieves 70.9% task success rate, exposes parallelism, and reduces latency by 31% while providing resilient orchestration across diverse computing devices.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251117] EarthSight: A Distributed Framework for Low-Latency Satellite Intelligence</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [distributed runtime framework, multi-task inference, shared backbones, ground-station query scheduler, dynamic filter ordering, resource-aware adaptive decisions]</li>
<li class=""><strong>authors:</strong> Ansel Kaplan Erol, Seungjun Lee, Divya Mahajan</li>
<li class=""><strong>institution:</strong> Georgia Institute of Technology, KAIST</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.10834" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.10834</a></li>
<li class=""><strong>Simple LLM Summary:</strong> EarthSight introduces a distributed framework that coordinates satellite constellations and ground stations to perform onboard multi-task inference using shared backbones, intelligent query scheduling, and dynamic filter ordering. This approach enables satellites to prioritize and analyze imagery in orbit while conserving bandwidth and power resources. The system reduces average compute time by 1.9x and cuts 90th percentile latency from 51 to 21 minutes compared to state-of-the-art baselines.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 18</strong></p>
<ul>
<li class="">[arXiv251117] Aligning Machiavellian Agents: Behavior Steering via Test-Time Policy Shaping <a href="https://arxiv.org/pdf/2511.11551" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] Incorporating Spatial Information into Goal-Conditioned Hierarchical Reinforcement Learning via Graph Representations <a href="https://arxiv.org/pdf/2511.10872" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] Honesty over Accuracy: Trustworthy Language Models through Reinforced Hesitation <a href="https://arxiv.org/pdf/2511.11500" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] Context-aware Adaptive Visualizations for Critical Decision Making <a href="https://arxiv.org/pdf/2511.11476" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] VIDEOP2R: Video Understanding from Perception to Reasoning <a href="https://arxiv.org/pdf/2511.11113" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] LoRaCompass: Robust Reinforcement Learning to Efficiently Search for a LoRa Tag <a href="https://arxiv.org/pdf/2511.11190" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] ARCTraj: A Dataset and Benchmark of Human Reasoning Trajectories for Abstract Problem Solving <a href="https://arxiv.org/pdf/2511.11079" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] When Data is the Algorithm: A Systematic Study and Curation of Preference Optimization Datasets <a href="https://arxiv.org/pdf/2511.10985" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] From Efficiency to Adaptivity: A Deeper Look at Adaptive Reasoning in Large Language Models <a href="https://arxiv.org/pdf/2511.10788" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] Multi-Phase Spacecraft Trajectory Optimization via Transformer-Based Reinforcement Learning <a href="https://arxiv.org/pdf/2511.11402" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] STaR: Towards Cognitive Table Reasoning via Slow-Thinking Large Language Models <a href="https://arxiv.org/pdf/2511.11233" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] Scalable Population Training for Zero-Shot Coordination <a href="https://arxiv.org/pdf/2511.11083" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] Data Poisoning Vulnerabilities Across Healthcare AI Architectures: A Security Threat Analysis <a href="https://arxiv.org/pdf/2511.11020" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] Robust and Efficient Communication in Multi-Agent Reinforcement Learning <a href="https://arxiv.org/pdf/2511.11393" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] RLSLM: A Hybrid Reinforcement Learning Framework Aligning Rule-Based Social Locomotion Model with Human Social Norms <a href="https://arxiv.org/pdf/2511.11323" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] Understanding the Nature of Depth-1 Equivariant Quantum Circuit <a href="https://arxiv.org/pdf/2511.10756" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] Behaviour Policy Optimization: Provably Lower Variance Return Estimates for Off-Policy Reinforcement Learning <a href="https://arxiv.org/pdf/2511.10843" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism <a href="https://arxiv.org/pdf/2511.11373" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 9</strong></p>
<ul>
<li class="">[arXiv251117] MMA-Sim: Bit-Accurate Reference Model of Tensor Cores and Matrix Cores <a href="https://arxiv.org/pdf/2511.10909" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] FarSkip-Collective: Unhobbling Blocking Communication in Mixture of Experts Models <a href="https://arxiv.org/pdf/2511.11505" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] LiteAttention: A Temporal Sparse Attention for Diffusion Transformers <a href="https://arxiv.org/pdf/2511.11062" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] Benchmarking Quantum Kernels Across Diverse and Complex Data <a href="https://arxiv.org/pdf/2511.10831" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] Improving Continual Learning of Knowledge Graph Embeddings via Informed Initialization <a href="https://arxiv.org/pdf/2511.11118" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] AIonopedia: an LLM agent orchestrating multimodal learning for ionic liquid discovery <a href="https://arxiv.org/pdf/2511.11257" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] Virtual Width Networks <a href="https://arxiv.org/pdf/2511.11238" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] Human-AI collaborative autonomous synthesis with pulsed laser deposition for remote epitaxy <a href="https://arxiv.org/pdf/2511.11558" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] LAD-BNet: Lag-Aware Dual-Branch Networks for Real-Time Energy Forecasting on Edge Devices <a href="https://arxiv.org/pdf/2511.10680" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-11-18">2025-11-18<a href="#2025-11-18" class="hash-link" aria-label="Direct link to 2025-11-18" title="Direct link to 2025-11-18" translate="no">​</a></h2>
<p><strong>cs.DC total: 53</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251118] Machine learning-based cloud resource allocation algorithms: a comprehensive comparative review</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [deep reinforcement learning, neural networks, multi-agent systems, hybrid architectures, edge computing]</li>
<li class=""><strong>authors:</strong> Deep Bodra, Sushil Khairnar</li>
<li class=""><strong>institution:</strong> Harrisburg University of Science and Technology, Virginia Tech</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11603" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11603</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper compares machine learning algorithms for cloud resource allocation, including deep reinforcement learning, neural networks, and multi-agent systems. The analysis shows that hybrid AI/ML architectures consistently outperform single-method approaches, with edge computing environments demonstrating the highest deployment readiness for these resource allocation strategies.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] The Anatomy of a Triton Attention Kernel</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [triton, paged attention, auto-tuning, just-in-time compilation, cross-platform kernels]</li>
<li class=""><strong>authors:</strong> Burkhard Ringlein, Jan van Lunteren, Radu Stoica, Thomas Parnell</li>
<li class=""><strong>institution:</strong> IBM Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11581" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11581</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper develops a portable paged attention kernel using Triton&#x27;s domain-specific language that achieves cross-platform state-of-the-art performance on both NVIDIA and AMD GPUs. The kernel eliminates the need for low-level hand-tuning through parameter auto-tuning and system-level optimizations. The results demonstrate that open-source domain-specific languages can enable efficient LLM inference across different hardware vendors while maintaining performance portability.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] ACE-GNN: Adaptive GNN Co-Inference with System-Aware Scheduling in Dynamic Edge Environments</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [device-edge co-inference, pipeline parallelism, data parallelism, adaptive scheduling, batch inference, system-aware optimization]</li>
<li class=""><strong>authors:</strong> Ao Zhou, Jianlei Yang, Tong Qiao, Yingjie Qi, Xinming Wei, Cenlin Duan, Weisheng Zhao, Chunming Hu</li>
<li class=""><strong>institution:</strong> Beihang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11586" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11586</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ACE-GNN introduces an adaptive GNN co-inference framework that dynamically schedules between pipeline and data parallelism to optimize performance in dynamic edge environments. The system uses runtime scheme optimization with performance prediction and specialized communication middleware to maintain stable inference. Experiments show significant improvements with up to 12.7× speedup and 82.3% energy savings compared to existing methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Evaluating Large Language Models for Workload Mapping and Scheduling in Heterogeneous HPC Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [constraint-based optimization, workload mapping, scheduling, makespan optimization, natural language reasoning]</li>
<li class=""><strong>authors:</strong> Aasish Kumar Sharma, Julian Kunkel</li>
<li class=""><strong>institution:</strong> Georg-August-Universität Göttingen</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11612" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11612</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This study evaluates 21 large language models on their ability to perform HPC workload mapping and scheduling from natural language descriptions. The models were tested on assigning tasks to nodes while computing makespan and explaining reasoning under given constraints. Results show that while some LLMs can reconstruct optimal schedules, most struggle with precise timing and dependency enforcement, positioning them better as explainable co-pilots rather than autonomous optimization solvers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Distributed Q-learning-based Shortest-Path Tree Construction in IoT Sensor Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Q-learning, distributed algorithms, shortest-path tree, IoT sensor networks, reinforcement learning]</li>
<li class=""><strong>authors:</strong> Van-Vi Vo, Tien-Dung Nguyen, Duc-Tai Le, Hyunseung Choo</li>
<li class=""><strong>institution:</strong> Sungkyunkwan University, Hanoi University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11598" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11598</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a distributed Q-learning framework for constructing shortest-path trees in IoT sensor networks, where nodes independently learn optimal routing decisions using local information. The method achieves near-optimal routing accuracy (over 99% for large networks) while reducing communication overhead and adapting to topology changes. The approach demonstrates superior scalability and energy efficiency compared to traditional centralized and flooding-based routing protocols.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] PACE Solver Description: twin_width_fmi</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [graph-algorithms], [greedy-heuristic, simulated-annealing, iterative-greedy, local-search, graph-reductions]</li>
<li class=""><strong>authors:</strong> David Balaban, Adrian Miclăuş</li>
<li class=""><strong>institution:</strong> International Computer High School of Bucharest, University of Bucharest</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11605" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11605</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents hedom5, an iterative-greedy heuristic for the Minimum Dominating Set problem that combines greedy construction, backward pruning, and local improvement steps. This approach outperformed both a standard greedy baseline and a simulated annealing variant on PACE 2025 benchmark instances. The authors selected hedom5 as their final submission due to its superior performance in finding smaller dominating sets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Why Should the Server Do It All?: A Scalable, Versatile, and Model-Agnostic Framework for Server-Light DNN Inference over Massively Distributed Clients via Training-Free Intermediate Feature Compression</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [intermediate feature compression, asymmetric top-K filtering, magnitude-splitting, adaptive bit quantization, split computing]</li>
<li class=""><strong>authors:</strong> Mingyu Sung, Suhwan Im, Daeho Bang, Il-Min Kim, Sangseok Yun, Jae-Mo Kang</li>
<li class=""><strong>institution:</strong> Kyungpook National University, Queen&#x27;s University, Pukyong National University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11608" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11608</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces SLICER, a training-free framework that compresses intermediate features in split computing using sparsification and quantization techniques. This approach reduces communication volume by up to 10x and server GPU time by up to 4.4x while maintaining task quality. The method works with off-the-shelf models without retraining, enabling scalable distributed inference.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Mind the Gap: Revealing Inconsistencies Across Heterogeneous AI Accelerators</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [differential testing, automated pipeline, model synthesis, operator implementations, numerical discrepancies, compilation failures]</li>
<li class=""><strong>authors:</strong> Elliott Wen, Sean Ma, Ewan Tempero, Jens Dietrich, Daniel Luo, Jiaxing Shen, Kaiqi Zhao, Bruce Sham, Yousong Song, Jiayi Hua, Jia Hong</li>
<li class=""><strong>institution:</strong> The University of Auckland, Hong Kong Polytechnic University, Victoria University of Wellington, Harbin Institute of Technology, Lingnan University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11601" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11601</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents an empirical study using differential testing with an automated pipeline that synthesizes over 100,000 variant models from real-world examples. The study reveals significant inconsistencies across heterogeneous AI accelerators, finding that newer platforms from Mac and Huawei support fewer operators and exhibit higher output discrepancy rates. These results highlight challenges in achieving consistent machine learning behavior across diverse hardware ecosystems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Beyond the GPU: The Strategic Role of FPGAs in the Next Wave of AI</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [FPGA, hardware reconfiguration, parallel pipelines, deterministic timing, energy efficiency, SoC integration, partial reconfiguration]</li>
<li class=""><strong>authors:</strong> Arturo Urías Jiménez</li>
<li class=""><strong>institution:</strong> Instituto Tecnológico y de Estudios Superiores de Monterrey</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11614" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11614</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes using Field-Programmable Gate Arrays (FPGAs) as reconfigurable hardware platforms that can directly map AI algorithms into device logic. The method enables parallel processing pipelines with deterministic timing and reduced power consumption through hardware-algorithm co-design. The main conclusion is that FPGAs offer strategic advantages over GPUs for AI workloads requiring predictable performance, deep customization, and edge deployment near sensors.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Parameter-Efficient and Personalized Federated Training of Generative Models at the Edge</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [federated learning, LoRA, parameter-efficient fine-tuning, FedAvg, client adapters]</li>
<li class=""><strong>authors:</strong> Kabir Khan, Manju Sarkar, Anita Kar, Suresh Ghosh</li>
<li class=""><strong>institution:</strong> San Francisco State University, University of Lakhimpur, Manipur University, Lakshadweep University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11585" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11585</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes FedGen-Edge, a federated learning framework that uses Low-Rank Adaptation (LoRA) to train lightweight client adapters while keeping a pre-trained global backbone frozen. This approach reduces communication costs by over 99%, stabilizes training with non-IID data, and enables personalization through local adapter tuning. Experiments show improved performance and faster convergence on language modeling and image generation tasks compared to full-model federated averaging.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] AnchorTP: Resilient LLM Inference with State-Preserving Elastic Tensor Parallelism</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [elastic tensor parallelism, state preservation, continuous minimal migration, KV cache preservation, bandwidth-aware planning]</li>
<li class=""><strong>authors:</strong> Wendong Xu, Chujie Chen, He Xiao, Kuan Li, Jing Xiong, Chen Zhang, Wenyong Zhou, Chaofan Tao, Yang Bai, Bei Yu, Ngai Wong</li>
<li class=""><strong>institution:</strong> The University of Hong Kong, Institute of Computing Technology Chinese Academy of Sciences, Hong Kong University of Science and Technology, Chinese University of Hong Kong</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11617" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11617</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AnchorTP introduces a state-preserving elastic tensor parallelism framework that maintains model parameters and KV caches during GPU failures. It uses a bandwidth-aware planner with Continuous Minimal Migration algorithm and execution scheduler to minimize data movement during recovery. The system reduces Time to First Success by up to 11x and Time to Peak by up to 59% compared to restart-and-reload approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] AIvailable: A Software-Defined Architecture for LLM-as-a-Service on Heterogeneous and Legacy GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [software-defined architecture, VRAM-aware allocation, heterogeneous GPU support, load balancing, dynamic reallocation]</li>
<li class=""><strong>authors:</strong> Pedro Antunes, Ana Rita Ortigoso, Gabriel Vieira, Daniel Fuentes, Luís Frazão, Nuno Costa, António Pereira</li>
<li class=""><strong>institution:</strong> Polytechnic University of Leiria</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11621" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11621</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AIvailable introduces a software-defined architecture for LLM-as-a-Service that enables efficient inference across heterogeneous and legacy GPU nodes through dynamic VRAM-aware allocation. The system provides a unified client interface and fully GPU-accelerated inference without CPU fallbacks. This approach allows resource-constrained organizations to repurpose legacy GPUs for running diverse open LLMs, democratizing access to generative AI.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Exploring Parallelism in FPGA-Based Accelerators for Machine Learning Applications</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [speculative backpropagation, FPGA acceleration, OpenMP parallelization, MNIST dataset]</li>
<li class=""><strong>authors:</strong> Sed Centeno, Christopher Sprague, Arnab A Purkayastha, Ray Simar, Neeraj Magotra</li>
<li class=""><strong>institution:</strong> Western New England University, Rice University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11640" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11640</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper implements speculative backpropagation using OpenMP to overlap forward and backward passes in neural network training. The method achieved up to 24% overall speedup and 35% step-level speedup on MNIST while maintaining accuracy within 3-4% of baseline. The work demonstrates the potential for hardware acceleration through planned FPGA synthesis.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Range Asymmetric Numeral Systems-Based Lightweight Intermediate Feature Compression for Split Computing of Deep Neural Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [rANS encoding, asymmetric integer quantization, sparse tensor representation, split computing, intermediate feature compression]</li>
<li class=""><strong>authors:</strong> Mingyu Sung, Suhwan Im, Vikas Palakonda, Jae-Mo Kang</li>
<li class=""><strong>institution:</strong> Kyungpook National University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11664" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11664</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a lightweight compression framework using Range Asymmetric Numeral Systems encoding with asymmetric integer quantization and sparse tensor representation to reduce transmission overhead in split computing. The method maintains near-baseline accuracy across various neural architectures and benchmarks while achieving sub-millisecond encoding/decoding latency. It effectively addresses communication bottlenecks in bandwidth-constrained environments without requiring network modifications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] HeteroSTA: A CPU-GPU Heterogeneous Static Timing Analysis Engine with Holistic Industrial Design Support</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [CPU-GPU heterogeneous computing, static timing analysis, delay calculation models, graph propagation, path search, SDC constraints, API integration]</li>
<li class=""><strong>authors:</strong> Zizheng Guo, Haichuan Liu, Xizhe Shi, Shenglu Hua, Zuodong Zhang, Chunyuan Zhao, Runsheng Wang, Yibo Lin</li>
<li class=""><strong>institution:</strong> Peking University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11660" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11660</a></li>
<li class=""><strong>Simple LLM Summary:</strong> HeteroSTA is a CPU-GPU heterogeneous static timing analysis engine that provides end-to-end GPU acceleration for both graph-based and path-based timing queries. It supports industry-standard formats and offers versatile delay calculation models without relying on external tools. The system demonstrates remarkable runtime speed-up while maintaining comparable quality in various EDA applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Characterizing and Understanding Energy Footprint and Efficiency of Small Language Model on Edges</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [edge computing, GPU acceleration, memory bandwidth, model architecture optimization, power efficiency benchmarking]</li>
<li class=""><strong>authors:</strong> Md Romyull Islam, Bobin Deng, Nobel Dhar, Tu N. Nguyen, Selena He, Yong Shi, Kun Suo</li>
<li class=""><strong>institution:</strong> Kennesaw State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11624" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11624</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper evaluates the energy efficiency of five small language models (SLMs) deployed on edge devices including Raspberry Pi 5 and Jetson platforms. The study found that Jetson Orin Nano with GPU acceleration achieves the highest energy-to-performance ratio, while Llama 3.2 provides the best balance of accuracy and power efficiency. Key factors influencing energy efficiency include GPU acceleration, memory bandwidth, and model architecture.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Mixture-of-Schedulers: An Adaptive Scheduling Agent as a Learned Router for Expert Policies</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [adaptive scheduling, machine learning model, time-weighted probability voting, sched_ext framework, workload pattern recognition]</li>
<li class=""><strong>authors:</strong> Xinbo Wang, Shian Jia, Ziyang Huang, Jing Cao, Mingli Song</li>
<li class=""><strong>institution:</strong> Zhejiang University, HangZhou City University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11628" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11628</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes an Adaptive Scheduling Agent (ASA) that uses a machine learning model to dynamically select optimal scheduling policies from a portfolio of expert schedulers at runtime. This approach combines offline training of a hardware-agnostic workload recognition model with online decision-making using time-weighted probability voting. Evaluation shows ASA outperforms the default Linux scheduler in 86.4% of test scenarios and achieves near-optimal performance in most cases.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] OSGym: Super-Scalable Distributed Data Engine for Generalizable Computer Agents</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [distributed data engine, OS replicas, parallelization, supervised fine-tuning, reinforcement learning]</li>
<li class=""><strong>authors:</strong> Zengyi Qin, Jinyuan Chen, Yunze Man, Shengcao Cao, Ziqi Pang, Zhuoyuan Wang, Xin Sun, Gen Lin, Han Fang, Ling Zhu, Zixin Xie, Zibu Wei, Tianshu Ran, Haoran Geng, Xander Wu, Zachary Bright, Qizhen Sun, Rui Wang, Yuyang Cai, Song Wang, Jiace Zhao, Han Cao, Yeyang Zhou, Tianrui Liu, Ray Pan, Chongye Yang, Xiang Ren, Bo Zhang, Yutong Ban, Jitendra Malik, Brian Anthony, Pieter Abbeel</li>
<li class=""><strong>institution:</strong> MIT, UIUC, CMU, USC, UVA, UC Berkeley</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11672" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11672</a></li>
<li class=""><strong>Simple LLM Summary:</strong> OSGym is a super-scalable distributed data engine that can parallelize over a thousand OS replicas for training computer agents across diverse tasks. It enables efficient data collection, supervised fine-tuning, and reinforcement learning pipelines at low cost. Models trained with OSGym outperform state-of-the-art baselines, demonstrating its potential to advance agent research scalability and universality.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] DIAP: A Decentralized Agent Identity Protocol with Zero-Knowledge Proofs and a Hybrid P2P Stack</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [zero-knowledge proofs, IPFS, Libp2p GossipSub, Iroh, QUIC, DID-Key, Noir]</li>
<li class=""><strong>authors:</strong> Yuanjie Liu, Wenpeng Xing, Ye Zhou, Gaowei Chang, Changting Lin, Meng Han</li>
<li class=""><strong>institution:</strong> Zhejiang University, Binjiang Institute of Zhejiang University, GenTel.io, ANP Open Source Community</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11619" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11619</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DIAP proposes a decentralized agent identity protocol that binds agent identities to IPFS content identifiers and uses zero-knowledge proofs for ownership verification. The framework combines a hybrid P2P stack with Libp2p for discovery and Iroh for data exchange, eliminating the need for centralized intermediaries. This establishes a practical foundation for trustless, privacy-preserving autonomous agent ecosystems and A2A economies.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] A Structure-Agnostic Co-Tuning Framework for LLMs and SLMs in Cloud-Edge Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [co-tuning, structure-agnostic mutual learning, distilled proxy models, cloud-edge systems, knowledge exchange]</li>
<li class=""><strong>authors:</strong> Yuze Liu, Yunhan Wang, Tiehua Zhang, Zhishu Shen, Cheng Peng, Libing Wu, Feng Xia, Jiong Jin</li>
<li class=""><strong>institution:</strong> Swinburne University of Technology, Tongji University, Wuhan University of Technology, INFLY TECH, Wuhan University, RMIT University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11678" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11678</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Co-PLMs, a structure-agnostic co-tuning framework that enables collaborative training between large language models on cloud servers and small language models on edge devices using distilled proxy models as bridges. The framework facilitates knowledge exchange between heterogeneous models while preserving domain-specific insights. Experimental results show Co-PLMs outperforms state-of-the-art methods with average improvements of 5.38% in Rouge-L and 4.88% in EM scores.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] A Meta-Heuristic Load Balancer for Cloud Computing Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [cloud resource management], [genetic algorithm, meta-heuristic, load balancing, service migration, resource utilization]</li>
<li class=""><strong>authors:</strong> Leszek Sliwko, Vladimir Getov</li>
<li class=""><strong>institution:</strong> University of Westminster</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11721" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11721</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a meta-heuristic load balancing strategy using a novel genetic algorithm seeded with outputs from other meta-heuristics for cloud computing systems. The approach aims to allocate services without overloading nodes while maintaining system stability with minimum cost. Experimental results demonstrate the effectiveness of the proposed method in managing cloud resources efficiently.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Federated Learning for Pediatric Pneumonia Detection: Enabling Collaborative Diagnosis Without Sharing Patient Data</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, chest x-ray classification, non-IID data, privacy-preserving training]</li>
<li class=""><strong>authors:</strong> Daniel M. Jimenez-Gutierrez, Enrique Zuazua, Joaquin Del Rio, Oleksii Sliusarenko, Xabi Uribe-Etxebarria</li>
<li class=""><strong>institution:</strong> Sherpa.ai</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11714" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11714</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper uses federated learning to enable multiple hospitals to collaboratively train a pneumonia detection model from chest X-rays without sharing patient data. The method achieved significant performance improvements (47.5% accuracy gain and 50.0% ROC-AUC gain) over single-hospital models while maintaining data privacy. The results demonstrate that federated learning enables secure, high-performing medical diagnosis across healthcare networks while keeping data local.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] ECCENTRIC: Edge-Cloud Collaboration Framework for Distributed Inference Using Knowledge Adaptation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [knowledge adaptation, edge-cloud collaboration, distributed inference, Pareto optimization, model compression]</li>
<li class=""><strong>authors:</strong> Mohammad Mahdi Kamani, Zhongwei Cheng, Lin Chen</li>
<li class=""><strong>institution:</strong> Wyze Labs, Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11719" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11719</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes ECCENTRIC, a framework that uses knowledge adaptation between edge and cloud models to achieve optimal trade-offs between computation, communication, and performance in distributed inference systems. This approach reduces both computation and communication costs while maintaining high performance. Empirical results on classification and object detection tasks demonstrate the framework&#x27;s effectiveness.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] How Machine Learning-Data Driven Replication Strategies Enhance Fault Tolerance in Large-Scale Distributed Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [machine learning, predictive analytics, reinforcement learning, data replication, adaptive mechanisms]</li>
<li class=""><strong>authors:</strong> Almond Kiruthu Murimi</li>
<li class=""><strong>institution:</strong> Kabarak University, Carnegie Mellon University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11749" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11749</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes using machine learning techniques, specifically predictive analytics and reinforcement learning, to create adaptive data replication strategies that forecast failures and optimize data placement in real-time. It concludes that ML-driven approaches significantly enhance fault tolerance in distributed systems compared to traditional static methods, though implementation challenges remain for real-world deployment.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Harli: Harvest Underutilized Resources in LLM Serving with Finetuning Tasks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [parameter-efficient finetuning, memory allocator, latency predictor, QoS scheduler, resource co-location]</li>
<li class=""><strong>authors:</strong> Ao Xu, Han Zhao, Weihao Cui, Quan Chen, Yukang Chen, Shulai Zhang, Shuang Chen, Jiemin Jiang, Zhibin Yu, Minyi Guo</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University, Zhejiang University, Chinese Academy of Science</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11729" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11729</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Harli improves GPU utilization in LLM serving by co-locating parameter-efficient finetuning tasks with decode instances. The system addresses memory and interference challenges through a unified memory allocator, latency predictor, and QoS-aware scheduler. Experimental results show Harli achieves 46.2% average finetuning throughput improvement while maintaining strict QoS guarantees for inference.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Speculative Decoding in Decentralized LLM Inference: Turning Communication Latency into Computation Throughput</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [speculative decoding, decentralized inference, adaptive verification, communication optimization]</li>
<li class=""><strong>authors:</strong> Jingwei Song, Wanyi Chen, Xinyuan Song, Chris Tong, Gufeng Chen, Tianyi Zhao, Eric Yang, Bill Shi, Lynn Ai</li>
<li class=""><strong>institution:</strong> Gradient Network, The University of Hong Kong, Soochow University, Emory University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11733" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11733</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Decentralized Speculative Decoding (DSD), a framework that converts network latency into computation throughput by verifying multiple candidate tokens in parallel across distributed nodes. The method includes an adaptive verification strategy that adjusts acceptance thresholds based on token-level semantic importance. DSD achieves up to 2.6× speedup on benchmarks while preserving accuracy, enabling faster distributed LLM inference without model retraining.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Flash-Fusion: Enabling Expressive, Low-Latency Queries on IoT Sensor Streams with LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [edge-based statistical summarization, query planning, context-rich prompts, sensor fusion, prompt engineering]</li>
<li class=""><strong>authors:</strong> Kausar Patherya, Ashutosh Dhekne, Francisco Romero</li>
<li class=""><strong>institution:</strong> Georgia Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11885" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11885</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Flash-Fusion is an edge-cloud system that reduces IoT data volume through edge-based statistical summarization and improves data interpretation via cloud-based query planning with context-rich prompts. The system achieves 95% latency reduction and 98% decrease in token usage while maintaining high-quality LLM responses, enabling efficient natural language queries on IoT sensor streams without manual preprocessing.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Noise-Aware Optimization in Nominally Identical Manufacturing and Measuring Systems for High-Throughput Parallel Workflows</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Bayesian optimization, distributional analysis, pairwise divergence metrics, clustering, noise-aware decision-making]</li>
<li class=""><strong>authors:</strong> Christina Schenk, Miguel Hernández-del-Valle, Luis Calero-Lumbreras, Marcus Noack, Maciej Haranczyk</li>
<li class=""><strong>institution:</strong> IMDEA Materials Institute, Universidad Carlos III de Madrid, Lawrence Berkeley National Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11739" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11739</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a noise-aware optimization framework that uses distributional analysis and clustering to select between single-device and robust multi-device Bayesian optimization strategies. The method explicitly models device-specific noise profiles to manage variability in automated manufacturing systems. Experimental results with nominally identical 3D printers show reduced redundancy, lower resource usage, and improved reliability compared to conventional approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] TD-Orch: Scalable Load-Balancing for Distributed Systems with Applications to Graph Processing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed systems], [task-data orchestration, distributed push-pull, load balancing]</li>
<li class=""><strong>authors:</strong> Yiwei Zhao, Qiushi Lin, Hongbo Kang, Guy E. Blelloch, Laxman Dhulipala, Charles McGuffey, Phillip B. Gibbons</li>
<li class=""><strong>institution:</strong> Carnegie Mellon University, Tsinghua University, University of Maryland, Reed College</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11843" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11843</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TD-Orch is a distributed orchestration framework that uses a push-pull technique to co-locate tasks with their required data while achieving scalable load balancing. The system demonstrates significant performance improvements, achieving up to 2.7x speedup over existing distributed scheduling baselines and enabling a graph processing system (TDO-GP) that outperforms prior systems by 4.1x on average.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Advancing Annotat3D with Harpia: A CUDA-Accelerated Library For Large-Scale Volumetric Data Segmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [CUDA acceleration, memory control, chunked execution, GPU-accelerated filtering, volumetric data segmentation]</li>
<li class=""><strong>authors:</strong> Camila Machado de Araujo, Egon P. B. S. Borges, Ricardo Marcelo Canteiro Grangeiro, Allan Pinto</li>
<li class=""><strong>institution:</strong> Brazilian Synchrotron Light Laboratory (LNLS), Brazilian Center for Research in Energy and Materials (CNPEM)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11890" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11890</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Harpia, a CUDA-accelerated library that enables large-scale volumetric data segmentation through strict memory control, native chunked execution, and GPU-accelerated tools. The system demonstrates significant improvements in processing speed, memory efficiency, and scalability compared to existing frameworks like NVIDIA cuCIM and scikit-image. The combination of interactive interfaces and efficient GPU management makes it suitable for collaborative scientific workflows in HPC environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] KVSwap: Disk-aware KV Cache Offloading for Long-Context On-device Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [KV cache offloading, disk-aware optimization, preloading prediction, hardware-aware disk access]</li>
<li class=""><strong>authors:</strong> Huawei Zhang, Chunwei Xia, Zheng Wang</li>
<li class=""><strong>institution:</strong> University of Leeds</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11907" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11907</a></li>
<li class=""><strong>Simple LLM Summary:</strong> KVSwap is a software framework that offloads key-value cache to disk storage for long-context on-device inference. It uses compact metadata to predict critical cache entries for preloading and orchestrates disk access patterns to match storage characteristics. The system achieves higher throughput under tight memory constraints while maintaining generation quality compared to existing offloading schemes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Modular GPU Programming with Typed Perspectives</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [typed perspectives, collective operations, Bundl calculus, modular programming, thread coordination]</li>
<li class=""><strong>authors:</strong> Manya Bansal, Daniel Sainati, Joseph W. Cutler, Saman Amarasinghe, Jonathan Ragan-Kelley</li>
<li class=""><strong>institution:</strong> Massachusetts Institute of Technology, University of Pennsylvania</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11939" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11939</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Prism, a new GPU programming language that uses typed perspectives to materialize thread granularity at the type level. The approach enables modular programming while maintaining low-level control over collective operations needed for high-performance GPU kernels. The authors demonstrate that Prism provides safety guarantees for writing modular code without sacrificing performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] A Quick and Exact Method for Distributed Quantile Computation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed computing], [Greenwald-Khanna Sketch, GK Select, tree-reduce, exact quantiles, Spark]</li>
<li class=""><strong>authors:</strong> Ivan Cao, Jaromir J. Saloni, David A. G. Harrison</li>
<li class=""><strong>institution:</strong> University of Mississippi</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.12025" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.12025</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces GK Select, a distributed algorithm that computes exact quantiles by using GK Sketch to identify a pivot, extracting candidate values within error bounds, and tree-reducing the results. This method avoids expensive global sorting while maintaining exactness. Empirical results show it achieves sketch-level latency and outperforms Spark&#x27;s full sort by approximately 10.5x on large datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Distributed Seasonal Temporal Pattern Mining</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [distributed hierarchical lookup hash structures, seasonal temporal patterns, time series mining]</li>
<li class=""><strong>authors:</strong> Van Ho-Long, Nguyen Ho, Anh-Vu Dinh-Duc, Ha Manh Tran, Ky Trung Nguyen, Tran Dung Pham, Quoc Viet Hung Nguyen</li>
<li class=""><strong>institution:</strong> International University, Vietnam National University, Loyola University Maryland, Griffith University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.12216" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.12216</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces DSTPM, a distributed framework that uses hierarchical lookup hash structures to mine seasonal temporal patterns from time series data. It significantly outperforms sequential methods in runtime and memory usage while scaling effectively to large datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] High-Performance N-Queens Solver on GPU: Iterative DFS with Zero Bank Conflicts</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [parallel computing], [iterative DFS, GPU shared memory, bank conflict avoidance, parallel backtracking]</li>
<li class=""><strong>authors:</strong> Guangchao Yao, Yali Li</li>
<li class=""><strong>institution:</strong> XiaoPeng Motors</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.12009" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.12009</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes an iterative depth-first search algorithm optimized for GPU parallel computing to solve the N-Queens problem, featuring shared memory mapping and bank conflict avoidance techniques. This approach verified the 27-Queens solution in 28.4 days using eight RTX 5090 GPUs, confirming previous results and achieving over 10x speedup compared to state-of-the-art methods. The method also reduces projected solving time for the 28-Queens problem to approximately 11 months, making it computationally feasible.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] PipeDiT: Accelerating Diffusion Transformers in Video Generation with Task Pipelining and Model Decoupling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [task pipelining, model decoupling, sequence parallelism, attention co-processing]</li>
<li class=""><strong>authors:</strong> Sijie Wang, Qiang Wang, Shaohuai Shi</li>
<li class=""><strong>institution:</strong> Harbin Institute of Technology, Shenzhen</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.12056" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.12056</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PipeDiT introduces a pipelining framework that accelerates video generation through three innovations: pipelined sequence parallelism, decoupling of diffusion and VAE modules, and attention co-processing. The method achieves 1.06x to 4.02x speedups over existing frameworks while maintaining video quality through system-level optimizations rather than algorithmic changes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Striking the Right Balance between Compute and Copy: Improving LLM Inferencing Under Speculative Decoding</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [speculative decoding, KV cache optimization, memory-compute tradeoff, in-place updates, redundant computation]</li>
<li class=""><strong>authors:</strong> Arun Ramachandran, Ramaswamy Govindarajan, Murali Annavaram, Prakash Raghavendra, Hossein Entezari Zarch, Lei Gao, Chaoyi Jiang</li>
<li class=""><strong>institution:</strong> Advanced Micro Devices, Indian Institute of Science, University of Southern California</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.12031" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.12031</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes BMC, a KV cache allocation mechanism that balances memory and compute by allocating redundant rows periodically to enable in-place updates without copy overhead. This approach reduces memory management costs while leveraging redundant computation for speculative decoding. BMC achieves significant throughput improvements over baseline methods and state-of-the-art inference servers on both CPUs and GPUs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Combining Serverless and High-Performance Computing Paradigms to support ML Data-Intensive Applications</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [serverless computing, high-performance computing, distributed data frames, AWS Lambda, NAT Traversal TCP Hole Punching, Cylon]</li>
<li class=""><strong>authors:</strong> Mills Staylor, Arup Kumar Sarker, Gregor von Laszewski, Geoffrey Fox, Yue Cheng, Judy Fox</li>
<li class=""><strong>institution:</strong> University of Virginia</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.12185" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.12185</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Cylon, a high-performance distributed data frame solution that combines serverless and HPC paradigms using a serverless communicator with NAT Traversal TCP Hole Punching. The research demonstrates that AWS Lambda performs below 1% of strong scaling experiments compared to serverful AWS EC2 and traditional HPC systems due to communication bottlenecks in serverless architectures.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] SEE++: Evolving Snowpark Execution Environment for Modern Workloads</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [gVisor, sandboxing, syscall filtering, virtual warehouse, Python execution]</li>
<li class=""><strong>authors:</strong> Gaurav Jain, Brandon Baker, Joe Yin, Chenwei Xie, Zihao Ye, Sidh Kulkarni, Sara Abdelrahman, Nova Qi, Urjeet Shrestha, Mike Halcrow, Dave Bailey, Yuxiong He</li>
<li class=""><strong>institution:</strong> Snowflake, Inc</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.12457" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.12457</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper describes how Snowpark transitioned from its in-house sandboxing solution to gVisor with targeted optimizations to handle modern Data Engineering and AI/ML workloads. The upgraded architecture provides improved security, performance, and maintainability while supporting arbitrary Python package execution. The enhanced Snowpark Execution Environment demonstrates extensibility and flexibility for next-generation workloads through case studies.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Design of A Low-Latency and Parallelizable SVD Dataflow Architecture on FPGA</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [hardware acceleration], [FPGA, SVD, Hestenes method, Dataflow Architecture, Jacobi method, BRAM optimization]</li>
<li class=""><strong>authors:</strong> Fangqiang Du, Sixuan Chong, Zixuan Huang, Rui Qin, Fengnan Mi, Caibao Hu, Jiangang Chen</li>
<li class=""><strong>institution:</strong> East China Normal University, Zhejiang Hospital, Shanghai Publishing and Printing College</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.12461" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.12461</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a Data Stream-Based SVD processing algorithm (DSB Jacobi) implemented on FPGA that reduces on-chip memory usage while improving computational speed. Experimental results show the method reduces on-chip RAM consumption by 41.5% and improves computational efficiency by 23 times compared to previous works, providing a practical solution for real-time SVD computation of large-scale data streams.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] A Decentralized Root Cause Localization Approach for Edge Computing Environments</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [Personalized PageRank, microservice clustering, anomaly scoring, decentralized systems]</li>
<li class=""><strong>authors:</strong> Duneesha Fernando, Maria A. Rodriguez, Rajkumar Buyya</li>
<li class=""><strong>institution:</strong> The University of Melbourne</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.12486" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.12486</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a decentralized root cause localization approach that uses Personalized PageRank within clustered microservices at the edge. The method reduces communication overhead through local cluster analysis and inter-cluster peer-to-peer coordination. Evaluation shows it achieves comparable or better accuracy than centralized approaches while reducing localization time by up to 34%.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Iris: First-Class Multi-GPU Programming Experience in Triton</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [multi-GPU programming, tile-based symmetric memory, compute-communication overlap, Triton, fused kernels]</li>
<li class=""><strong>authors:</strong> Muhammad Awad, Muhammad Osama, Brandon Potter</li>
<li class=""><strong>institution:</strong> Advanced Micro Devices, Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.12500" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.12500</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Iris is a multi-GPU communication library implemented in Python and Triton that provides tile-based symmetric memory abstractions to seamlessly interleave computation and communication. It enables various overlap patterns with minimal code changes while maintaining high performance. The evaluation shows Iris achieves near-optimal bandwidth utilization and up to 1.79x speedup over existing libraries while dramatically simplifying multi-GPU programming.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Artifact for A Non-Intrusive Framework for Deferred Integration of Cloud Patterns in Energy-Efficient Data-Sharing Pipelines</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [cloud computing], [Kubernetes, cloud design patterns, data-sharing pipelines, energy metrics, non-intrusive pattern injection]</li>
<li class=""><strong>authors:</strong> Sepideh Masoudi, Mark Edward Michael Daly, Jannis Kiesel</li>
<li class=""><strong>institution:</strong> Technische Universität Berlin</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.12667" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.12667</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents a Kubernetes-based tool that enables non-intrusive application of cloud design patterns to data-sharing pipelines without modifying service code. The tool automates pattern injection and collects energy metrics to support energy-aware decisions. This approach preserves service reusability across different pipeline structures while improving energy efficiency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] The Time to Consensus in a Blockchain: Insights into Bitcoin&#x27;s &quot;6 Blocks Rule&#x27;&#x27;</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [blockchain consensus], [queueing theory, Laplace transform, simulation, random delays, competing growth processes]</li>
<li class=""><strong>authors:</strong> Partha S. Dey, Aditya S. Gopalan, Vijay G. Subramanian</li>
<li class=""><strong>institution:</strong> University of Illinois Urbana-Champaign</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.12687" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.12687</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes consensus time in Nakamoto blockchains using queueing techniques to model competing honest and adversarial growth processes with random delays. The authors compute the Laplace transform for time to consensus in a Bitcoin model and validate their approach through simulation, providing insights into Bitcoin&#x27;s &quot;6 blocks rule&quot; for achieving permanent consensus.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] QPU Micro-Kernels for Stencil Computation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [QPU micro-kernels, stencil computation, Monte Carlo estimation, shallow quantum circuits, sampling accelerator]</li>
<li class=""><strong>authors:</strong> Stefano Markidis, Luca Pennati, Marco Pasquale, Gilbert Netzer, Ivy Peng</li>
<li class=""><strong>institution:</strong> KTH Royal Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.12617" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.12617</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces QPU micro-kernels - shallow quantum circuits that perform stencil node updates using Monte Carlo estimation from repeated measurements. The approach treats quantum processors as sampling accelerators for solving PDEs while maintaining classical time loops and parallelizing across grid points. Experimental results show the Bernoulli micro-kernel achieves lower errors than branching methods on quantum hardware, with QPU execution dominating wall time.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] A Closer Look at Personalized Fine-Tuning in Heterogeneous Federated Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [federated learning, personalized fine-tuning, linear probing, feature distortion, domain shift]</li>
<li class=""><strong>authors:</strong> Minghui Chen, Hrad Ghoukasian, Ruinan Jin, Zehua Wang, Sai Praneeth Karimireddy, Xiaoxiao Li</li>
<li class=""><strong>institution:</strong> University of British Columbia, Vector Institute, McMaster University, University of Southern California</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.12695" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.12695</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper adapts Linear Probing followed by Fine-Tuning (LP-FT) to federated learning for personalized model training. The method addresses federated feature distortion by using phased parameter updates during local fine-tuning. The authors demonstrate LP-FT&#x27;s superiority over standard fine-tuning in balancing personalization and generalization across various datasets and conditions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Learning Process Energy Profiles from Node-Level Power Data</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [eBPF, perf, power distribution unit, regression model, resource metrics]</li>
<li class=""><strong>authors:</strong> Jonathan Bader, Julius Irion, Jannis Kappel, Joel Witzke, Niklas Fomin, Diellza Sherifi, Odej Kao</li>
<li class=""><strong>institution:</strong> Technische Universit¨at Berlin</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.13155" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.13155</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a method that uses eBPF and perf to collect process-level resource metrics, which are synchronized with node-level power measurements and modeled through regression to predict per-process energy consumption. This approach enables fine-grained energy profiling without hardware-specific limitations like Intel RAPL. The method provides more accurate process-level energy insights for improving data center energy efficiency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] MACKO: Sparse Matrix-Vector Multiplication for Low Sparsity</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [sparse matrix-vector multiplication, GPU optimization, unstructured pruning, kernel co-design, memory reduction]</li>
<li class=""><strong>authors:</strong> Vladimír Macko, Vladimír Boža</li>
<li class=""><strong>institution:</strong> Comenius University Bratislava, GrizzlyTech</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.13061" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.13061</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces MACKO-SpMV, a GPU-optimized format and kernel co-designed for efficient sparse matrix-vector multiplication at low sparsity levels. This approach enables significant memory reduction and speedup for unstructured sparsity without requiring specialized hardware or format-specific precomputation. Empirical results demonstrate that MACKO achieves 1.5× memory reduction and 1.2-1.5× speedup over dense representation at 50% sparsity, making unstructured pruning practical for real-world LLM inference workloads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] On the Fundamental Limits of LLMs at Scale</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [diagonalization, finite description length, positional curricula, sparse attention, hierarchical attention, bounded-oracle retrieval]</li>
<li class=""><strong>authors:</strong> Muhammad Ahmed Mohsin, Muhammad Umer, Ahsan Bilal, Zeeshan Memon, Muhammad Ibtsaam Qadir, Sagnik Bhattacharya, Hassan Rizwan, Abhiram R. Gorle, Maahe Zehra Kazmi, Ayesha Mohsin, Muhammad Usman Rafique, Zihao He, Pulkit Mehta, Muhammad Ali Jamshed, John M. Cioffi</li>
<li class=""><strong>institution:</strong> Stanford University, The University of Oklahoma, Emory University, Purdue University, UC Riverside, UC Berkeley, National University of Sciences and Technology, Zoox, Meta, Google DeepMind, University of Glasgow</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.12869" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.12869</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a unified theoretical framework that formalizes the fundamental limitations of LLM scaling through computability theory, information theory, and statistical learning. It demonstrates that scaling cannot overcome irreducible errors from uncomputable tasks, information-theoretic bounds, and geometric constraints in context processing. The work provides both theoretical foundations and practical mitigation strategies like positional curricula and sparse attention to address these inherent limitations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Pico-Cloud: Cloud Infrastructure for Tiny Edge Devices</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [container-based virtualization, service discovery, lightweight orchestration, single-board computers, edge computing]</li>
<li class=""><strong>authors:</strong> Mordechai Guri</li>
<li class=""><strong>institution:</strong> Ben-Gurion University of the Negev</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.13253" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.13253</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Pico-Cloud, a micro-edge cloud architecture built on minimal hardware platforms like Raspberry Pi Zero that provides container-based virtualization and lightweight orchestration. This approach enables local operation with low latency and power consumption without relying on centralized data centers. The results demonstrate Pico-Cloud as a cost-effective, decentralized platform for lightweight distributed workloads at the network edge.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Distributed Hierarchical Machine Learning for Joint Resource Allocation and Slice Selection in In-Network Edge Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [DeepSets, distributed hierarchical machine learning, permutation equivariance, slack-aware normalization, mixed-integer nonlinear programming, network slicing]</li>
<li class=""><strong>authors:</strong> Sulaiman Muhammad Rashid, Ibrahim Aliyu, Jaehyung Park, Jinsul Kim</li>
<li class=""><strong>institution:</strong> Chonnam National University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.13313" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.13313</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a distributed hierarchical DeepSets-based model for joint resource allocation and slice selection in edge systems. The method decomposes the optimization problem into three sub-problems and uses a shared encoder with task-specific decoders to achieve near-optimal solutions. Experimental results show the approach reduces execution time by 86.1% while maintaining close-to-optimal system performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Asymptotic analysis of cooperative censoring policies in sensor networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [sensor networks], [Markov Decision Process, constant-threshold rules, centralized algorithm]</li>
<li class=""><strong>authors:</strong> Jesus Fernandez-Bes, Rocío Arroyo-Valles, Jesús Cid-Sueiro</li>
<li class=""><strong>institution:</strong> Universidad Carlos III de Madrid, Delft University of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.13492" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.13492</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper models cooperative data censoring in sensor networks using a joint Markov Decision Process to find optimal energy-saving policies. The computationally prohibitive optimal rules are approximated by constant-threshold rules computed via a centralized algorithm. Experimental results show these cooperative censoring policies are energy-efficient and outperform non-cooperative schemes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] InfoDecom: Decomposing Information for Defending against Privacy Leakage in Split Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [split inference, information decomposition, privacy protection, data reconstruction attacks, differential privacy]</li>
<li class=""><strong>authors:</strong> Ruijun Deng, Zhihui Lu, Qiang Duan</li>
<li class=""><strong>institution:</strong> Fudan University, Pennsylvania State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.13365" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.13365</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes InfoDecom, a defense framework that decomposes and removes redundant information from smashed data before injecting calibrated noise to protect against privacy leakage in split inference. This approach addresses the utility degradation problem in existing defenses by targeting only non-redundant information. Experiments show that InfoDecom achieves a superior utility-privacy trade-off compared to existing baselines.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 50</strong></p>
<ul>
<li class="">[arXiv251118] Clustering-Based Weight Orthogonalization for Stabilizing Deep Reinforcement Learning <a href="https://arxiv.org/pdf/2511.11607" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Mind Your Entropy: From Maximum Entropy to Trajectory Entropy-Constrained RL <a href="https://arxiv.org/pdf/2511.11592" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Environment-Aware Transfer Reinforcement Learning for Sustainable Beam Selection <a href="https://arxiv.org/pdf/2511.11647" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Convergence of Multiagent Learning Systems for Traffic control <a href="https://arxiv.org/pdf/2511.11654" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Enhancing Reinforcement Learning in 3D Environments through Semantic Segmentation: A Case Study in ViZDoom <a href="https://arxiv.org/pdf/2511.11703" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Learning to Refine: An Agentic RL Approach for Iterative SPARQL Query Construction <a href="https://arxiv.org/pdf/2511.11770" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Image-POSER: Reflective RL for Multi-Expert Image Generation and Editing <a href="https://arxiv.org/pdf/2511.11780" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Conformal Constrained Policy Optimization for Cost-Effective LLM Agents <a href="https://arxiv.org/pdf/2511.11828" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Better LLM Reasoning via Dual-Play <a href="https://arxiv.org/pdf/2511.11881" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] VULPO: Context-Aware Vulnerability Detection via On-Policy LLM Optimization <a href="https://arxiv.org/pdf/2511.11896" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Quantile Q-Learning: Revisiting Offline Extreme Q-Learning with Quantile Regression <a href="https://arxiv.org/pdf/2511.11973" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Goal-Oriented Multi-Agent Reinforcement Learning for Decentralized Agent Teams <a href="https://arxiv.org/pdf/2511.11992" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Look As You Think: Unifying Reasoning and Visual Evidence Attribution for Verifiable Document RAG via Reinforcement Learning <a href="https://arxiv.org/pdf/2511.12003" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] EARL: Entropy-Aware RL Alignment of LLMs for Reliable RTL Code Generation <a href="https://arxiv.org/pdf/2511.12033" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Intelligent Collaborative Optimization for Rubber Tyre Film Production Based on Multi-path Differentiated Clipping Proximal Policy Optimization <a href="https://arxiv.org/pdf/2511.12060" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Treatment Stitching with Schrödinger Bridge for Enhancing Offline Reinforcement Learning in Adaptive Treatment Strategies <a href="https://arxiv.org/pdf/2511.12075" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] HCPO: Hierarchical Conductor-Based Policy Optimization in Multi-Agent Reinforcement Learning <a href="https://arxiv.org/pdf/2511.12123" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Reward and Guidance through Rubrics: Promoting Exploration to Improve Multi-Domain Reasoning <a href="https://arxiv.org/pdf/2511.12344" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Dynamic Reward Scaling for Multivariate Time Series Anomaly Detection: A VAE-Enhanced Reinforcement Learning Approach <a href="https://arxiv.org/pdf/2511.12351" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Integrating Neural Differential Forecasting with Safe Reinforcement Learning for Blood Glucose Regulation <a href="https://arxiv.org/pdf/2511.12417" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Tailored Primitive Initialization is the Secret Key to Reinforcement Learning <a href="https://arxiv.org/pdf/2511.12429" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Mitigating Length Bias in RLHF through a Causal Lens <a href="https://arxiv.org/pdf/2511.12573" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] NFQ2.0: The CartPole Benchmark Revisited <a href="https://arxiv.org/pdf/2511.12644" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Beyond Fixed Tasks: Unsupervised Environment Design for Task-Level Pairs <a href="https://arxiv.org/pdf/2511.12706" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Prompt-Driven Domain Adaptation for End-to-End Autonomous Driving via In-Context RL <a href="https://arxiv.org/pdf/2511.12755" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Scalable Multi-Objective and Meta Reinforcement Learning via Gradient Estimation <a href="https://arxiv.org/pdf/2511.12779" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Multi-Agent Reinforcement Learning for Heterogeneous Satellite Cluster Resources Optimization <a href="https://arxiv.org/pdf/2511.12792" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Maximizing the efficiency of human feedback in AI alignment: a comparative analysis <a href="https://arxiv.org/pdf/2511.12796" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Expressive Temporal Specifications for Reward Monitoring <a href="https://arxiv.org/pdf/2511.12808" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Mapping fNIRS Signals to Agent Performance: Toward Reinforcement Learning from Neural Feedback <a href="https://arxiv.org/pdf/2511.12844" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Think, Speak, Decide: Language-Augmented Multi-Agent Reinforcement Learning for Economic Decision-Making <a href="https://arxiv.org/pdf/2511.12876" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] DeepSport: A Multimodal Large Language Model for Comprehensive Sports Video Reasoning via Agentic Reinforcement Learning <a href="https://arxiv.org/pdf/2511.12908" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Learning Branching Policies for MILPs with Proximal Policy Optimization <a href="https://arxiv.org/pdf/2511.12986" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] The Good, The Bad, and The Hybrid: A Reward Structure Showdown in Reasoning Models Training <a href="https://arxiv.org/pdf/2511.13016" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Scaling Generative Verifiers For Natural Language Mathematical Proof Verification And Selection <a href="https://arxiv.org/pdf/2511.13027" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] One-Step Generative Policies with Q-Learning: A Reformulation of MeanFlow <a href="https://arxiv.org/pdf/2511.13035" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] STEP: Success-Rate-Aware Trajectory-Efficient Policy Optimization <a href="https://arxiv.org/pdf/2511.13091" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Transformer-Based Scalable Multi-Agent Reinforcement Learning for Networked Systems with Long-Range Interactions <a href="https://arxiv.org/pdf/2511.13103" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Soft Conflict-Resolution Decision Transformer for Offline Multi-Task Reinforcement Learning <a href="https://arxiv.org/pdf/2511.13133" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Conditional Diffusion Model for Multi-Agent Dynamic Task Decomposition <a href="https://arxiv.org/pdf/2511.13137" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] DiffFP: Learning Behaviors from Scratch via Diffusion-based Fictitious Play <a href="https://arxiv.org/pdf/2511.13186" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Learning to Solve Resource-Constrained Project Scheduling Problems with Duration Uncertainty using Graph Neural Networks <a href="https://arxiv.org/pdf/2511.13214" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Explainable RL Policies by Distilling to Locally-Specialized Linear Policies with Voronoi State Partitioning <a href="https://arxiv.org/pdf/2511.13322" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Finding Kissing Numbers with Game-theoretic Reinforcement Learning <a href="https://arxiv.org/pdf/2511.13391" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Artificial Intelligence-driven Intelligent Wearable Systems: A full-stack Integration from Material Design to Personalized Interaction <a href="https://arxiv.org/pdf/2511.13565" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] P1: Mastering Physics Olympiads with Reinforcement Learning <a href="https://arxiv.org/pdf/2511.13612" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Reinforcement Learning for Charging Optimization of Inhomogeneous Dicke Quantum Batteries <a href="https://arxiv.org/pdf/2511.12176" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Reinforcement Learning for Chemical Ordering in Alloy Nanoparticles <a href="https://arxiv.org/pdf/2511.12260" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Discovering autonomous quantum error correction via deep reinforcement learning <a href="https://arxiv.org/pdf/2511.12482" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Accelerated Distributional Temporal Difference Learning with Linear Function Approximation <a href="https://arxiv.org/pdf/2511.12688" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 34</strong></p>
<ul>
<li class="">[arXiv251118] Physics-Informed Neural Network-based Reliability Analysis of Buried Pipelines <a href="https://arxiv.org/pdf/2511.11613" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] DAOpt: Modeling and Evaluation of Data-Driven Optimization under Uncertainty with LLMs <a href="https://arxiv.org/pdf/2511.11576" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Lightweight Hopfield Neural Networks for Bioacoustic Detection and Call Monitoring of Captive Primates <a href="https://arxiv.org/pdf/2511.11615" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] SA-EMO: Structure-Aligned Encoder Mixture of Operators for Generalizable Full-waveform Inversion <a href="https://arxiv.org/pdf/2511.11627" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Tactile Data Recording System for Clothing with Motion-Controlled Robotic Sliding <a href="https://arxiv.org/pdf/2511.11634" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Environment-Aware Transfer Reinforcement Learning for Sustainable Beam Selection <a href="https://arxiv.org/pdf/2511.11647" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Hierarchical Schedule Optimization for Fast and Robust Diffusion Model Sampling <a href="https://arxiv.org/pdf/2511.11688" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Diffusion Models: A Mathematical Introduction <a href="https://arxiv.org/pdf/2511.11746" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Towards autonomous quantum physics research using LLM agents with access to intelligent tools <a href="https://arxiv.org/pdf/2511.11752" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Mesh-based Super-resolution of Detonation Flows with Multiscale Graph Transformers <a href="https://arxiv.org/pdf/2511.12041" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] D<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">^{3}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span></span>ToM: Decider-Guided Dynamic Token Merging for Accelerating Diffusion MLLMs <a href="https://arxiv.org/pdf/2511.12280" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Sangam: Chiplet-Based DRAM-PIM Accelerator with CXL Integration for LLM Inferencing <a href="https://arxiv.org/pdf/2511.12286" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] FERMI-ML: A Flexible and Resource-Efficient Memory-In-Situ SRAM Macro for TinyML acceleration <a href="https://arxiv.org/pdf/2511.12544" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] PID-controlled Langevin Dynamics for Faster Sampling of Generative Models <a href="https://arxiv.org/pdf/2511.12603" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] FedTopo: Topology-Informed Representation Alignment in Federated Learning under Non-I.I.D. Conditions <a href="https://arxiv.org/pdf/2511.12628" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Beyond Fixed Tasks: Unsupervised Environment Design for Task-Level Pairs <a href="https://arxiv.org/pdf/2511.12706" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] INC: An Indirect Neural Corrector for Auto-Regressive Hybrid PDE Solvers <a href="https://arxiv.org/pdf/2511.12764" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] RoS-Guard: Robust and Scalable Online Change Detection with Delay-Optimal Guarantees <a href="https://arxiv.org/pdf/2511.12846" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] An approach of deep reinforcement learning for maximizing the net present value of stochastic projects <a href="https://arxiv.org/pdf/2511.12865" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] MCAQ-YOLO: Morphological Complexity-Aware Quantization for Efficient Object Detection with Curriculum Learning <a href="https://arxiv.org/pdf/2511.12976" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] MeanFlow Transformers with Representation Autoencoders <a href="https://arxiv.org/pdf/2511.13019" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Learning Time-Scale Invariant Population-Level Neural Representations <a href="https://arxiv.org/pdf/2511.13022" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] MM-Telco: Benchmarks and Multimodal Large Language Models for Telecom Applications <a href="https://arxiv.org/pdf/2511.13131" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Warm-starting active-set solvers using graph neural networks <a href="https://arxiv.org/pdf/2511.13174" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] KForge: Program Synthesis for Diverse AI Hardware Accelerators <a href="https://arxiv.org/pdf/2511.13274" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Hardware optimization on Android for inference of AI models <a href="https://arxiv.org/pdf/2511.13453" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Automated Construction of Medical Indicator Knowledge Graphs Using Retrieval Augmented Large Language Models <a href="https://arxiv.org/pdf/2511.13526" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping <a href="https://arxiv.org/pdf/2511.13587" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Data-driven Acceleration of MPC with Guarantees <a href="https://arxiv.org/pdf/2511.13588" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] T-SAR: A Full-Stack Co-design for CPU-Only Ternary LLM Inference via In-Place SIMD ALU Reorganization <a href="https://arxiv.org/pdf/2511.13676" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] QUILL: An Algorithm-Architecture Co-Design for Cache-Local Deformable Attention <a href="https://arxiv.org/pdf/2511.13679" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] From Black Box to Insight: Explainable AI for Extreme Event Preparedness <a href="https://arxiv.org/pdf/2511.13712" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] The Singularity Warfare: The metatheoretical Framework <a href="https://arxiv.org/pdf/2511.11674" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Discovering autonomous quantum error correction via deep reinforcement learning <a href="https://arxiv.org/pdf/2511.12482" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-11-18T14:21:45.000Z" itemprop="dateModified">Nov 18, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/Recommend-System-Note/daily/20251110-20251116"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20251110-20251116</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/Recommend-System-Note/paper/Algorithm"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Algorithm</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-11-17" class="table-of-contents__link toc-highlight">2025-11-17</a></li><li><a href="#2025-11-18" class="table-of-contents__link toc-highlight">2025-11-18</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 xiezilailai, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>