# 20251124-20251130

## 2025-11-24

**cs.DC total: 4**

- **[arXiv251124] Modeling Anomaly Detection in Cloud Services: Analysis of the Properties that Impact Latency and Resource Consumption**
  - **tags:** [sys], [cloud computing], [Stochastic Reward Nets, anomaly detection, precision, recall, inspection frequency]
  - **authors:** Gabriel Job Antunes Grabher, Fumio Machida, Thomas Ropars
  - **institution:** Université Grenoble-Alpes, University of Tsukuba
  - **link:** https://arxiv.org/pdf/2511.17119
  - **Simple LLM Summary:** This paper uses Stochastic Reward Nets to model cloud services with performance anomaly detection. The study analyzes how detector characteristics like precision, recall, and inspection frequency affect latency and resource consumption. Results show that high precision is sufficient when detection runs frequently, while recall becomes more important with infrequent detection.

- **[arXiv251124] MicroMoE: Fine-Grained Load Balancing for Mixture-of-Experts with Token Scheduling**
  - **tags:** [mlsys], [llm training], [token scheduling, expert parallelism, micro-batch optimization]
  - **authors:** Chenqi Zhao, Wenfei Wu, Linhai Song, Yuchen Xu
  - **institution:** Peking University, Institute of Computing Technology, Chinese Academy of Sciences
  - **link:** https://arxiv.org/pdf/2511.16947
  - **Simple LLM Summary:** The paper proposes MicroEP, a parallelization strategy that achieves fine-grained load balancing in Mixture-of-Experts systems through token scheduling across GPUs. It introduces MicroMoE, a distributed training system implementing this approach, which improves training throughput by up to 47.6% compared to state-of-the-art systems while maintaining optimal load balance.

- **[arXiv251124] Optimizing PyTorch Inference with LLM-Based Multi-Agent Systems**
  - **tags:** [mlsys], [GPU kernels], [multi-agent systems, PyTorch optimization, LLM-based tuning, explore-exploit tradeoff, error-fixing agents]
  - **authors:** Kirill Nagaitsev, Luka Grbcic, Samuel Williams, Costin Iancu
  - **institution:** Northwestern University, Lawrence Berkeley National Laboratory
  - **link:** https://arxiv.org/pdf/2511.16964
  - **Simple LLM Summary:** This paper presents a multi-agent system using LLMs to optimize PyTorch inference performance on GPUs. The research shows that exploit-heavy strategies combined with error-fixing agents achieve the best results, with the optimal implementation delivering an average 2.88x speedup on H100 GPUs across diverse machine learning tasks.

- **[arXiv251124] Training Foundation Models on a Full-Stack AMD Platform: Compute, Networking, and System Design**
  - **tags:** [mlsys], [llm training], [mixture-of-experts, transformer sizing, context parallelism, compressed convolutional attention, fault-tolerance, checkpoint-reshaping, microbenchmarks, Pollara interconnect]
  - **authors:** Quentin Anthony, Yury Tokpanov, Skyler Szot, Srivatsan Rajagopal, Praneeth Medepalli, Rishi Iyer, Vasu Shyam, Anna Golubeva, Ansh Chaurasia, Xiao Yang, Tomas Figliolia, Robert Washbourne, Drew Thorstensen, Amartey Pearson, Zack Grossbart, Jason van Patten, Emad Barsoum, Zhenyu Gu, Yao Fu, Beren Millidge
  - **institution:** Zyphra, IBM, AMD
  - **link:** https://arxiv.org/pdf/2511.17127
  - **Simple LLM Summary:** This paper presents the first large-scale mixture-of-experts pretraining study on pure AMD hardware using MI300X GPUs and Pollara interconnect. The authors developed MI300X-aware transformer sizing rules and introduced ZAYA1-base model with novel architectural components like compressed convolutional attention. The results demonstrate that AMD's hardware, networking, and software stack are mature enough for competitive large-scale pretraining, with ZAYA1-base achieving performance comparable to leading base models.


**cs.AI/cs.LG contains "reinforcement learning" total: 11**
- [arXiv251124] CroTad: A Contrastive Reinforcement Learning Framework for Online Trajectory Anomaly Detection [link](https://arxiv.org/pdf/2511.16929)
- [arXiv251124] Multi-Agent Pointer Transformer: Seq-to-Seq Reinforcement Learning for Multi-Vehicle Dynamic Pickup-Delivery Problems [link](https://arxiv.org/pdf/2511.17435)
- [arXiv251124] Convergence and stability of Q-learning in Hierarchical Reinforcement Learning [link](https://arxiv.org/pdf/2511.17351)
- [arXiv251124] Dissecting Quantum Reinforcement Learning: A Systematic Evaluation of Key Components [link](https://arxiv.org/pdf/2511.17112)
- [arXiv251124] Hybrid Differential Reward: Combining Temporal Difference and Action Gradients for Efficient Multi-Agent Reinforcement Learning in Cooperative Driving [link](https://arxiv.org/pdf/2511.16916)
- [arXiv251124] R2PS: Worst-Case Robust Real-Time Pursuit Strategies under Partial Observability [link](https://arxiv.org/pdf/2511.17367)
- [arXiv251124] Predicting Talent Breakout Rate using Twitter and TV data [link](https://arxiv.org/pdf/2511.16905)
- [arXiv251124] MIR: Efficient Exploration in Episodic Multi-Agent Reinforcement Learning via Mutual Intrinsic Reward [link](https://arxiv.org/pdf/2511.17165)
- [arXiv251124] FireScope: Wildfire Risk Prediction with a Chain-of-Thought Oracle [link](https://arxiv.org/pdf/2511.17171)
- [arXiv251124] Harnessing Data from Clustered LQR Systems: Personalized and Collaborative Policy Optimization [link](https://arxiv.org/pdf/2511.17489)
- [arXiv251124] Intrinsic preservation of plasticity in continual quantum learning [link](https://arxiv.org/pdf/2511.17228)

**cs.AI/cs.LG contains "accelerate" total: 8**
- [arXiv251124] Layer-wise Weight Selection for Power-Efficient Neural Network Acceleration [link](https://arxiv.org/pdf/2511.17123)
- [arXiv251124] BITS for GAPS: Bayesian Information-Theoretic Sampling for hierarchical GAussian Process Surrogates [link](https://arxiv.org/pdf/2511.16815)
- [arXiv251124] Stable Coresets via Posterior Sampling: Aligning Induced and Full Loss Landscapes [link](https://arxiv.org/pdf/2511.17399)
- [arXiv251124] Mesh RAG: Retrieval Augmentation for Autoregressive Mesh Generation [link](https://arxiv.org/pdf/2511.16807)
- [arXiv251124] Towards Hyper-Efficient RAG Systems in VecDBs: Distributed Parallel Multi-Resolution Vector Search [link](https://arxiv.org/pdf/2511.16681)
- [arXiv251124] Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation [link](https://arxiv.org/pdf/2511.16757)
- [arXiv251124] Generating transition states of chemical reactions via distance-geometry-based flow matching [link](https://arxiv.org/pdf/2511.17229)
- [arXiv251124] Energy Scaling Laws for Diffusion Models: Quantifying Compute and Carbon Emissions in Image Generation [link](https://arxiv.org/pdf/2511.17031)

## 2025-11-25

**cs.DC total: 18**

- **[arXiv251125] A novel strategy for multi-resource load balancing in agent-based systems**
  - **tags:** [ai], [multi-agent systems], [load balancing, multi-resource management, agent self-assessment, social behavior modeling, adaptation abilities]
  - **authors:** Leszek Sliwko, Aleksander Zgrzywa
  - **institution:** Wroclaw University of Technology
  - **link:** https://arxiv.org/pdf/2511.17580
  - **Simple LLM Summary:** This paper presents a multi-resource load balancing strategy that leverages agent social behavior and adaptation capabilities to optimize enterprise system architectures. The approach enables agents to perform self-assessment for determining optimal configurations. Experimental results demonstrate the effectiveness of the implemented agent system for load balancing in complex environments.

- **[arXiv251125] Simulating Dynamic Cloud Marketspaces: Modeling Spot Instance Behavior and Scheduling with CloudSim Plus**
  - **tags:** [sys], [cloud resource management], [CloudSim Plus, spot instances, HLEM-VMP algorithm, Google Cluster Trace, virtual machine allocation]
  - **authors:** Christoph Goldgruber, Benedikt Pittl, Erich Schikuta
  - **institution:** University of Vienna
  - **link:** https://arxiv.org/pdf/2511.18137
  - **Simple LLM Summary:** This paper extends the CloudSim Plus simulation framework to model spot instance lifecycle management and adapts the HLEM-VMP allocation algorithm for dynamic cloud markets. The enhanced framework demonstrates reduced spot instance interruptions and shorter maximum interruption durations compared to baseline strategies. The work contributes to more robust and cost-effective resource management in volatile cloud computing environments.

- **[arXiv251125] Root Cause Analysis for Microservice Systems via Cascaded Conditional Learning with Hypergraphs**
  - **tags:** [mlsys], [fault-tolerance], [cascaded conditional learning, hypergraph modeling, root cause localization, failure type identification]
  - **authors:** Shuaiyu Xie, Hanbin He, Jian Wang, Bing Li
  - **institution:** Wuhan University, Zhongguancun Laboratory
  - **link:** https://arxiv.org/pdf/2511.17566
  - **Simple LLM Summary:** This paper proposes CCLH, a root cause analysis framework that uses cascaded conditional learning and heterogeneous hypergraphs to model group-level relationships between microservice instances. The method addresses limitations in existing approaches by capturing causal dependencies between diagnostic tasks and simulating failure propagation. Experimental results show CCLH outperforms state-of-the-art methods in both root cause localization and failure type identification.

- **[arXiv251125] Pier: Efficient Large Language Model pretraining with Relaxed Global Communication**
  - **tags:** [mlsys], [llm training], [relaxed global communication, momentum warmup, momentum decay, data parallel, tensor parallel]
  - **authors:** Shuyuan Fan, Zhao Zhang
  - **institution:** Rutgers University
  - **link:** https://arxiv.org/pdf/2511.17849
  - **Simple LLM Summary:** Pier introduces an efficient optimizer that reduces global communication bottlenecks in LLM pretraining through momentum warmup and momentum decay techniques. The system achieves significant speedups (up to 3.7x) for GPT model training while maintaining model performance across validation loss and downstream tasks. It demonstrates effectiveness across different hardware configurations including NVIDIA A100 GPUs and GH200 Superchips.

- **[arXiv251125] ADF-LoRA: Alternating Low-Rank Aggregation for Decentralized Federated Fine-Tuning**
  - **tags:** [mlsys], [llm training], [LoRA, decentralized federated learning, alternating optimization, parameter-efficient fine-tuning, low-rank adaptation]
  - **authors:** Xiaoyu Wang, Xiaotian Li, Zhixiang Zhou, Chen Li, Yong Liu
  - **institution:** New York University
  - **link:** https://arxiv.org/pdf/2511.18291
  - **Simple LLM Summary:** This paper introduces ADF-LoRA, a decentralized federated learning method that synchronizes updates of only one low-rank matrix per round while mixing both matrices to maintain parameter consistency. The approach addresses challenges of phase-state mismatch and block-wise divergence in peer-to-peer communication settings. Experimental results show ADF-LoRA achieves faster convergence and higher accuracy than existing LoRA variants in decentralized federated learning.

- **[arXiv251125] Monotone Decontamination of Arbitrary Dynamic Graphs with Mobile Agents**
  - **tags:** [sys], [network algorithms], [mobile agents, monotone decontamination, dynamic graphs, edge-search, node-search, mixed-search]
  - **authors:** Rajashree Bar, Daibik Barik, Adri Bhattacharya, Partha Sarathi Mandal
  - **institution:** Indian Institute of Technology Guwahati, Indian Statistical Institute
  - **link:** https://arxiv.org/pdf/2511.18315
  - **Simple LLM Summary:** This paper studies monotone decontamination in arbitrary dynamic graphs using mobile agents, proposing two dynamic graph models based on edge reappearance timing. The authors establish both lower bounds and upper bounds on the number of agents required for complete decontamination. Their results demonstrate the challenges posed by dynamic edge changes and optimize agent requirements for network decontamination.

- **[arXiv251125] MIDAS: Adaptive Proxy Middleware for Mitigating Metadata Hotspots in HPC I/O at Scale**
  - **tags:** [sys], [HPC I/O optimization], [consistent hashing, power-of-d sampling, cooperative caching, self-stabilizing control loop, namespace-aware load balancing]
  - **authors:** Sangam Ghimire, Nigam Niraula, Nirjal Bhurtel, Paribartan Timalsina, Bishal Neupane, James Bhattarai, Sudan Jha
  - **institution:** Kathmandu University
  - **link:** https://arxiv.org/pdf/2511.18124
  - **Simple LLM Summary:** MIDAS introduces an adaptive middleware layer that uses namespace-aware load balancing, cooperative caching, and self-stabilizing control loops to mitigate metadata hotspots in HPC systems. The approach operates transparently between clients and metadata servers without requiring kernel or backend modifications. Experimental results show MIDAS reduces average queue lengths by 23% and mitigates worst-case hotspots by up to 80% compared to round-robin scheduling.

- **[arXiv251125] AVERY: Adaptive VLM Split Computing through Embodied Self-Awareness for Efficient Disaster Response Systems**
  - **tags:** [mlsys], [multi-modal inference], [split computing, dual-stream architecture, adaptive compression, vision-language models, edge-cloud computing]
  - **authors:** Rajat Bhattacharjya, Sing-Yao Wu, Hyunwoo Oh, Chaewon Nam, Suyeon Koo, Mohsen Imani, Elaheh Bozorgzadeh, Nikil Dutt
  - **institution:** University of California, Irvine, Kookmin University
  - **link:** https://arxiv.org/pdf/2511.18151
  - **Simple LLM Summary:** AVERY introduces an adaptive split computing framework that divides Vision-Language Models into dual streams for real-time awareness and deep analysis, managed by a self-aware controller. The system dynamically adapts to network conditions by selecting appropriate compression models, balancing accuracy and throughput. Evaluation shows it achieves 11.2% higher accuracy than raw compression and 93.98% lower energy consumption than full-edge execution in disaster response scenarios.

- **[arXiv251125] Comparative Analysis of Large Language Model Inference Serving Systems: A Performance Study of vLLM and HuggingFace TGI**
  - **tags:** [mlsys], [llm inference], [PagedAttention, throughput optimization, latency optimization, GPU memory utilization, batch processing]
  - **authors:** Saicharan Kolluru
  - **institution:** Independent researcher (based on personal email domain)
  - **link:** https://arxiv.org/pdf/2511.17593
  - **Simple LLM Summary:** This paper empirically compares two LLM serving frameworks - vLLM and HuggingFace TGI - across performance metrics like throughput and latency. The study finds that vLLM achieves significantly higher throughput using its PagedAttention mechanism, while TGI performs better for latency-sensitive interactive applications. The choice between frameworks should depend on specific workload requirements, with vLLM excelling in high-throughput scenarios and TGI better for interactive use cases.

- **[arXiv251125] SAGkit: A Python SAG Toolkit for Response Time Analysis of Hybrid-Triggered Jobs**
  - **tags:** [sys], [real-time systems], [schedule-abstraction graph, response-time analysis, hybrid-triggered jobs, non-preemptive systems, job absence]
  - **authors:** Ruide Cao, Zhuyun Qi, Qinyang He, Chenxi Ling, Yi Wang, Guoming Tang
  - **institution:** SUSTech, Tsinghua University, Nankai University, Peng Cheng Laboratory, The Hong Kong University of Science and Technology (Guangzhou)
  - **link:** https://arxiv.org/pdf/2511.17882
  - **Simple LLM Summary:** This paper introduces SAGkit, a Python toolkit implementing the schedule-abstraction graph framework for exact response-time analysis of hybrid-triggered jobs in distributed control systems. The method addresses state-space explosion problems in non-preemptive systems by allowing job absence in the analysis. Experimental results show the toolkit achieves exact timing guarantees with acceptable runtime and memory overhead.

- **[arXiv251125] Federated style aware transformer aggregation of representations**
  - **tags:** [mlsys], [others], [federated learning, prototype aggregation, transformer attention, content-style disentanglement, communication efficiency]
  - **authors:** Mincheol Jeon, Euinam Huh
  - **institution:** Kyung Hee University
  - **link:** https://arxiv.org/pdf/2511.18841
  - **Simple LLM Summary:** The paper proposes FedSTAR, a federated learning framework that disentangles client-specific style factors from shared content representations and uses Transformer-based attention for prototype aggregation. This approach enables adaptive weighting of client contributions while maintaining personalization. Experimental results show improved personalization and robustness in heterogeneous environments without increasing communication costs.

- **[arXiv251125] An Online Fragmentation-Aware GPU Scheduler for Multi-Tenant MIG-based Clouds**
  - **tags:** [mlsys], [cluster infrastructure], [GPU fragmentation, MIG partitioning, greedy scheduling, fragmentation metric]
  - **authors:** Marco Zambianco, Lorenzo Fasol, Roberto Doriguzzi-Corin
  - **institution:** Fondazione Bruno Kessler (FBK)
  - **link:** https://arxiv.org/pdf/2511.18906
  - **Simple LLM Summary:** The paper proposes a fragmentation-aware greedy scheduling algorithm for MIG-based GPU clouds that uses a fragmentation metric to guide allocation decisions. The method minimizes fragmentation growth when placing incoming workloads on GPU slices. Evaluation shows it achieves 10% higher workload acceptance rates under heavy load while using similar GPU resources as baseline methods.

- **[arXiv251125] Low-Rank GEMM: Efficient Matrix Multiplication via Low-Rank Approximation with FP8 Acceleration**
  - **tags:** [mlsys], [GPU kernels], [low-rank approximation, FP8 precision, SVD, randomized SVD, memory bandwidth optimization]
  - **authors:** Alfredo Metere
  - **institution:** Metere Consulting, LLC
  - **link:** https://arxiv.org/pdf/2511.18674
  - **Simple LLM Summary:** This paper introduces Low-Rank GEMM, a matrix multiplication method that uses low-rank approximations with FP8 precision to achieve sub-quadratic complexity. The system automatically selects optimal decomposition methods and precision levels based on hardware capabilities and matrix characteristics. On NVIDIA RTX 4090, it achieves up to 378 TFLOPS with 75% memory savings and 7.8× speedup over traditional approaches for large matrices.

- **[arXiv251125] Constant-Size Certificates for Leader Election in Chordal Graphs and Related Classes**
  - **tags:** [sys], [distributed computing], [local certification, proof labelling schemes, self-stabilizing algorithms, chordal graphs, dismantlable graphs]
  - **authors:** Jérémie Chalopin, Maria Kokkou
  - **institution:** Aix Marseille University, CNRS, LIS
  - **link:** https://arxiv.org/pdf/2511.19208
  - **Simple LLM Summary:** This paper presents constant-size local certification schemes for leader election in chordal graphs and spanning tree construction in dismantlable graphs. The authors develop verification methods where each node can check correctness using only local neighborhood information. They also propose a transformation that converts any certification scheme into a silent self-stabilizing algorithm with minimal overhead.

- **[arXiv251125] AME: An Efficient Heterogeneous Agentic Memory Engine for Smartphones**
  - **tags:** [mlsys], [others], [vector database, hardware-aware matrix pipeline, workload-aware scheduling, on-chip storage optimization, heterogeneous computing]
  - **authors:** Xinkui Zhao, Qingyu Ma, Yifan Zhang, Hengxuan Lou, Guanjie Cheng, Shuiguang Deng, Jianwei Yin
  - **institution:** Zhejiang University
  - **link:** https://arxiv.org/pdf/2511.19192
  - **Simple LLM Summary:** AME introduces a hardware-aware matrix pipeline and workload-aware scheduling scheme to optimize vector database operations on smartphone SoCs. The system achieves significant performance improvements including 1.4x higher query throughput, 7x faster index construction, and 6x higher insertion throughput compared to existing approaches. This demonstrates effective co-design of memory systems with mobile hardware constraints for on-device AI agents.

- **[arXiv251125] N2N: A Parallel Framework for Large-Scale MILP under Distributed Memory**
  - **tags:** [mlsys], [cluster infrastructure], [branch-and-bound, distributed memory, sliding-window algorithm, MPI processes, deterministic solving]
  - **authors:** Longfei Wang, Junyan Liu, Fan Zhang, Jiangwen Wei, Yuanhua Tang, Jie Sun, Xiaodong Luo
  - **institution:** Shenzhen Research Institute of Big Data, Huawei Technologies Co., Ltd., Chinese University of Hong Kong, Shenzhen
  - **link:** https://arxiv.org/pdf/2511.18723
  - **Simple LLM Summary:** The paper proposes N2N, a parallel framework that maps branch-and-bound nodes to distributed computing nodes for solving large-scale MILP problems. It supports both deterministic and nondeterministic modes and integrates with existing solvers like SCIP and HiGHS. Experimental results show N2N-SCIP achieves significant speedups over ParaSCIP, demonstrating 1.98-2.08x faster performance with 1,000 MPI processes.

- **[arXiv251125] IOMMU Support for Virtual-Address Remote DMA in an ARMv8 environment**
  - **tags:** [sys], [computer architecture], [IOMMU, SMMU, DMA, virtual address translation, kernel modules, ARMv8]
  - **authors:** Antonis Psistakis
  - **institution:** University of Crete, Foundation for Research and Technology - Hellas
  - **link:** https://arxiv.org/pdf/2511.19258
  - **Simple LLM Summary:** This thesis implemented custom kernel modules to test ARM's System Memory Management Unit (SMMU) for virtual-to-physical address translation in DMA operations. The research successfully demonstrated SMMU functionality by mapping virtual addresses and enabling DMA transfers from both Processing System and Programmable Logic components. The work confirmed that SMMU can correctly handle address translation across all tested scenarios, supporting virtual-address remote DMA in ARMv8 environments.

- **[arXiv251125] CycleSL: Server-Client Cyclical Update Driven Scalable Split Learning**
  - **tags:** [mlsys], [others], [split learning, cyclical updates, feature resampling, block coordinate descent, aggregation-free training]
  - **authors:** Mengdi Wang, Efe Bozkir, Enkelejda Kasneci
  - **institution:** Technical University of Munich, Munich Center for Machine Learning
  - **link:** https://arxiv.org/pdf/2511.18611
  - **Simple LLM Summary:** CycleSL introduces a cyclical update framework for split learning where the server model is optimized first using resampled client features, followed by client updates using the updated server. This aggregation-free approach addresses scalability issues and performance degradation in existing split learning methods. Empirical results demonstrate improved model performance across multiple datasets with non-iid data distribution and partial client participation.


**cs.AI/cs.LG contains "reinforcement learning" total: 47**
- [arXiv251125] Non-stationary and Varying-discounting Markov Decision Processes for Reinforcement Learning [link](https://arxiv.org/pdf/2511.17598)
- [arXiv251125] A Reinforcement Learning Framework for Resource Allocation in Uplink Carrier Aggregation in the Presence of Self Interference [link](https://arxiv.org/pdf/2511.17931)
- [arXiv251125] PA-FAS: Towards Interpretable and Generalizable Multimodal Face Anti-Spoofing via Path-Augmented Reinforcement Learning [link](https://arxiv.org/pdf/2511.17927)
- [arXiv251125] Boosting Reinforcement Learning in 3D Visuospatial Tasks Through Human-Informed Curriculum Design [link](https://arxiv.org/pdf/2511.17595)
- [arXiv251125] LLM Reasoning for Cold-Start Item Recommendation [link](https://arxiv.org/pdf/2511.18261)
- [arXiv251125] Deterministic Inference across Tensor Parallel Sizes That Eliminates Training-Inference Mismatch [link](https://arxiv.org/pdf/2511.17826)
- [arXiv251125] SPINE: Token-Selective Test-Time Reinforcement Learning with Entropy-Band Regularization [link](https://arxiv.org/pdf/2511.17938)
- [arXiv251125] Hybrid LSTM and PPO Networks for Dynamic Portfolio Optimization [link](https://arxiv.org/pdf/2511.17963)
- [arXiv251125] LEARN: Learning End-to-End Aerial Resource-Constrained Multi-Robot Navigation [link](https://arxiv.org/pdf/2511.17765)
- [arXiv251125] AURA: Adaptive Unified Reasoning and Automation with LLM-Guided MARL for NextG Cellular Networks [link](https://arxiv.org/pdf/2511.17506)
- [arXiv251125] Can we use LLMs to bootstrap reinforcement learning? -- A case study in digital health behavior change [link](https://arxiv.org/pdf/2511.17630)
- [arXiv251125] MOMA-AC: A preference-driven actor-critic framework for continuous multi-objective multi-agent reinforcement learning [link](https://arxiv.org/pdf/2511.18181)
- [arXiv251125] Multi-Value Alignment for LLMs via Value Decorrelation and Extrapolation [link](https://arxiv.org/pdf/2511.17579)
- [arXiv251125] Reward Engineering for Spatial Epidemic Simulations: A Reinforcement Learning Platform for Individual Behavioral Learning [link](https://arxiv.org/pdf/2511.18000)
- [arXiv251125] Generative Adversarial Post-Training Mitigates Reward Hacking in Live Human-AI Music Interaction [link](https://arxiv.org/pdf/2511.17879)
- [arXiv251125] Enhancing Robustness of Offline Reinforcement Learning Under Data Corruption via Sharpness-Aware Minimization [link](https://arxiv.org/pdf/2511.17568)
- [arXiv251125] Smart Manufacturing: MLOps-Enabled Event-Driven Architecture for Enhanced Control in Steel Production [link](https://arxiv.org/pdf/2511.17632)
- [arXiv251125] IE-Critic-R1: Advancing the Explanatory Measurement of Text-Driven Image Editing for Human Perception Alignment [link](https://arxiv.org/pdf/2511.18055)
- [arXiv251125] Deep Gaussian Process Proximal Policy Optimization [link](https://arxiv.org/pdf/2511.18214)
- [arXiv251125] Training Emergent Joint Associations: A Reinforcement Learning Approach to Creative Thinking in Language Models [link](https://arxiv.org/pdf/2511.17876)
- [arXiv251125] Dialogue Diplomats: An End-to-End Multi-Agent Reinforcement Learning System for Automated Conflict Resolution and Consensus Building [link](https://arxiv.org/pdf/2511.17654)
- [arXiv251125] A New Error Temporal Difference Algorithm for Deep Reinforcement Learning in Microgrid Optimization [link](https://arxiv.org/pdf/2511.18093)
- [arXiv251125] A Novel and Practical Universal Adversarial Perturbations against Deep Reinforcement Learning based Intrusion Detection Systems [link](https://arxiv.org/pdf/2511.18223)
- [arXiv251125] Transformers with RL or SFT Provably Learn Sparse Boolean Functions, But Differently [link](https://arxiv.org/pdf/2511.17852)
- [arXiv251125] Tail Distribution of Regret in Optimistic Reinforcement Learning [link](https://arxiv.org/pdf/2511.18247)
- [arXiv251125] General Agentic Memory Via Deep Research [link](https://arxiv.org/pdf/2511.18423)
- [arXiv251125] ORIGAMISPACE: Benchmarking Multimodal LLMs in Multi-Step Spatial Reasoning with Mathematical Constraints [link](https://arxiv.org/pdf/2511.18450)
- [arXiv251125] How to Train Your Latent Control Barrier Function: Smooth Safety Filtering Under Hard-to-Model Constraints [link](https://arxiv.org/pdf/2511.18606)
- [arXiv251125] Multi-Agent Cross-Entropy Method with Monotonic Nonlinear Critic Decomposition [link](https://arxiv.org/pdf/2511.18671)
- [arXiv251125] Reinforcement Learning for Self-Healing Material Systems [link](https://arxiv.org/pdf/2511.18728)
- [arXiv251125] ProxT2I: Efficient Reward-Guided Text-to-Image Generation via Proximal Diffusion [link](https://arxiv.org/pdf/2511.18742)
- [arXiv251125] Periodic Asynchrony: An Effective Method for Accelerating On-Policy Reinforcement Learning [link](https://arxiv.org/pdf/2511.18871)
- [arXiv251125] Accelerating Reinforcement Learning via Error-Related Human Brain Signals [link](https://arxiv.org/pdf/2511.18878)
- [arXiv251125] Learning to Compress Graphs via Dual Agents for Consistent Topological Robustness Evaluation [link](https://arxiv.org/pdf/2511.18958)
- [arXiv251125] FastForward Pruning: Efficient LLM Pruning via Single-Step Reinforcement Learning [link](https://arxiv.org/pdf/2511.18977)
- [arXiv251125] Dynamic Mixture of Experts Against Severe Distribution Shifts [link](https://arxiv.org/pdf/2511.18987)
- [arXiv251125] RAVEN++: Pinpointing Fine-Grained Violations in Advertisement Videos with Active Reinforcement Reasoning [link](https://arxiv.org/pdf/2511.19168)
- [arXiv251125] Adversarial Attack-Defense Co-Evolution for LLM Safety Alignment via Tree-Group Dual-Aware Search and Optimization [link](https://arxiv.org/pdf/2511.19218)
- [arXiv251125] MAESTRO: Multi-Agent Environment Shaping through Task and Reward Optimization [link](https://arxiv.org/pdf/2511.19253)
- [arXiv251125] Leveraging LLMs for reward function design in reinforcement learning control tasks [link](https://arxiv.org/pdf/2511.19355)
- [arXiv251125] LLM-Driven Stationarity-Aware Expert Demonstrations for Multi-Agent Reinforcement Learning in Mobile Systems [link](https://arxiv.org/pdf/2511.19368)
- [arXiv251125] DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research [link](https://arxiv.org/pdf/2511.19399)
- [arXiv251125] Learning Robust Social Strategies with Large Language Models [link](https://arxiv.org/pdf/2511.19405)
- [arXiv251125] SLMFix: Leveraging Small Language Models for Error Fixing with Reinforcement Learning [link](https://arxiv.org/pdf/2511.19422)
- [arXiv251125] Prequential posteriors [link](https://arxiv.org/pdf/2511.17721)
- [arXiv251125] On a Reinforcement Learning Methodology for Epidemic Control, with application to COVID-19 [link](https://arxiv.org/pdf/2511.18035)
- [arXiv251125] Reinforcement Learning for Portfolio Optimization with a Financial Goal and Defined Time Horizons [link](https://arxiv.org/pdf/2511.18076)

**cs.AI/cs.LG contains "accelerate" total: 27**
- [arXiv251125] Learning Rate Scheduling with Matrix Factorization for Private Training [link](https://arxiv.org/pdf/2511.17994)
- [arXiv251125] Reduced-Basis Deep Operator Learning for Parametric PDEs with Independently Varying Boundary and Source Data [link](https://arxiv.org/pdf/2511.18260)
- [arXiv251125] Graph Neural Networks vs Convolutional Neural Networks for Graph Domination Number Prediction [link](https://arxiv.org/pdf/2511.18150)
- [arXiv251125] FastMMoE: Accelerating Multimodal Large Language Models through Dynamic Expert Activation and Routing-Aware Token Pruning [link](https://arxiv.org/pdf/2511.17885)
- [arXiv251125] scipy.spatial.transform: Differentiable Framework-Agnostic 3D Transformations in Python [link](https://arxiv.org/pdf/2511.18157)
- [arXiv251125] A Multidisciplinary Design and Optimization (MDO) Agent Driven by Large Language Models [link](https://arxiv.org/pdf/2511.17511)
- [arXiv251125] Energy-based Autoregressive Generation for Neural Population Dynamics [link](https://arxiv.org/pdf/2511.17606)
- [arXiv251125] Cross-Disciplinary Knowledge Retrieval and Synthesis: A Compound AI Architecture for Scientific Discovery [link](https://arxiv.org/pdf/2511.18298)
- [arXiv251125] Accelerating Time Series Foundation Models with Speculative Decoding [link](https://arxiv.org/pdf/2511.18191)
- [arXiv251125] ProHD: Projection-Based Hausdorff Distance Approximation [link](https://arxiv.org/pdf/2511.18207)
- [arXiv251125] Periodicity-Enforced Neural Network for Designing Deterministic Lateral Displacement Devices [link](https://arxiv.org/pdf/2511.17754)
- [arXiv251125] Gate-level boolean evolutionary geometric attention neural networks [link](https://arxiv.org/pdf/2511.17550)
- [arXiv251125] NEZHA: A Zero-sacrifice and Hyperspeed Decoding Architecture for Generative Recommendations [link](https://arxiv.org/pdf/2511.18793)
- [arXiv251125] FlowSteer: Guiding Few-Step Image Synthesis with Authentic Trajectories [link](https://arxiv.org/pdf/2511.18834)
- [arXiv251125] Accelerating Reinforcement Learning via Error-Related Human Brain Signals [link](https://arxiv.org/pdf/2511.18878)
- [arXiv251125] Skeletons Matter: Dynamic Data Augmentation for Text-to-Query [link](https://arxiv.org/pdf/2511.18934)
- [arXiv251125] Understanding, Accelerating, and Improving MeanFlow Training [link](https://arxiv.org/pdf/2511.19065)
- [arXiv251125] Unboxing the Black Box: Mechanistic Interpretability for Algorithmic Understanding of Neural Networks [link](https://arxiv.org/pdf/2511.19265)
- [arXiv251125] CDLM: Consistency Diffusion Language Models For Faster Sampling [link](https://arxiv.org/pdf/2511.19269)
- [arXiv251125] Tiny-TSM: Efficiently Training a Lightweight SOTA Time Series Foundation Model [link](https://arxiv.org/pdf/2511.19272)
- [arXiv251125] LLM-Driven Stationarity-Aware Expert Demonstrations for Multi-Agent Reinforcement Learning in Mobile Systems [link](https://arxiv.org/pdf/2511.19368)
- [arXiv251125] Flow Map Distillation Without Data [link](https://arxiv.org/pdf/2511.19428)
- [arXiv251125] Physics-informed Neural Operator Learning for Nonlinear Grad-Shafranov Equation [link](https://arxiv.org/pdf/2511.19114)
- [arXiv251125] TorchQuantumDistributed [link](https://arxiv.org/pdf/2511.19291)
- [arXiv251125] High-throughput validation of phase formability and simulation accuracy of Cantor alloys [link](https://arxiv.org/pdf/2511.19335)
- [arXiv251125] Artificial Intelligence Driven Workflow for Accelerating Design of Novel Photosensitizers [link](https://arxiv.org/pdf/2511.19347)
- [arXiv251125] Beyond Protein Language Models: An Agentic LLM Framework for Mechanistic Enzyme Design [link](https://arxiv.org/pdf/2511.19423)

## 2025-11-26

**cs.DC total: 24**

- **[arXiv251126] Temperature in SLMs: Impact on Incident Categorization in On-Premises Environments**
  - **tags:** [mlsys], [llm inference], [temperature hyperparameter, small language models, on-premises deployment, GPU architectures, execution time analysis]
  - **authors:** Marcio Pohlmann, Alex Severo, Gefté Almeida, Diego Kreutz, Tiago Heinrich, Lourenço Pereira
  - **institution:** Federal University of Pampa, Max Planck Institute for Informatics, Instituto Tecnológico de Aeronáutica
  - **link:** https://arxiv.org/pdf/2511.19464
  - **Simple LLM Summary:** This paper evaluates 21 small language models (1B-20B parameters) for automated incident categorization in on-premises environments, testing different temperature settings across two GPU architectures. The study found that temperature has minimal impact on performance, while model size and GPU capacity are the most significant factors affecting execution time and precision.

- **[arXiv251126] Systemic approach for modeling a generic smart grid**
  - **tags:** [sys], [power systems], [systemic approach, distributed optimization, agent/discrete models, multiscale systems, JADE]
  - **authors:** Sofiane Ben Amor, Guillaume Guerard, Loup-Noé Levy
  - **institution:** Laboratory LI-PaRAD, Pole Universitaire Léonard de Vinci, De Vinci Research Center
  - **link:** https://arxiv.org/pdf/2511.19460
  - **Simple LLM Summary:** This paper proposes a systemic approach for modeling smart grids using distributed optimization of subsystems to coordinate production and consumption scheduling. The method employs agent-based and discrete models to create a flexible backbone framework for testing alternative grid scenarios. The approach maintains scalability while enabling validation of assumptions before human-scale implementation.

- **[arXiv251126] Asynchronous Cooperative Optimization of a Capacitated Vehicle Routing Problem Solution**
  - **tags:** [sys], [parallel optimization], [parallel shared-memory, asynchronous optimization, single-trajectory parallelism, iteration-based parallelism]
  - **authors:** Luca Accorsi, Demetrio Laganà, Federico Michelotto, Roberto Musmanno, Daniele Vigo
  - **institution:** Google, University of Calabria, University of Bologna
  - **link:** https://arxiv.org/pdf/2511.19445
  - **Simple LLM Summary:** The paper proposes FILO2^x, a parallel shared-memory adaptation of the FILO2 algorithm for solving Capacitated Vehicle Routing Problems. This approach enables asynchronous optimization of multiple solution areas with minimal synchronization. Computational results show FILO2^x significantly reduces resolution time while maintaining similar solution quality compared to the original single-threaded approach.

- **[arXiv251126] Towards a future space-based, highly scalable AI infrastructure system design**
  - **tags:** [mlsys], [cluster infrastructure], [satellite clusters, free-space optical links, formation flight, tensor processing units, radiation testing, low-Earth orbit]
  - **authors:** Blaise Agüera y Arcas, Travis Beals, Maria Biggs, Jessica V. Bloom, Thomas Fischbacher, Konstantin Gromov, Urs Köster, Rishiraj Pravahan, James Manyika
  - **institution:** Google
  - **link:** https://arxiv.org/pdf/2511.19468
  - **Simple LLM Summary:** This paper proposes a space-based AI infrastructure system using solar-powered satellite clusters equipped with TPU accelerators and interconnected via free-space optical links. The system employs formation flight of satellites in close proximity and ML-based control models for large-scale constellations. The authors conclude that this approach could provide a highly scalable solution for future AI compute demands by directly tapping into solar energy in space.

- **[arXiv251126] SparOA: Sparse and Operator-aware Hybrid Scheduling for Edge DNN Inference**
  - **tags:** [mlsys], [llm inference], [hybrid scheduling, sparsity, computational intensity, reinforcement learning, asynchronous execution, batch optimization]
  - **authors:** Ziyang Zhang, Jie Liu, Luca Mottola
  - **institution:** Politecnico di Milano, Harbin Institute of Technology
  - **link:** https://arxiv.org/pdf/2511.19457
  - **Simple LLM Summary:** SparOA is a CPU-GPU hybrid inference framework that optimizes DNN operator scheduling by leveraging sparsity and computational intensity through threshold prediction and reinforcement learning. The system achieves significant performance improvements with 1.22-1.31x average speedup over baselines and reduces energy consumption by 7-16% compared to state-of-the-art co-execution methods.

- **[arXiv251126] AI-driven Predictive Shard Allocation for Scalable Next Generation Blockchains**
  - **tags:** [mlsys], [cluster infrastructure], [predictive shard allocation, temporal workload forecasting, safe-PPO, deterministic machine learning execution, cross-shard communication optimization]
  - **authors:** M. Zeeshan Haider, Tayyaba Noreen, M. D. Assuncao, Kaiwen Zhang
  - **institution:** École de technologie supérieure (ÉTS)
  - **link:** https://arxiv.org/pdf/2511.19450
  - **Simple LLM Summary:** This paper proposes PSAP, an AI-driven framework that integrates temporal workload forecasting with safety-constrained reinforcement learning to dynamically allocate blockchain shards. The protocol enables multi-block-ahead prediction and adaptive reconfiguration while maintaining Byzantine safety through deterministic execution. Experimental results show up to 2x throughput improvement and 35% lower latency compared to existing sharding approaches.

- **[arXiv251126] Optimizations on Graph-Level for Domain Specific Computations in Julia and Application to QED**
  - **tags:** [sys], [HPC Optimization], [Directed Acyclic Graph, Code Generation, Static Scheduling, Domain-Specific Information, Julia Programming]
  - **authors:** Anton Reinhard, Simeon Ehrig, René Widera, Michael Bussmann, Uwe Hernandez Acosta
  - **institution:** Helmholtz-Zentrum Dresden-Rossendorf, Center for Advanced Systems Understanding
  - **link:** https://arxiv.org/pdf/2511.19456
  - **Simple LLM Summary:** The paper presents a Julia framework that uses directed acyclic graphs with domain-specific information to automatically generate statically scheduled and compiled code for heterogeneous hardware. This approach enables graph-level optimizations that traditional compilers cannot achieve due to lack of domain knowledge. The method is demonstrated through quantum electrodynamics matrix element computations, showing improved performance portability across diverse hardware systems.

- **[arXiv251126] AVS: A Computational and Hierarchical Storage System for Autonomous Vehicles**
  - **tags:** [mlsys], [others], [hierarchical storage, hot-cold tiering, modality-aware compression, metadata indexing, embedded indexing, data archival]
  - **authors:** Yuxin Wang, Yuankai He, Weisong Shi
  - **institution:** University of Delaware
  - **link:** https://arxiv.org/pdf/2511.19453
  - **Simple LLM Summary:** The paper presents AVS, a computational storage system for autonomous vehicles that co-designs computation with hierarchical storage including modality-aware reduction, hot-cold tiering, and lightweight metadata indexing. The system was validated on embedded hardware using real autonomous driving traces and demonstrates predictable real-time data ingestion, fast selective retrieval, and significant footprint reduction. The work concludes that storage should be treated as a first-class component in autonomous vehicle stacks to support emerging third-party applications and efficient data management.

- **[arXiv251126] Opt4GPTQ: Co-Optimizing Memory and Computation for 4-bit GPTQ Quantized LLM Inference on Heterogeneous Platforms**
  - **tags:** [mlsys], [llm inference], [GPTQ quantization, shared memory buffering, vectorized memory loading, inline assembly, vLLM]
  - **authors:** Yaozheng Zhang, Wei Wang, Jie Kong, Jiehan Zhou, Huanqing Cui
  - **institution:** Shandong University of Science and Technology
  - **link:** https://arxiv.org/pdf/2511.19438
  - **Simple LLM Summary:** This paper proposes Opt4GPTQ, a platform-level optimization method for 4-bit GPTQ quantized LLM inference that integrates three optimization strategies: shared memory buffering, vectorized memory loading, and inline assembly. The method achieves up to 84.42% throughput improvement and 51.35% latency reduction across different models. The work demonstrates the importance of platform-level engineering optimizations for efficient LLM inference on heterogeneous AI accelerators.

- **[arXiv251126] Urban Buildings Energy Consumption Estimation Using HPC: A Case Study of Bologna**
  - **tags:** [sys], [urban energy modeling], [EnergyPlus, HPC, LiDAR, geospatial data, building simulation]
  - **authors:** Aldo Canfora, Eleonora Bergamaschi, Riccardo Mioli, Federico Battini, Mirko Degli Esposti, Giorgio Pedrazzi, Chiara Dellacasa
  - **institution:** University of Bologna, Cineca, IFAB Foundation
  - **link:** https://arxiv.org/pdf/2511.19463
  - **Simple LLM Summary:** This paper presents an Urban Building Energy Modeling pipeline that integrates EnergyPlus simulations with high-performance computing and open geospatial datasets to estimate building energy demand in Bologna. The method combines building geometry from open data and LiDAR with construction attributes from regulatory databases. Using the Leonardo supercomputer, the system successfully simulated approximately 25,000 buildings in under 30 minutes, demonstrating efficient city-scale energy consumption analysis.

- **[arXiv251126] Federated Learning Framework for Scalable AI in Heterogeneous HPC and Cloud Environments**
  - **tags:** [mlsys], [cluster infrastructure], [federated learning, heterogeneous systems, resource scheduling, fault tolerance, non-IID data]
  - **authors:** Sangam Ghimire, Paribartan Timalsina, Nirjal Bhurtel, Bishal Neupane, Bigyan Byanju Shrestha, Subarna Bhattarai, Prajwal Gaire, Jessica Thapa, Sudan Jha
  - **institution:** Kathmandu University
  - **link:** https://arxiv.org/pdf/2511.19479
  - **Simple LLM Summary:** This paper presents a federated learning framework designed to operate efficiently across mixed HPC and cloud environments. The system addresses challenges like system heterogeneity, communication overhead, and resource scheduling while maintaining model accuracy and data privacy. Experimental results demonstrate strong scalability, fault tolerance, and convergence performance even with non-IID data distributions and varied hardware.

- **[arXiv251126] ParaBlock: Communication-Computation Parallel Block Coordinate Federated Learning for Large Language Models**
  - **tags:** [mlsys], [llm training], [federated learning, block coordinate descent, communication-computation parallelism]
  - **authors:** Yujia Wang, Yuanpu Cao, Jinghui Chen
  - **institution:** Pennsylvania State University
  - **link:** https://arxiv.org/pdf/2511.19959
  - **Simple LLM Summary:** ParaBlock introduces a communication-computation parallel approach for federated block coordinate descent in large language model training. The method establishes parallel threads to overlap communication and computation phases, reducing latency. Theoretical analysis shows equivalent convergence to standard methods while empirical results demonstrate maintained performance with significantly improved communication efficiency.

- **[arXiv251126] PolarStore: High-Performance Data Compression for Large-Scale Cloud-Native Databases**
  - **tags:** [sys], [database compression], [dual-layer compression, in-storage compression, PolarCSD hardware, software compression, compression-aware scheduling]
  - **authors:** Qingda Hu, Xinjun Yang, Feifei Li, Junru Li, Ya Lin, Yuqi Zhou, Yicong Zhu, Junwei Zhang, Rongbiao Xie, Ling Zhou, Bin Wu, Wenchao Zhou
  - **institution:** Alibaba Cloud Computing
  - **link:** https://arxiv.org/pdf/2511.19949
  - **Simple LLM Summary:** PolarStore introduces a dual-layer compression system combining hardware-based in-storage compression with lightweight software compression for cloud-native databases. The system achieves a 3.55 compression ratio and reduces storage costs by approximately 60% while maintaining performance comparable to uncompressed clusters through database-oriented optimizations and compression-aware scheduling.

- **[arXiv251126] Stragglers Can Contribute More: Uncertainty-Aware Distillation for Asynchronous Federated Learning**
  - **tags:** [mlsys], [others], [asynchronous federated learning, uncertainty-aware distillation, model aggregation, heterogeneous data, straggler clients]
  - **authors:** Yujia Wang, Fenglong Ma, Jinghui Chen
  - **institution:** Pennsylvania State University
  - **link:** https://arxiv.org/pdf/2511.19966
  - **Simple LLM Summary:** The paper proposes FedEcho, a framework that uses uncertainty-aware distillation to dynamically assess the reliability of predictions from straggler clients in asynchronous federated learning. This approach mitigates the negative impacts of outdated updates and data heterogeneity by prioritizing more certain predictions while leveraging diverse client information. Experimental results show FedEcho outperforms existing asynchronous FL baselines while maintaining client data privacy.

- **[arXiv251126] Improved Linear-Time Construction of Minimal Dominating Set via Mobile Agents**
  - **tags:** [sys], [distributed algorithms], [mobile agents, minimal dominating set, synchronous model, dispersion algorithm, spanning tree, leader election]
  - **authors:** Prabhat Kumar Chand, Anisur Rahaman Molla
  - **institution:** Indian Statistical Institute
  - **link:** https://arxiv.org/pdf/2511.19880
  - **Simple LLM Summary:** This paper presents two new algorithms that use mobile agents to compute minimal dominating sets in anonymous graphs with linear time complexity. The algorithms build upon optimal dispersion techniques and achieve O(n) rounds using only O(log n) bits of memory per agent without requiring global parameter knowledge. As byproducts, the methods also construct spanning trees and elect leaders efficiently within the same time bounds.

- **[arXiv251126] Batch Denoising for AIGC Service Provisioning in Wireless Edge Networks**
  - **tags:** [mlsys], [diffusion inference], [batch denoising, STACKING algorithm, bandwidth allocation, joint optimization]
  - **authors:** Jinghang Xu, Kun Guo, Wei Teng, Chenxi Liu, Wei Feng
  - **institution:** East China Normal University, Shaanxi Normal University, Beijing University of Posts and Telecommunications, Tsinghua University
  - **link:** https://arxiv.org/pdf/2511.19847
  - **Simple LLM Summary:** The paper proposes a batch denoising framework and STACKING algorithm to optimize AIGC service provisioning in wireless edge networks, jointly optimizing content generation and transmission. The approach leverages empirical observations about denoising step importance and parallelism to reduce latency while maintaining quality. Simulation results demonstrate superior performance in delivering high-quality, lower-latency AIGC services compared to existing methods.

- **[arXiv251126] Enabling Scientific Workflow Scheduling Research in Non-Uniform Memory Access Architectures**
  - **tags:** [sys], [HPC workflow scheduling], [NUMA-aware scheduling, workflow execution runtime, simulation models, data locality optimization]
  - **authors:** Aurelio Vivas, Harold Castro
  - **institution:** Universidad de los Andes
  - **link:** https://arxiv.org/pdf/2511.19832
  - **Simple LLM Summary:** This paper introduces nFlows, a NUMA-aware workflow execution runtime system that enables modeling, simulation, and bare-metal execution of scheduling algorithms for data-intensive workflows on HPC systems. The system addresses the challenges of non-uniform memory access architectures by considering data locality both across distributed environments and within individual computing nodes. The main conclusion is that nFlows facilitates the study of NUMA effects, design of NUMA-aware algorithms, and identification of performance bottlenecks in scientific workflow execution.

- **[arXiv251126] SwitchDelta: Asynchronous Metadata Updating for Distributed Storage with In-Network Data Visibility**
  - **tags:** [sys], [distributed storage], [programmable switches, asynchronous metadata updating, in-network data visibility, ordered writes]
  - **authors:** Junru Li, Qing Wang, Zhe Yang, Shuo Liu, Jiwu Shu, Youyou Lu
  - **institution:** Tsinghua University, Nanjing University, Huawei Technologies Co., Ltd.
  - **link:** https://arxiv.org/pdf/2511.19978
  - **Simple LLM Summary:** SwitchDelta accelerates distributed storage write operations by moving metadata updates out of the critical path using programmable switches to buffer in-flight metadata updates. This approach enables data visibility in the network while maintaining strong consistency. Evaluation shows it reduces write latency by up to 52.4% and increases throughput by up to 126.9% under write-heavy workloads.

- **[arXiv251126] Accelerating Wireless Distributed Learning via Hybrid Split and Federated Learning Optimization**
  - **tags:** [mlsys], [others], [hybrid split and federated learning, batch size optimization, block coordinate descent, rounding algorithm, delay minimization]
  - **authors:** Kun Guo, Xuefei Li, Xijun Wang, Howard H. Yang, Wei Feng, Tony Q. S. Quek
  - **institution:** East China Normal University, Sun Yat-sen University, Zhejiang University, Tsinghua University, Singapore University of Technology and Design
  - **link:** https://arxiv.org/pdf/2511.19851
  - **Simple LLM Summary:** This paper proposes a hybrid split and federated learning (HSFL) optimization approach that combines parallel FL training with sequential SL training. It develops a two-stage solution using block coordinate descent and rounding algorithms to jointly optimize learning modes, batch sizes, and resource allocation. Experimental results show this method significantly accelerates convergence to target accuracy compared to existing approaches.

- **[arXiv251126] QiMeng-Kernel: Macro-Thinking Micro-Coding Paradigm for LLM-Based High-Performance GPU Kernel Generation**
  - **tags:** [mlsys], [GPU kernels], [Macro Thinking Micro Coding, reinforcement learning, LLM-based code generation, staged optimization, hierarchical framework]
  - **authors:** Xinguo Zhu, Shaohui Peng, Jiaming Guo, Yunji Chen, Qi Guo, Yuanbo Wen, Hang Qin, Ruizhi Chen, Qirui Zhou, Ke Gao, Yanjun Wu, Chen Zhao, Ling Li
  - **institution:** Institute of Software Chinese Academy of Sciences, Institute of Computing Technology Chinese Academy of Sciences, University of Chinese Academy of Sciences
  - **link:** https://arxiv.org/pdf/2511.20100
  - **Simple LLM Summary:** The paper proposes a hierarchical framework called Macro Thinking Micro Coding (MTMC) that decouples optimization strategy from implementation details for GPU kernel generation. Macro Thinking uses reinforcement learning to guide lightweight LLMs in exploring optimization strategies, while Micro Coding leverages general-purpose LLMs for incremental implementation. Results show MTMC achieves superior accuracy and speedup compared to existing approaches on standard benchmarks.

- **[arXiv251126] Beluga: A CXL-Based Memory Architecture for Scalable and Efficient LLM KVCache Management**
  - **tags:** [mlsys], [llm inference], [CXL, KVCache management, memory architecture, disaggregated memory, vLLM]
  - **authors:** Xinjun Yang, Qingda Hu, Junru Li, Feifei Li, Yuqi Zhou, Yicong Zhu, Qiuru Lin, Jian Dai, Yang Kong, Jiayu Zhang, Guoqiang Xu, Qiang Liu
  - **institution:** Alibaba Cloud Computing
  - **link:** https://arxiv.org/pdf/2511.20172
  - **Simple LLM Summary:** Beluga proposes a CXL-based memory architecture that enables GPUs and CPUs to directly access shared large-scale memory pools through CXL switches for efficient LLM KVCache management. The system achieves near-local memory latency while reducing programming complexity compared to RDMA-based solutions. Beluga-KVCache demonstrates 89.6% reduction in Time-To-First-Token and 7.35× throughput improvement in vLLM inference engine.

- **[arXiv251126] Interactive Visualization of Proof-of-Work Consensus Protocol on Raspberry Pi**
  - **tags:** [sys], [blockchain education], [proof-of-work consensus, ethereum nodes, raspberry pi network, lcd visualization, web-based interface]
  - **authors:** Anton Ivashkevich, Matija Piškorec, Claudio J. Tessone
  - **institution:** University of Zurich, Ruđer Bošković Institute
  - **link:** https://arxiv.org/pdf/2511.20391
  - **Simple LLM Summary:** The paper presents an educational prototype system that runs a fully functional Ethereum Proof-of-Work blockchain network on multiple Raspberry Pi computers with LCD displays for real-time visualization. The system demonstrates blockchain consensus mechanisms and their degradation under configurable network conditions through a web-based control interface. This setup effectively makes complex blockchain concepts accessible to non-technical audiences for educational purposes.

- **[arXiv251126] Parallel simulation and adaptive mesh refinement for 3D elastostatic contact mechanics problems between deformable bodies**
  - **tags:** [sys], [computational mechanics], [adaptive mesh refinement, finite element method, MPI parallelization, node-to-node contact pairing, penalization technique, super-parametric elements]
  - **authors:** Alexandre Epalle, Isabelle Ramière, Guillaume Latu, Frédéric Lebon
  - **institution:** CEA, Aix-Marseille Université
  - **link:** https://arxiv.org/pdf/2511.20142
  - **Simple LLM Summary:** This paper presents a parallel adaptive mesh refinement algorithm for 3D elastostatic contact problems that combines node-to-node contact pairing with penalization techniques and non-conforming h-adaptive refinement. The method uses super-parametric elements to preserve domain curvature and ensures contact-paired nodes reside on the same MPI tasks to reduce communication. The approach demonstrates strong scalability up to 1024 cores and effectively handles contact detection even with coarse initial meshes and low-order discretizations.

- **[arXiv251126] Efficient Parallel Implementation of the Pilot Assignment Problem in Massive MIMO Systems**
  - **tags:** [mlsys], [others], [genetic algorithm, k-means clustering, FPGA, high-level synthesis, loop unrolling, pipelining, function inlining]
  - **authors:** Eman Alqudah, Ashfaq Khokhar
  - **institution:** Iowa State University
  - **link:** https://arxiv.org/pdf/2511.20511
  - **Simple LLM Summary:** This paper proposes a hybrid K-means clustering and Genetic Algorithm approach for pilot assignment in massive MIMO systems, with a parallel FPGA implementation using Vivado High-Level Synthesis. The method achieves significant speed improvements, reducing convergence time from 116s with conventional GA to just 3.5ms with the parallel implementation. This makes the approach highly suitable for low-latency real-time applications in 6G wireless networks.


**cs.AI/cs.LG contains "reinforcement learning" total: 28**
- [arXiv251126] Position: The Complexity of Perfect AI Alignment -- Formalizing the RLHF Trilemma [link](https://arxiv.org/pdf/2511.19504)
- [arXiv251126] Discover, Learn, and Reinforce: Scaling Vision-Language-Action Pretraining with Diverse RL-Generated Trajectories [link](https://arxiv.org/pdf/2511.19528)
- [arXiv251126] HunyuanOCR Technical Report [link](https://arxiv.org/pdf/2511.19575)
- [arXiv251126] Learning Massively Multitask World Models for Continuous Control [link](https://arxiv.org/pdf/2511.19584)
- [arXiv251126] Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs [link](https://arxiv.org/pdf/2511.19773)
- [arXiv251126] Learning to Clean: Reinforcement Learning for Noisy Label Correction [link](https://arxiv.org/pdf/2511.19808)
- [arXiv251126] CropVLM: Learning to Zoom for Fine-Grained Vision-Language Perception [link](https://arxiv.org/pdf/2511.19820)
- [arXiv251126] Reinforcement Learning with $ω$-Regular Objectives and Constraints [link](https://arxiv.org/pdf/2511.19849)
- [arXiv251126] Complex Instruction Following with Diverse Style Policies in Football Games [link](https://arxiv.org/pdf/2511.19885)
- [arXiv251126] Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning [link](https://arxiv.org/pdf/2511.19900)
- [arXiv251126] Designing Reputation Systems for Manufacturing Data Trading Markets: A Multi-Agent Evaluation with Q-Learning and IRL-Estimated Utilities [link](https://arxiv.org/pdf/2511.19930)
- [arXiv251126] Optimize Flip Angle Schedules In MR Fingerprinting Using Reinforcement Learning [link](https://arxiv.org/pdf/2511.19941)
- [arXiv251126] Differential Smoothing Mitigates Sharpening and Improves LLM Reasoning [link](https://arxiv.org/pdf/2511.19942)
- [arXiv251126] Energy Costs and Neural Complexity Evolution in Changing Environments [link](https://arxiv.org/pdf/2511.20018)
- [arXiv251126] SOMBRL: Scalable and Optimistic Model-Based RL [link](https://arxiv.org/pdf/2511.20066)
- [arXiv251126] From data to concepts via wiring diagrams [link](https://arxiv.org/pdf/2511.20138)
- [arXiv251126] Interactive AI NPCs Powered by LLMs: Technical Report for the CPDC Challenge 2025 [link](https://arxiv.org/pdf/2511.20200)
- [arXiv251126] Leveraging weights signals - Predicting and improving generalizability in reinforcement learning [link](https://arxiv.org/pdf/2511.20234)
- [arXiv251126] Quantum-Enhanced Reinforcement Learning for Accelerating Newton-Raphson Convergence with Ising Machines: A Case Study for Power Flow Analysis [link](https://arxiv.org/pdf/2511.20237)
- [arXiv251126] NNGPT: Rethinking AutoML with Large Language Models [link](https://arxiv.org/pdf/2511.20333)
- [arXiv251126] Soft Adaptive Policy Optimization [link](https://arxiv.org/pdf/2511.20347)
- [arXiv251126] Complexity Reduction Study Based on RD Costs Approximation for VVC Intra Partitioning [link](https://arxiv.org/pdf/2511.20349)
- [arXiv251126] DRAFT-RL: Multi-Agent Chain-of-Draft Reasoning for Reinforcement Learning-Enhanced LLMs [link](https://arxiv.org/pdf/2511.20468)
- [arXiv251126] Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning [link](https://arxiv.org/pdf/2511.20549)
- [arXiv251126] Attention Trajectories as a Diagnostic Axis for Deep Reinforcement Learning [link](https://arxiv.org/pdf/2511.20591)
- [arXiv251126] MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models [link](https://arxiv.org/pdf/2511.20629)
- [arXiv251126] CycleChemist: A Dual-Pronged Machine Learning Framework for Organic Photovoltaic Discovery [link](https://arxiv.org/pdf/2511.19500)
- [arXiv251126] Optimization and Regularization Under Arbitrary Objectives [link](https://arxiv.org/pdf/2511.19628)

**cs.AI/cs.LG contains "accelerate" total: 25**
- [arXiv251126] stable-pretraining-v1: Foundation Model Research Made Simple [link](https://arxiv.org/pdf/2511.19484)
- [arXiv251126] RFX: High-Performance Random Forests with GPU Acceleration and QLORA Compression [link](https://arxiv.org/pdf/2511.19493)
- [arXiv251126] Towards Efficient VLMs: Information-Theoretic Driven Compression via Adaptive Structural Pruning [link](https://arxiv.org/pdf/2511.19518)
- [arXiv251126] Learning to Solve Weighted Maximum Satisfiability with a Co-Training Architecture [link](https://arxiv.org/pdf/2511.19544)
- [arXiv251126] Trust-Based Social Learning for Communication (TSLEC) Protocol Evolution in Multi-Agent Reinforcement Learning [link](https://arxiv.org/pdf/2511.19562)
- [arXiv251126] Learning Massively Multitask World Models for Continuous Control [link](https://arxiv.org/pdf/2511.19584)
- [arXiv251126] Agint: Agentic Graph Compilation for Software Engineering Agents [link](https://arxiv.org/pdf/2511.19635)
- [arXiv251126] Synthetic Data: AI's New Weapon Against Android Malware [link](https://arxiv.org/pdf/2511.19649)
- [arXiv251126] Training-Free Active Learning Framework in Materials Science with Large Language Models [link](https://arxiv.org/pdf/2511.19730)
- [arXiv251126] CAMformer: Associative Memory is All You Need [link](https://arxiv.org/pdf/2511.19740)
- [arXiv251126] Scaling Item-to-Standard Alignment with Large Language Models: Accuracy, Limits, and Solutions [link](https://arxiv.org/pdf/2511.19749)
- [arXiv251126] When +1% Is Not Enough: A Paired Bootstrap Protocol for Evaluating Small Improvements [link](https://arxiv.org/pdf/2511.19794)
- [arXiv251126] Designing Reputation Systems for Manufacturing Data Trading Markets: A Multi-Agent Evaluation with Q-Learning and IRL-Estimated Utilities [link](https://arxiv.org/pdf/2511.19930)
- [arXiv251126] Optimize Flip Angle Schedules In MR Fingerprinting Using Reinforcement Learning [link](https://arxiv.org/pdf/2511.19941)
- [arXiv251126] On-Demand Multi-Task Sparsity for Efficient Large-Model Deployment on Edge Devices [link](https://arxiv.org/pdf/2511.19986)
- [arXiv251126] Decoupling and Damping: Structurally-Regularized Gradient Matching for Multimodal Graph Condensation [link](https://arxiv.org/pdf/2511.20222)
- [arXiv251126] MXtalTools: A Toolkit for Machine Learning on Molecular Crystals [link](https://arxiv.org/pdf/2511.20327)
- [arXiv251126] 3D Motion Perception of Binocular Vision Target with PID-CNN [link](https://arxiv.org/pdf/2511.20332)
- [arXiv251126] Complexity Reduction Study Based on RD Costs Approximation for VVC Intra Partitioning [link](https://arxiv.org/pdf/2511.20349)
- [arXiv251126] Block Cascading: Training Free Acceleration of Block-Causal Video Models [link](https://arxiv.org/pdf/2511.20426)
- [arXiv251126] Dance Style Classification using Laban-Inspired and Frequency-Domain Motion Features [link](https://arxiv.org/pdf/2511.20469)
- [arXiv251126] Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning [link](https://arxiv.org/pdf/2511.20549)
- [arXiv251126] A Tale of Two Geometries: Adaptive Optimizers and Non-Euclidean Descent [link](https://arxiv.org/pdf/2511.20584)
- [arXiv251126] DiFR: Inference Verification Despite Nondeterminism [link](https://arxiv.org/pdf/2511.20621)
- [arXiv251126] Image2Gcode: Image-to-G-code Generation for Additive Manufacturing Using Diffusion-Transformer Model [link](https://arxiv.org/pdf/2511.20636)

## 2025-11-27

**cs.DC total: 14**

- **[arXiv251127] Privacy in Federated Learning with Spiking Neural Networks**
  - **tags:** [mlsys], [others], [federated learning, spiking neural networks, gradient inversion attacks, surrogate gradients, neuromorphic computation]
  - **authors:** Dogukan Aksu, Jesus Martinez del Rincon, Ihsen Alouani
  - **institution:** Queen's University Belfast
  - **link:** https://arxiv.org/pdf/2511.21181
  - **Simple LLM Summary:** This paper investigates gradient inversion attacks on Spiking Neural Networks in federated learning settings. The research adapts gradient leakage attacks to SNNs and finds that their event-driven dynamics and surrogate-gradient training produce noisy, uninformative reconstructions compared to conventional neural networks. The study concludes that SNNs offer inherent privacy-preserving advantages in federated learning systems.

- **[arXiv251127] Readout-Side Bypass for Residual Hybrid Quantum-Classical Models**
  - **tags:** [mlsys], [others], [residual hybrid architecture, quantum-classical interface, federated learning, privacy preservation, measurement bottleneck]
  - **authors:** Guilin Zhang, Wulan Guo, Ziqi Tan, Hongyang He, Hailong Jiang
  - **institution:** George Washington University, University of Warwick, Kent State University, Youngstown State University
  - **link:** https://arxiv.org/pdf/2511.20922
  - **Simple LLM Summary:** The paper proposes a residual hybrid quantum-classical architecture that concatenates raw inputs with quantum features before classification, bypassing the quantum measurement bottleneck. This approach achieves up to 55% accuracy improvement over quantum baselines while maintaining low communication costs. The method demonstrates enhanced privacy robustness in both centralized and federated learning settings.

- **[arXiv251127] Automated Dynamic AI Inference Scaling on HPC-Infrastructure: Integrating Kubernetes, Slurm and vLLM**
  - **tags:** [mlsys], [llm inference], [vLLM, Kubernetes, Slurm, HPC, dynamic scaling]
  - **authors:** Tim Trappen, Robert Keßler, Roland Pabel, Viktor Achter, Stefan Wesner
  - **institution:** Ruhr University Bochum, University of Cologne
  - **link:** https://arxiv.org/pdf/2511.21413
  - **Simple LLM Summary:** This paper proposes a solution for serving LLMs by integrating vLLM, Slurm and Kubernetes on the RAMSES supercomputer to enable automated dynamic AI inference scaling on HPC infrastructure. The architecture efficiently handles concurrent requests with minimal latency overhead, demonstrating effective scaling for 100-1000 concurrent users while maintaining approximately 500ms end-to-end latency overhead.

- **[arXiv251127] MemFine: Memory-Aware Fine-Grained Scheduling for MoE Training**
  - **tags:** [mlsys], [llm training], [chunked recomputation, memory-aware scheduling, load balancing, dynamic token routing]
  - **authors:** Lu Zhao, Rong Shi, Shaoqing Zhang, Yueqiang Chen, Baoguo He, Hongfeng Sun, Ziqing Yin, Shangchao Su, Zhiyan Cui, Liang Dong, Xiyuan Li, Lingbin Wang, Jianwei He, Jiesong Ma, Weikang Huang, Jianglei Tong, Dongdong Gao, Jian Zhang, Hong Tian
  - **institution:** AIH Training Team
  - **link:** https://arxiv.org/pdf/2511.21431
  - **Simple LLM Summary:** MemFine introduces a memory-aware fine-grained scheduling framework that decomposes token distribution and expert computation into chunks with optimized recomputation. This approach reduces activation memory by 48.03% and improves throughput by 4.42% compared to full recomputation baselines. The method enables stable large-scale MoE training on memory-constrained GPUs without compromising model accuracy.

- **[arXiv251127] Assessing Redundancy Strategies to Improve Availability in Virtualized System Architectures**
  - **tags:** [sys], [cloud computing], [Stochastic Petri Nets, redundancy strategies, virtual machine redundancy, host-level redundancy, availability analysis]
  - **authors:** Alison Silva, Gustavo Callou
  - **institution:** Universidade de Pernambuco, Universidade Federal Rural de Pernambuco
  - **link:** https://arxiv.org/pdf/2511.20780
  - **Simple LLM Summary:** This paper presents a methodology using Stochastic Petri Nets to model and evaluate redundancy strategies for improving availability in private cloud file servers. The study analyzed four architectural configurations including baseline, host-level redundancy, VM redundancy, and combined redundancy. Results showed that implementing redundancy at both host and VM levels significantly improves system availability and reduces expected downtime.

- **[arXiv251127] Aragog: Just-in-Time Model Routing for Scalable Serving of Agentic Workflows**
  - **tags:** [mlsys], [llm inference], [model routing, configuration selection, just-in-time scheduling, agentic workflows, dynamic adaptation]
  - **authors:** Yinwei Dai, Zhuofu Chen, Anand Iyer, Ravi Netravali
  - **institution:** Princeton University, Georgia Institute of Technology
  - **link:** https://arxiv.org/pdf/2511.20975
  - **Simple LLM Summary:** Aragog introduces a just-in-time model routing system that dynamically adapts LLM configurations during workflow execution to match runtime system conditions. It uses a two-stage approach with offline routing to identify accuracy-preserving configurations and online scheduling for real-time selection. This approach significantly improves throughput (50-217%) and reduces latency (32.5-78.9%) while maintaining accuracy comparable to expensive configurations.

- **[arXiv251127] Modeling the Effect of Data Redundancy on Speedup in MLFMA Near-Field Computation**
  - **tags:** [sys], [GPU computing], [data redundancy, memory locality optimization, MLFMA, CUDA, cache behavior, performance modeling]
  - **authors:** Morteza Sadeghi
  - **institution:** University of Tehran
  - **link:** https://arxiv.org/pdf/2511.21535
  - **Simple LLM Summary:** This paper introduces data redundancy to improve spatial locality in MLFMA near-field computation on GPUs by reducing memory access dispersion. The approach achieves up to 7× kernel speedup through improved cache behavior, though end-to-end application speedup is limited to 1.04× due to data restructuring overheads. The work demonstrates that data redundancy can enhance GPU performance when locality gains outweigh data movement costs.

- **[arXiv251127] Handling of Memory Page Faults during Virtual-Address RDMA**
  - **tags:** [sys], [high-performance computing], [RDMA, page fault handling, DMA engine, SMMU, hardware-software co-design, memory pinning]
  - **authors:** Antonis Psistakis
  - **institution:** University of Crete
  - **link:** https://arxiv.org/pdf/2511.21018
  - **Simple LLM Summary:** This paper implements a hardware-software mechanism for handling memory page faults during virtual-address RDMA using ARM System Memory Management Unit detection and DMA engine integration. The solution enables fault tolerance without requiring memory pinning, overcoming limitations of traditional RDMA approaches. Evaluation shows advantages over alternative methods like pinning and pre-faulting in terms of programming complexity and memory utilization.

- **[arXiv251127] Accelerating Sparse Convolutions in Voxel-Based Point Cloud Networks**
  - **tags:** [mlsys], [GPU kernels], [sparse convolution, voxel indexing, kernel map, one-shot search, packed-native processing, dual-dataflow execution]
  - **authors:** Dionysios Adamopoulos, Anastasia Poulopoulou, Georgios Goumas, Christina Giannoula
  - **institution:** Max Planck Institute for Software Systems, National Technical University of Athens
  - **link:** https://arxiv.org/pdf/2511.20834
  - **Simple LLM Summary:** This paper introduces Spira, a GPU-optimized sparse convolution engine for voxel-based point cloud networks that exploits voxel coordinate properties through a one-shot search algorithm, packed-native processing, and dual-dataflow execution. Spira eliminates preprocessing overheads and builds kernel maps for all network layers concurrently. Evaluation shows Spira outperforms prior sparse convolution engines by 1.71-2.31× for end-to-end inference and 2.13-3.32× for layer-wise execution.

- **[arXiv251127] A Dynamic PD-Disaggregation Architecture for Maximizing Goodput in LLM Inference Serving**
  - **tags:** [mlsys], [llm inference], [prefill-decoding disaggregation, dynamic resource allocation, request scheduling, KV caching, load monitoring]
  - **authors:** Junhan Liao, Minxian Xu, Wanyi Zheng, Yan Wang, Kejiang Ye, Rajkumar Buyya, Chengzhong Xu
  - **institution:** Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Southern University of Science and Technology, Inner Mongolia University, University of Melbourne, University of Macau
  - **link:** https://arxiv.org/pdf/2511.20982
  - **Simple LLM Summary:** DOPD proposes a dynamic LLM inference system that adjusts prefill-to-decoding instance ratios based on real-time load monitoring to resolve resource imbalances. The system combines dynamic resource allocation with intelligent request scheduling to handle heterogeneous workloads under high concurrency. Experimental results show DOPD improves goodput by up to 1.5× while reducing latency metrics and achieving over 99% SLO attainment with fewer resources.

- **[arXiv251127] MAD-DAG: Protecting Blockchain Consensus from MEV**
  - **tags:** [sys], [blockchain consensus], [MAD-DAG, selfish mining, Markov Decision Process, security threshold, MEV protection]
  - **authors:** Roi Bar-Zur, Aviv Tamar, Ittay Eyal
  - **institution:** Technion
  - **link:** https://arxiv.org/pdf/2511.21552
  - **Simple LLM Summary:** The paper introduces MAD-DAG, a blockchain protocol that counters selfish mining under adverse conditions like MEV and rushing through a novel ledger function that discards contents of competing chains. Using Markov Decision Process modeling, the authors demonstrate MAD-DAG maintains security thresholds between 11-31% under conditions where existing protocols like Colordag and Bitcoin fail completely. This represents the first practical DAG-based protocol resilient to selfish mining with variable block rewards.

- **[arXiv251127] AI/ML Model Cards in Edge AI Cyberinfrastructure: towards Agentic AI**
  - **tags:** [mlsys], [cluster infrastructure], [Model Context Protocol, REST interface, dynamic model cards, Patra Model Cards]
  - **authors:** Beth Plale, Neelesh Karthikeyan, Isuru Gamage, Joe Stubbs, Sachith Withana
  - **institution:** University of Oregon, Indiana University, University of Texas
  - **link:** https://arxiv.org/pdf/2511.21661
  - **Simple LLM Summary:** This paper studies dynamic AI/ML model cards using Patra Model Cards embedded in the ICICLE AI Institute ecosystem. It evaluates the Model Context Protocol (MCP) as an interface to the model card server, finding MCP introduces some overhead compared to REST but enables active sessions. The research concludes that MCP's qualitative benefits for dynamic model card management may outweigh its quantitative performance costs.

- **[arXiv251127] Diagonal Scaling: A Multi-Dimensional Resource Model and Optimization Framework for Distributed Databases**
  - **tags:** [sys], [distributed databases], [diagonal scaling, scaling plane, local-search algorithm, horizontal scaling, vertical scaling, multi-dimensional resource model]
  - **authors:** Shahir Abdullah, Syed Rohit Zaman
  - **institution:** Bangladesh University of Engineering and Technology
  - **link:** https://arxiv.org/pdf/2511.21612
  - **Simple LLM Summary:** This paper introduces a two-dimensional Scaling Plane model and DIAGONALSCALE algorithm that jointly optimizes horizontal and vertical scaling in distributed databases. The proposed diagonal scaling approach reduces p95 latency by up to 40%, lowers cost-per-query by up to 37%, and significantly reduces rebalancing compared to traditional scaling methods. The results demonstrate that coordinated multi-dimensional scaling provides superior performance and efficiency for cloud database systems.

- **[arXiv251127] DSD: A Distributed Speculative Decoding Solution for Edge-Cloud Agile Large Model Serving**
  - **tags:** [mlsys], [llm inference], [distributed speculative decoding, adaptive window control, discrete-event simulation]
  - **authors:** Fengze Yu, Leshu Li, Brad McDanel, Saiqian Zhang
  - **institution:** New York University, University of Minnesota Twin Cities, Franklin & Marshall College
  - **link:** https://arxiv.org/pdf/2511.21669
  - **Simple LLM Summary:** The paper proposes DSD, a distributed speculative decoding framework that extends speculative decoding to multi-device deployments through coordinated draft-target execution. It introduces DSD-Sim for simulation and an Adaptive Window Control policy to dynamically optimize throughput. Experiments show DSD achieves up to 1.1× speedup and 9.7% higher throughput over existing baselines, enabling scalable LLM serving across edge-cloud environments.


**cs.AI/cs.LG contains "reinforcement learning" total: 18**
- [arXiv251127] Monet: Reasoning in Latent Visual Space Beyond Images and Language [link](https://arxiv.org/pdf/2511.21395)
- [arXiv251127] Subgoal Graph-Augmented Planning for LLM-Guided Open-World Reinforcement Learning [link](https://arxiv.org/pdf/2511.20993)
- [arXiv251127] Hybrid-AIRL: Enhancing Inverse Reinforcement Learning with Supervised Expert Guidance [link](https://arxiv.org/pdf/2511.21356)
- [arXiv251127] Independent policy gradient-based reinforcement learning for economic and reliable energy management of multi-microgrid systems [link](https://arxiv.org/pdf/2511.20977)
- [arXiv251127] SPHINX: A Synthetic Environment for Visual Perception and Reasoning [link](https://arxiv.org/pdf/2511.20814)
- [arXiv251127] Staggered Environment Resets Improve Massively Parallel On-Policy Reinforcement Learning [link](https://arxiv.org/pdf/2511.21011)
- [arXiv251127] Aligning LLMs with Biomedical Knowledge using Balanced Fine-Tuning [link](https://arxiv.org/pdf/2511.21075)
- [arXiv251127] ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for Efficient Reinforcement Learning [link](https://arxiv.org/pdf/2511.21005)
- [arXiv251127] Breaking the Safety-Capability Tradeoff: Reinforcement Learning with Verifiable Rewards Maintains Safety Guardrails in LLMs [link](https://arxiv.org/pdf/2511.21050)
- [arXiv251127] SocialNav: Training Human-Inspired Foundation Model for Socially-Aware Embodied Navigation [link](https://arxiv.org/pdf/2511.21135)
- [arXiv251127] Predictive Safety Shield for Dyna-Q Reinforcement Learning [link](https://arxiv.org/pdf/2511.21531)
- [arXiv251127] Maglev-Pentabot: Magnetic Levitation System for Non-Contact Manipulation using Deep Reinforcement Learning [link](https://arxiv.org/pdf/2511.21149)
- [arXiv251127] Exploring Time-Step Size in Reinforcement Learning for Sepsis Treatment [link](https://arxiv.org/pdf/2511.20913)
- [arXiv251127] BAMAS: Structuring Budget-Aware Multi-Agent Systems [link](https://arxiv.org/pdf/2511.21572)
- [arXiv251127] Escaping the Verifier: Learning to Reason via Demonstrations [link](https://arxiv.org/pdf/2511.21667)
- [arXiv251127] ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration [link](https://arxiv.org/pdf/2511.21689)
- [arXiv251127] Cryptocurrency Portfolio Management with Reinforcement Learning: Soft Actor--Critic and Deep Deterministic Policy Gradient Algorithms [link](https://arxiv.org/pdf/2511.20678)
- [arXiv251127] Morality in AI. A plea to embed morality in LLM architectures and frameworks [link](https://arxiv.org/pdf/2511.20689)

**cs.AI/cs.LG contains "accelerate" total: 15**
- [arXiv251127] SONAR: Spectral-Contrastive Audio Residuals for Generalizable Deepfake Detection [link](https://arxiv.org/pdf/2511.21325)
- [arXiv251127] Data-Driven Methods and AI in Engineering Design: A Systematic Literature Review Focusing on Challenges and Opportunities [link](https://arxiv.org/pdf/2511.20730)
- [arXiv251127] Conversational no-code and multi-agentic disease module identification and drug repurposing prediction with ChatDRex [link](https://arxiv.org/pdf/2511.21438)
- [arXiv251127] Staggered Environment Resets Improve Massively Parallel On-Policy Reinforcement Learning [link](https://arxiv.org/pdf/2511.21011)
- [arXiv251127] Aligning LLMs with Biomedical Knowledge using Balanced Fine-Tuning [link](https://arxiv.org/pdf/2511.21075)
- [arXiv251127] DeeAD: Dynamic Early Exit of Vision-Language Action for Efficient Autonomous Driving [link](https://arxiv.org/pdf/2511.20720)
- [arXiv251127] IntAttention: A Fully Integer Attention Pipeline for Efficient Edge Inference [link](https://arxiv.org/pdf/2511.21513)
- [arXiv251127] RISC-V Based TinyML Accelerator for Depthwise Separable Convolutions in Edge AI [link](https://arxiv.org/pdf/2511.21232)
- [arXiv251127] DUALGUAGE: Automated Joint Security-Functionality Benchmarking for Secure Code Generation [link](https://arxiv.org/pdf/2511.20709)
- [arXiv251127] A Brief History of Digital Twin Technology [link](https://arxiv.org/pdf/2511.20695)
- [arXiv251127] Dual-Domain Deep Learning Method to Accelerate Local Basis Functions Computation for Reservoir Simulation in High-Contrast Porous Media [link](https://arxiv.org/pdf/2511.20685)
- [arXiv251127] Sawtooth Sampling for Time Series Denoising Diffusion Implicit Models [link](https://arxiv.org/pdf/2511.21320)
- [arXiv251127] Beyond URLs: Metadata Diversity and Position for Efficient LLM Pretraining [link](https://arxiv.org/pdf/2511.21613)
- [arXiv251127] AI4X Roadmap: Artificial Intelligence for the advancement of scientific pursuit and its future directions [link](https://arxiv.org/pdf/2511.20976)
- [arXiv251127] Lattice-to-total thermal conductivity ratio: a phonon-glass electron-crystal descriptor for data-driven thermoelectric design [link](https://arxiv.org/pdf/2511.21213)
