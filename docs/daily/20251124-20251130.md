# 20251124-20251130

## 2025-11-24

**cs.DC total: 4**

- **[arXiv251124] Modeling Anomaly Detection in Cloud Services: Analysis of the Properties that Impact Latency and Resource Consumption**
  - **tags:** [sys], [cloud computing], [Stochastic Reward Nets, anomaly detection, precision, recall, inspection frequency]
  - **authors:** Gabriel Job Antunes Grabher, Fumio Machida, Thomas Ropars
  - **institution:** Universit√© Grenoble-Alpes, University of Tsukuba
  - **link:** https://arxiv.org/pdf/2511.17119
  - **Simple LLM Summary:** This paper uses Stochastic Reward Nets to model cloud services with performance anomaly detection. The study analyzes how detector characteristics like precision, recall, and inspection frequency affect latency and resource consumption. Results show that high precision is sufficient when detection runs frequently, while recall becomes more important with infrequent detection.

- **[arXiv251124] MicroMoE: Fine-Grained Load Balancing for Mixture-of-Experts with Token Scheduling**
  - **tags:** [mlsys], [llm training], [token scheduling, expert parallelism, micro-batch optimization]
  - **authors:** Chenqi Zhao, Wenfei Wu, Linhai Song, Yuchen Xu
  - **institution:** Peking University, Institute of Computing Technology, Chinese Academy of Sciences
  - **link:** https://arxiv.org/pdf/2511.16947
  - **Simple LLM Summary:** The paper proposes MicroEP, a parallelization strategy that achieves fine-grained load balancing in Mixture-of-Experts systems through token scheduling across GPUs. It introduces MicroMoE, a distributed training system implementing this approach, which improves training throughput by up to 47.6% compared to state-of-the-art systems while maintaining optimal load balance.

- **[arXiv251124] Optimizing PyTorch Inference with LLM-Based Multi-Agent Systems**
  - **tags:** [mlsys], [GPU kernels], [multi-agent systems, PyTorch optimization, LLM-based tuning, explore-exploit tradeoff, error-fixing agents]
  - **authors:** Kirill Nagaitsev, Luka Grbcic, Samuel Williams, Costin Iancu
  - **institution:** Northwestern University, Lawrence Berkeley National Laboratory
  - **link:** https://arxiv.org/pdf/2511.16964
  - **Simple LLM Summary:** This paper presents a multi-agent system using LLMs to optimize PyTorch inference performance on GPUs. The research shows that exploit-heavy strategies combined with error-fixing agents achieve the best results, with the optimal implementation delivering an average 2.88x speedup on H100 GPUs across diverse machine learning tasks.

- **[arXiv251124] Training Foundation Models on a Full-Stack AMD Platform: Compute, Networking, and System Design**
  - **tags:** [mlsys], [llm training], [mixture-of-experts, transformer sizing, context parallelism, compressed convolutional attention, fault-tolerance, checkpoint-reshaping, microbenchmarks, Pollara interconnect]
  - **authors:** Quentin Anthony, Yury Tokpanov, Skyler Szot, Srivatsan Rajagopal, Praneeth Medepalli, Rishi Iyer, Vasu Shyam, Anna Golubeva, Ansh Chaurasia, Xiao Yang, Tomas Figliolia, Robert Washbourne, Drew Thorstensen, Amartey Pearson, Zack Grossbart, Jason van Patten, Emad Barsoum, Zhenyu Gu, Yao Fu, Beren Millidge
  - **institution:** Zyphra, IBM, AMD
  - **link:** https://arxiv.org/pdf/2511.17127
  - **Simple LLM Summary:** This paper presents the first large-scale mixture-of-experts pretraining study on pure AMD hardware using MI300X GPUs and Pollara interconnect. The authors developed MI300X-aware transformer sizing rules and introduced ZAYA1-base model with novel architectural components like compressed convolutional attention. The results demonstrate that AMD's hardware, networking, and software stack are mature enough for competitive large-scale pretraining, with ZAYA1-base achieving performance comparable to leading base models.


**cs.AI/cs.LG contains "reinforcement learning" total: 11**
- [arXiv251124] CroTad: A Contrastive Reinforcement Learning Framework for Online Trajectory Anomaly Detection [link](https://arxiv.org/pdf/2511.16929)
- [arXiv251124] Multi-Agent Pointer Transformer: Seq-to-Seq Reinforcement Learning for Multi-Vehicle Dynamic Pickup-Delivery Problems [link](https://arxiv.org/pdf/2511.17435)
- [arXiv251124] Convergence and stability of Q-learning in Hierarchical Reinforcement Learning [link](https://arxiv.org/pdf/2511.17351)
- [arXiv251124] Dissecting Quantum Reinforcement Learning: A Systematic Evaluation of Key Components [link](https://arxiv.org/pdf/2511.17112)
- [arXiv251124] Hybrid Differential Reward: Combining Temporal Difference and Action Gradients for Efficient Multi-Agent Reinforcement Learning in Cooperative Driving [link](https://arxiv.org/pdf/2511.16916)
- [arXiv251124] R2PS: Worst-Case Robust Real-Time Pursuit Strategies under Partial Observability [link](https://arxiv.org/pdf/2511.17367)
- [arXiv251124] Predicting Talent Breakout Rate using Twitter and TV data [link](https://arxiv.org/pdf/2511.16905)
- [arXiv251124] MIR: Efficient Exploration in Episodic Multi-Agent Reinforcement Learning via Mutual Intrinsic Reward [link](https://arxiv.org/pdf/2511.17165)
- [arXiv251124] FireScope: Wildfire Risk Prediction with a Chain-of-Thought Oracle [link](https://arxiv.org/pdf/2511.17171)
- [arXiv251124] Harnessing Data from Clustered LQR Systems: Personalized and Collaborative Policy Optimization [link](https://arxiv.org/pdf/2511.17489)
- [arXiv251124] Intrinsic preservation of plasticity in continual quantum learning [link](https://arxiv.org/pdf/2511.17228)

**cs.AI/cs.LG contains "accelerate" total: 8**
- [arXiv251124] Layer-wise Weight Selection for Power-Efficient Neural Network Acceleration [link](https://arxiv.org/pdf/2511.17123)
- [arXiv251124] BITS for GAPS: Bayesian Information-Theoretic Sampling for hierarchical GAussian Process Surrogates [link](https://arxiv.org/pdf/2511.16815)
- [arXiv251124] Stable Coresets via Posterior Sampling: Aligning Induced and Full Loss Landscapes [link](https://arxiv.org/pdf/2511.17399)
- [arXiv251124] Mesh RAG: Retrieval Augmentation for Autoregressive Mesh Generation [link](https://arxiv.org/pdf/2511.16807)
- [arXiv251124] Towards Hyper-Efficient RAG Systems in VecDBs: Distributed Parallel Multi-Resolution Vector Search [link](https://arxiv.org/pdf/2511.16681)
- [arXiv251124] Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation [link](https://arxiv.org/pdf/2511.16757)
- [arXiv251124] Generating transition states of chemical reactions via distance-geometry-based flow matching [link](https://arxiv.org/pdf/2511.17229)
- [arXiv251124] Energy Scaling Laws for Diffusion Models: Quantifying Compute and Carbon Emissions in Image Generation [link](https://arxiv.org/pdf/2511.17031)
