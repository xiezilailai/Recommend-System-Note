# 20251208-20251214

## 2025-12-08

**cs.DC total: 7**

- **[arXiv251208] Metronome: Differentiated Delay Scheduling for Serverless Functions**
  - **tags:** [mlsys], [cluster infrastructure], [delay scheduling, online random forest regression, differentiated scheduling, locality-aware scheduling, SLA compliance]
  - **authors:** Zhuangbin Chen, Juzheng Zheng, Zibin Zheng
  - **institution:** Sun Yat-sen University
  - **link:** https://arxiv.org/pdf/2512.05703
  - **Simple LLM Summary:** This paper proposes Metronome, a differentiated delay scheduling framework for serverless functions that uses an online Random Forest Regression model to predict function execution times and identify optimal locality-aware nodes. The system significantly reduces mean execution time by 64.88%-95.83% compared to baselines while maintaining SLA compliance under increased concurrency.

- **[arXiv251208] FedGMR: Federated Learning with Gradual Model Restoration under Asynchrony and Model Heterogeneity**
  - **tags:** [mlsys], [others], [federated learning, model heterogeneity, gradual model restoration, mask-aware aggregation, asynchronous training, bandwidth-constrained clients]
  - **authors:** Chengjie Ma, Seungeun Oh, Jihong Park, Seong-Lyun Kim
  - **institution:** Yonsei University
  - **link:** https://arxiv.org/pdf/2512.05372
  - **Simple LLM Summary:** The paper proposes FedGMR, a federated learning method that gradually increases the density of sub-models for bandwidth-constrained clients during training to maintain their effectiveness. It introduces a mask-aware aggregation rule for asynchronous, model-heterogeneous settings and provides convergence guarantees. Experiments show FedGMR achieves faster convergence and higher accuracy, especially under high heterogeneity and non-IID data.

- **[arXiv251208] NVLang: Unified Static Typing for Actor-Based Concurrency on the BEAM**
  - **tags:** [sys], [programming languages], [algebraic data types, Hindley-Milner type inference, typed process identifiers, typed futures, Core Erlang]
  - **authors:** Miguel de Oliveira Guerreiro
  - **institution:** University of Lisbon
  - **link:** https://arxiv.org/pdf/2512.05224
  - **Simple LLM Summary:** NVLang is a statically typed functional language that uses algebraic data types to encode actor message protocols and extends Hindley-Milner type inference to enforce them at compile time. It introduces typed process identifiers and futures to provide type safety for message passing on the BEAM virtual machine. The paper concludes that this approach eliminates a class of runtime errors while preserving interoperability with the Erlang ecosystem.

- **[arXiv251208] InvarDiff: Cross-Scale Invariance Caching for Accelerated Diffusion Models**
  - **tags:** [mlsys], [diffusion inference], [invariance caching, deterministic sampling, quantile-based change metrics, re-sampling correction, binary cache plan matrix]
  - **authors:** Zihao Wu
  - **institution:** Peking University
  - **link:** https://arxiv.org/pdf/2512.05134
  - **Simple LLM Summary:** InvarDiff is a training-free acceleration method for diffusion models that exploits feature invariance across timesteps and layers to cache and reuse computations, guided by a precomputed binary cache plan. It achieves 2–3× end-to-end speed-ups on models like DiT and FLUX with minimal impact on fidelity or visual quality.

- **[arXiv251208] Model Gateway: Model Management Platform for Model-Driven Drug Discovery**
  - **tags:** [mlsys], [cluster infrastructure], [MLOps, model registry, LLM Agents, Generative AI, dynamic consensus model, asynchronous execution, cloud computing]
  - **authors:** Yan-Shiun Wu, Nathan A. Morin
  - **institution:** Eli Lilly and Company
  - **link:** https://arxiv.org/pdf/2512.05462
  - **Simple LLM Summary:** The paper presents Model Gateway, a cloud-based platform for managing machine learning and scientific computational models in drug discovery pipelines. It integrates MLOps practices, LLM Agents, and Generative AI tools to handle tasks like model registration, asynchronous execution, and dynamic consensus building. The platform demonstrated scalability with a 0% failure rate under high load and is concluded to be a fundamental component for accelerating model-driven drug discovery.

- **[arXiv251208] Are Bus-Mounted Edge Servers Feasible?**
  - **tags:** [sys], [edge computing, vehicular networks], [bus-mounted edge servers, server placement, greedy heuristic algorithm, trace-driven simulation, coverage optimization]
  - **authors:** Xuezhi Li, Jiancong He, Ming Xie, Xuyang Chen, Le Chang, Li Jiang, Gui Gui
  - **institution:** Guangdong University of Technology, Central South University
  - **link:** https://arxiv.org/pdf/2512.05543
  - **Simple LLM Summary:** This paper investigates the feasibility of deploying edge servers on public buses to serve vehicular networks, using real-world datasets from Shanghai. It proposes a mathematical model and a greedy heuristic algorithm to select a limited number of buses to maximize coverage of demand points under capacity and budget constraints. The trace-driven simulation results demonstrate that bus-mounted edge servers are a feasible, beneficial, and valuable approach for handling dynamic user demand in urban areas.

- **[arXiv251208] Compiler-supported reduced precision and AoS-SoA transformations for heterogeneous hardware**
  - **tags:** [sys], [high-performance computing, GPU optimization], [AoS-to-SoA transformation, reduced precision, compiler annotations, GPU offloading, unified memory]
  - **authors:** Pawel K. Radtke, Tobias Weinzierl
  - **institution:** Durham University
  - **link:** https://arxiv.org/pdf/2512.05516
  - **Simple LLM Summary:** This paper introduces compiler annotations to support AoS-to-SoA data layout transformations and reduced precision for particle simulation codes on heterogeneous GPU hardware. It evaluates the performance of these optimizations across different GPU platforms, finding significant speedups on NVIDIA G200 and more robust performance on AMD MI300A. The authors conclude that their compiler-based techniques are broadly applicable to Lagrangian codes and similar workloads.


**cs.AI/cs.LG contains "reinforcement learning" total: 9**
- [arXiv251208] Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning [link](https://arxiv.org/pdf/2512.05591)
- [arXiv251208] Hierarchical Reinforcement Learning for the Dynamic VNE with Alternatives Problem [link](https://arxiv.org/pdf/2512.05207)
- [arXiv251208] Whatever Remains Must Be True: Filtering Drives Reasoning in LLMs, Shaping Diversity [link](https://arxiv.org/pdf/2512.05962)
- [arXiv251208] Variational Quantum Rainbow Deep Q-Network for Optimizing Resource Allocation Problem [link](https://arxiv.org/pdf/2512.05946)
- [arXiv251208] Semore: VLM-guided Enhanced Semantic Motion Representations for Visual Reinforcement Learning [link](https://arxiv.org/pdf/2512.05172)
- [arXiv251208] Bayesian Active Inference for Intelligent UAV Anti-Jamming and Adaptive Trajectory Planning [link](https://arxiv.org/pdf/2512.05711)
- [arXiv251208] Bridging Interpretability and Optimization: Provably Attribution-Weighted Actor-Critic in Reproducing Kernel Hilbert Spaces [link](https://arxiv.org/pdf/2512.05291)
- [arXiv251208] A Fast Anti-Jamming Cognitive Radar Deployment Algorithm Based on Reinforcement Learning [link](https://arxiv.org/pdf/2512.05753)
- [arXiv251208] Enhancing Deep Deterministic Policy Gradients on Continuous Control Tasks with Decoupled Prioritized Experience Replay [link](https://arxiv.org/pdf/2512.05320)

**cs.AI/cs.LG contains "accelerate" total: 9**
- [arXiv251208] Documenting SME Processes with Conversational AI: From Tacit Knowledge to BPMN [link](https://arxiv.org/pdf/2512.05122)
- [arXiv251208] When Forgetting Builds Reliability: LLM Unlearning for Reliable Hardware Code Generation [link](https://arxiv.org/pdf/2512.05341)
- [arXiv251208] Coefficient of Variation Masking: A Volatility-Aware Strategy for EHR Foundation Models [link](https://arxiv.org/pdf/2512.05216)
- [arXiv251208] Trusted AI Agents in the Cloud [link](https://arxiv.org/pdf/2512.05951)
- [arXiv251208] Bootstrapping Fuzzers for Compilers of Low-Resource Language Dialects Using Language Models [link](https://arxiv.org/pdf/2512.05887)
- [arXiv251208] KQ-SVD: Compressing the KV Cache with Provable Guarantees on Attention Fidelity [link](https://arxiv.org/pdf/2512.05916)
- [arXiv251208] To Err Is Human: Systematic Quantification of Errors in Published AI Papers via LLM Analysis [link](https://arxiv.org/pdf/2512.05925)
- [arXiv251208] AI & Human Co-Improvement for Safer Co-Superintelligence [link](https://arxiv.org/pdf/2512.05356)
- [arXiv251208] UniFS: Unified Multi-Contrast MRI Reconstruction via Frequency-Spatial Fusion [link](https://arxiv.org/pdf/2512.05481)

## 2025-12-09

**cs.DC total: 17**

- **[arXiv251209] A-3PO: Accelerating Asynchronous LLM Training with Staleness-aware Proximal Policy Approximation**
  - **tags:** [mlsys], [llm training], [proximal policy optimization, decoupled loss, asynchronous RL, policy approximation, interpolation]
  - **authors:** Xiaocan Li, Shiliang Wu, Zheng Shen
  - **institution:** Huawei Canada
  - **link:** https://arxiv.org/pdf/2512.06547
  - **Simple LLM Summary:** This paper introduces A-3PO, a method that accelerates asynchronous LLM training by approximating the proximal policy in decoupled loss algorithms through simple interpolation, eliminating the need for an extra forward pass. This reduces training time by 18% while maintaining performance comparable to methods that compute the proximal policy explicitly.

- **[arXiv251209] Cloud Revolution: Tracing the Origins and Rise of Cloud Computing**
  - **tags:** [sys], [cloud computing], [virtualization, distributed systems, high-speed networking, hyperscale data centers, edge computing, quantum computing services, pay-per-use model]
  - **authors:** Deepa Gurung, S M Zia Ur Rashid, Zain ul Abdeen, Suman Rath
  - **institution:** Joongbu University, The University of Tulsa, Virginia Tech
  - **link:** https://arxiv.org/pdf/2512.06800
  - **Simple LLM Summary:** This paper reexamines the historical evolution of cloud computing, analyzing the technological and economic forces behind its rise. It concludes that cloud computing is a rapidly changing paradigm whose future depends on balancing scalability, openness, and trust, while also highlighting current limitations like security and vendor lock-in.

- **[arXiv251209] PIR-DSN: A Decentralized Storage Network Supporting Private Information Retrieval**
  - **tags:** [sys], [decentralized storage, privacy], [Private Information Retrieval (PIR), secure mapping, Byzantine-robust retrieval, file replication]
  - **authors:** Jiahao Zhang, Minghui Xu, Hechuan Guo, Xiuzhen Cheng
  - **institution:** Shandong University
  - **link:** https://arxiv.org/pdf/2512.07189
  - **Simple LLM Summary:** The paper introduces PIR-DSN, a protocol that integrates Private Information Retrieval (PIR) into Decentralized Storage Networks to protect user privacy during file retrieval. It uses a secure mapping method for efficient private queries and ensures robustness via file replication. The evaluation shows it achieves practical overhead and throughput, making it viable for privacy-sensitive applications.

- **[arXiv251209] Optimizing video analytics inference pipelines: a case study**
  - **tags:** [mlsys], [multi-modal inference], [GPU acceleration, parallel processing, vectorized clustering, memory-efficient post-processing]
  - **authors:** Saeid Ghafouri, Yuming Ding, Katerine Diaz Chito, Jesús Martinez del Rincón, Niamh O'Connell, Hans Vandierendonck
  - **institution:** Queen's University Belfast
  - **link:** https://arxiv.org/pdf/2512.07009
  - **Simple LLM Summary:** This paper presents a case study on optimizing a video analytics pipeline for poultry welfare monitoring through system-level improvements. The core methods include multi-level parallelization, GPU acceleration, and vectorized clustering, which together achieve up to a 2x speedup without loss of accuracy. The findings offer practical strategies for building cost-effective, high-throughput video inference systems in agricultural and other large-scale applications.

- **[arXiv251209] A Chunked-Object Pattern for Multi-Region Large Payload Storage in Managed NoSQL Databases**
  - **tags:** [sys], [distributed databases], [chunked-object pattern, multi-region replication, DynamoDB Global Tables, pointer pattern, active-active architecture]
  - **authors:** Manideep Reddy Chinthareddy
  - **institution:** Independent researcher (based on email domain)
  - **link:** https://arxiv.org/pdf/2512.06852
  - **Simple LLM Summary:** This paper proposes a "chunked-object" pattern for storing large payloads in managed NoSQL databases by splitting them into ordered chunks that fit within per-item size limits. This approach eliminates the replication lag and consistency hazards of the standard "pointer pattern" that uses separate object storage. The evaluation shows the pattern reduces cross-region consistency time for large payloads by keeping data and metadata within a single database consistency domain.

- **[arXiv251209] DCO: Dynamic Cache Orchestration for LLM Accelerators through Predictive Management**
  - **tags:** [mlsys], [llm inference], [dynamic cache orchestration, predictive management, dead-block prediction, cache bypassing, thrashing mitigation, shared system-level cache, analytical model]
  - **authors:** Zhongchun Zhou, Chengtao Lai, Yuhang Gu, Wei Zhang
  - **institution:** The Hong Kong University of Science and Technology, Southeast University
  - **link:** https://arxiv.org/pdf/2512.07312
  - **Simple LLM Summary:** This paper proposes DCO, a dynamic cache orchestration system for LLM accelerators that uses predictive management based on dataflow information to guide cache replacement and bypass decisions, mitigating thrashing. The design, implemented in RTL, achieves up to 1.80x speedup over conventional caches and demonstrates the potential of shared cache designs for future AI accelerators.

- **[arXiv251209] ContinuumConductor : Decentralized Process Mining on the Edge-Cloud Continuum**
  - **tags:** [sys], [distributed computing], [process mining, edge-cloud continuum, decentralized framework, decision criteria, trade-off analysis]
  - **authors:** Hendrik Reiter, Janick Edinger, Martin Kabierski, Agnes Koschmider, Olaf Landsiedel, Arvid Lepsien, Xixi Lu, Andrea Marrella, Estefania Serral, Stefan Schulte, Florian Tschorsch, Matthias Weidlich, Wilhelm Hasselbring
  - **institution:** Kiel University, University of Hamburg, University of Vienna, University of Bayreuth, Hamburg University of Technology, Utrecht University, Sapienza University of Rome, KU Leuven, Dresden University of Technology, Humboldt University of Berlin
  - **link:** https://arxiv.org/pdf/2512.07280
  - **Simple LLM Summary:** This paper introduces ContinuumConductor, a layered decision framework for decentralizing process mining tasks across the edge-cloud continuum. It analyzes the trade-offs between centralization and decentralization for steps like preprocessing and discovery, proposing criteria for resource-efficient and privacy-preserving analysis. The framework is demonstrated in a real-world use case for optimizing processes in inland ports.

- **[arXiv251209] Stable-MoE: Lyapunov-based Token Routing for Distributed Mixture-of-Experts Training over Edge Networks**
  - **tags:** [mlsys], [llm training], [Lyapunov optimization, token routing, mixture of experts, edge computing, stochastic optimization]
  - **authors:** Long Shi, Bingyan Ou, Kang Wei, Weihao Zhu, Zhe Wang, Zhiyong Chen
  - **institution:** Nanjing University of Science and Technology, Southeast University, Shanghai Jiao Tong University
  - **link:** https://arxiv.org/pdf/2512.06784
  - **Simple LLM Summary:** The paper proposes Stable-MoE, a Lyapunov optimization-based framework for online token routing and resource allocation in distributed Mixture-of-Experts training over heterogeneous edge networks. It aims to maximize throughput and gating consistency while ensuring queue stability. Experiments on SVHN and CIFAR-100 show it improves system throughput by at least 40% and test accuracy by 5% compared to baselines.

- **[arXiv251209] Vec-LUT: Vector Table Lookup for Parallel Ultra-Low-Bit LLM Inference on Edge Devices**
  - **tags:** [mlsys], [llm inference], [vector lookup table, ultra-low-bit quantization, parallel inference, memory bandwidth optimization, cache-aware streamed lookup]
  - **authors:** Xiangyu Li, Chengyu Yin, Weijun Wang, Jianyu Wei, Ting Cao, Yunxin Liu
  - **institution:** Tsinghua University, Beijing Jiaotong University, University of Science and Technology of China
  - **link:** https://arxiv.org/pdf/2512.06443
  - **Simple LLM Summary:** This paper proposes Vec-LUT, a new vector lookup table paradigm for parallel ultra-low-bit LLM inference on edge devices. It addresses the memory bandwidth underutilization of scalar LUTs by constructing a unified table across tokens and using a single 1→N lookup per index, supported by a vector LUT-centric tensor layout and cache-aware streamed lookup. Evaluations show Vec-LUT outperforms state-of-the-art baselines by up to 4.2x on various edge devices.

- **[arXiv251209] Venus: An Efficient Edge Memory-and-Retrieval System for VLM-based Online Video Understanding**
  - **tags:** [mlsys], [multi-modal inference], [edge-cloud disaggregated architecture, scene segmentation and clustering, hierarchical memory, threshold-based progressive sampling, keyframe retrieval]
  - **authors:** Shengyuan Ye, Bei Ouyang, Tianyi Qian, Liekang Zeng, Mu Yuan, Xiaowen Chu, Weijie Hong, Xu Chen
  - **institution:** Sun Yat-sen University, The Chinese University of Hong Kong, HKUST (Guangzhou), Shenzhen Smart City Communications Co., Ltd.
  - **link:** https://arxiv.org/pdf/2512.07344
  - **Simple LLM Summary:** The paper proposes Venus, an efficient edge memory-and-retrieval system for VLM-based online video understanding. It uses a two-stage edge-cloud architecture for keyframe ingestion and querying, employing scene segmentation, clustering, and a progressive sampling algorithm. The system achieves significant latency speedups while maintaining or improving reasoning accuracy.

- **[arXiv251209] Bandwidth-Aware Network Topology Optimization for Decentralized Learning**
  - **tags:** [mlsys], [cluster infrastructure], [ADMM, conjugate gradient, Mixed-Integer SDP, bandwidth allocation, network topology optimization]
  - **authors:** Yipeng Shen, Zehan Zhu, Yan Huang, Changzhi Yan, Cheng Zhuo, Jinming Xu
  - **institution:** Zhejiang University, KTH Royal Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.07536
  - **Simple LLM Summary:** The paper proposes a bandwidth-aware network topology optimization framework for decentralized learning, which uses an ADMM-based method to solve a reformulated Mixed-Integer SDP problem. The resulting optimized topologies maximize consensus speed under bandwidth constraints and reduce training time for decentralized learning tasks on real-world datasets.

- **[arXiv251209] Otus Supercomputer**
  - **tags:** [sys], [high-performance computing], [supercomputer, cluster, CPU nodes, GPU nodes, FPGA nodes, energy efficiency, Top500, Green500]
  - **authors:** Sadaf Ehtesabi, Manoar Hossain, Tobias Kenter, Andreas Krawinkel, Holger Nitsche, Lukas Ostermann, Christian Plessl, Heinrich Riebler, Stefan Rohde, Robert Schade, Michael Schwarz, Jens Simon, Nils Winnwa, Alex Wiens, Xin Wu
  - **institution:** Paderborn University, Paderborn Center for Parallel Computing (PC2)
  - **link:** https://arxiv.org/pdf/2512.07401
  - **Simple LLM Summary:** This paper provides a comprehensive overview of the Otus supercomputer, detailing its hardware, software, and system integration for energy-efficient operation. It describes the system's node types and its performance rankings on the Top500 and Green500 lists. The article serves as a reference for scientists and HPC center operators.

- **[arXiv251209] Designing Co-operation in Systems of Hierarchical, Multi-objective Schedulers for Stream Processing**
  - **tags:** [sys], [stream processing], [hierarchical schedulers, load balancing, constraint solver, tier-based architecture, multi-objective scheduling]
  - **authors:** Animesh Dangwal, Yufeng Jiang, Charlie Arnold, Jun Fan, Mohamed Bassem, Aish Rajagopal
  - **institution:** Meta Platforms
  - **link:** https://arxiv.org/pdf/2512.07792
  - **Simple LLM Summary:** The paper designs a system called SPTLB (StreamProcessing Tier Load Balancer) to automate load balancing across hierarchical tiers in Meta's stream processing infrastructure, using a constraint solver to coordinate multiple schedulers and optimize resource usage. It concludes that this holistic, multi-objective approach outperforms a baseline greedy scheduler by efficiently balancing load while respecting application properties like SLOs and network costs.

- **[arXiv251209] Quantifying the Carbon Reduction of DAG Workloads: A Job Shop Scheduling Perspective**
  - **tags:** [mlsys], [cluster infrastructure], [job shop scheduling, DAG workloads, carbon-aware scheduling, flexible job-shop, makespan, carbon intensity]
  - **authors:** Roozbeh Bostandoost, Adam Lechowicz, Walid A. Hanafy, Prashant Shenoy, Mohammad Hajiesmaili
  - **institution:** University of Massachusetts Amherst
  - **link:** https://arxiv.org/pdf/2512.07799
  - **Simple LLM Summary:** This paper models the problem of scheduling DAG-based batch workloads as a flexible job-shop scheduling variant to quantify the maximum potential carbon reduction. It finds that a dependency-aware scheduler can achieve up to 25% lower carbon emissions without increasing the optimal makespan, highlighting a tension between carbon, energy, and job completion time.

- **[arXiv251209] A Performance Analyzer for a Public Cloud's ML-Augmented VM Allocator**
  - **tags:** [mlsys], [cluster infrastructure], [bi-level optimization, VM placement, bin-packing, performance analysis, live migration]
  - **authors:** Roozbeh Bostandoost, Pooria Namyar, Siva Kesava Reddy Kakarla, Ryan Beckett, Santiago Segarra, Eli Cortez, Ankur Mallick, Kevin Hsieh, Rodrigo Fonseca, Mohammad Hajiesmaili, Behnaz Arzani
  - **institution:** University of Massachusetts Amherst, Microsoft Research, Rice University
  - **link:** https://arxiv.org/pdf/2512.07750
  - **Simple LLM Summary:** The paper introduces SANJESH, a tool that uses a novel bi-level optimization to analyze how multiple interacting ML models affect the end-to-end performance of cloud systems like VM allocators. It successfully solves complex optimizations that prior methods could not and identifies scenarios where ML-augmented systems can cause performance up to 4x worse than simulations predict.

- **[arXiv251209] Communication-Efficient Serving for Video Diffusion Models with Latent Parallelism**
  - **tags:** [mlsys], [diffusion inference], [latent parallelism, patch-aligned overlapping partition, position-aware latent reconstruction, communication-efficient serving, video diffusion models]
  - **authors:** Zhiyuan Wu, Shuai Wang, Li Chen, Kaihui Gao, Dan Li, Yanyu Ren, Qiming Zhang, Yong Wang
  - **institution:** Tsinghua University, Zhongguancun Laboratory, ZTE Corporation
  - **link:** https://arxiv.org/pdf/2512.07350
  - **Simple LLM Summary:** This paper proposes Latent Parallelism (LP), a novel parallelism strategy for serving Video Diffusion Models (VDMs) that reduces communication overhead by dynamically rotating partition dimensions across timesteps in the latent space. It introduces a patch-aligned overlapping partition and a position-aware reconstruction mechanism to maintain generation quality. Experiments show LP reduces communication by up to 97% compared to baseline methods while preserving video quality.

- **[arXiv251209] An Adaptive Multi-Layered Honeynet Architecture for Threat Behavior Analysis via Deep Learning**
  - **tags:** [mlsys], [cluster infrastructure], [honeynet, reinforcement learning, anomaly detection, dynamic container deployment, threat intelligence]
  - **authors:** Lukas Johannes Möller
  - **institution:** Georgia Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.07827
  - **Simple LLM Summary:** This paper proposes ADLAH, an adaptive honeynet architecture that uses a reinforcement learning agent to dynamically escalate suspicious sessions from low-interaction sensors to high-interaction honeypots. The core method involves AI-driven orchestration for cost-efficient capture of high-value adversary behavior and automated bot attack chain analysis. The main conclusion is that this blueprint provides a practical path toward generating actionable threat intelligence, though field-scale validation remains future work.


**cs.AI/cs.LG contains "reinforcement learning" total: 37**
- [arXiv251209] Know your Trajectory -- Trustworthy Reinforcement Learning deployment through Importance-Based Trajectory Analysis [link](https://arxiv.org/pdf/2512.06917)
- [arXiv251209] The Role of Entropy in Visual Grounding: Analysis and Optimization [link](https://arxiv.org/pdf/2512.06726)
- [arXiv251209] JaxWildfire: A GPU-Accelerated Wildfire Simulator for Reinforcement Learning [link](https://arxiv.org/pdf/2512.06102)
- [arXiv251209] RLAX: Large-Scale, Distributed Reinforcement Learning for Large Language Models on TPUs [link](https://arxiv.org/pdf/2512.06392)
- [arXiv251209] Parent-Guided Semantic Reward Model (PGSRM): Embedding-Based Reward Functions for Reinforcement Learning of Transformer Language Models [link](https://arxiv.org/pdf/2512.06920)
- [arXiv251209] LLM-Upgraded Graph Reinforcement Learning for Carbon-Aware Job Scheduling in Smart Manufacturing [link](https://arxiv.org/pdf/2512.06351)
- [arXiv251209] PrivORL: Differentially Private Synthetic Dataset for Offline Reinforcement Learning [link](https://arxiv.org/pdf/2512.07342)
- [arXiv251209] LLM-Driven Composite Neural Architecture Search for Multi-Source RL State Encoding [link](https://arxiv.org/pdf/2512.06982)
- [arXiv251209] Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning [link](https://arxiv.org/pdf/2512.06835)
- [arXiv251209] Energy-Efficient Navigation for Surface Vehicles in Vortical Flow Fields [link](https://arxiv.org/pdf/2512.06912)
- [arXiv251209] Reinforcement Learning Integrated Agentic RAG for Software Test Cases Authoring [link](https://arxiv.org/pdf/2512.06060)
- [arXiv251209] Towards Robust Protective Perturbation against DeepFake Face Swapping [link](https://arxiv.org/pdf/2512.07228)
- [arXiv251209] SINRL: Socially Integrated Navigation with Reinforcement Learning using Spiking Neural Networks [link](https://arxiv.org/pdf/2512.07266)
- [arXiv251209] Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning [link](https://arxiv.org/pdf/2512.06533)
- [arXiv251209] AI Application in Anti-Money Laundering for Sustainable and Transparent Financial Systems [link](https://arxiv.org/pdf/2512.06240)
- [arXiv251209] Learning Without Time-Based Embodiment Resets in Soft-Actor Critic [link](https://arxiv.org/pdf/2512.06252)
- [arXiv251209] JT-DA: Enhancing Data Analysis with Tool-Integrated Table Reasoning Large Language Models [link](https://arxiv.org/pdf/2512.06859)
- [arXiv251209] Learning When to Switch: Adaptive Policy Selection via Reinforcement Learning [link](https://arxiv.org/pdf/2512.06250)
- [arXiv251209] Auto-exploration for online reinforcement learning [link](https://arxiv.org/pdf/2512.06244)
- [arXiv251209] Why Goal-Conditioned Reinforcement Learning Works: Relation to Dual Control [link](https://arxiv.org/pdf/2512.06471)
- [arXiv251209] LightSearcher: Efficient DeepSearch via Experiential Memory [link](https://arxiv.org/pdf/2512.06653)
- [arXiv251209] Less is More: Non-uniform Road Segments are Efficient for Bus Arrival Prediction [link](https://arxiv.org/pdf/2512.07200)
- [arXiv251209] TrajMoE: Scene-Adaptive Trajectory Planning with Mixture of Experts and Reinforcement Learning [link](https://arxiv.org/pdf/2512.07135)
- [arXiv251209] Video Models Start to Solve Chess, Maze, Sudoku, Mental Rotation, and Raven' Matrices [link](https://arxiv.org/pdf/2512.05969)
- [arXiv251209] Average-reward reinforcement learning in semi-Markov decision processes via relative value iteration [link](https://arxiv.org/pdf/2512.06218)
- [arXiv251209] PrivLLMSwarm: Privacy-Preserving LLM-Driven UAV Swarms for Secure IoT Surveillance [link](https://arxiv.org/pdf/2512.06747)
- [arXiv251209] Adaptive Tuning of Parameterized Traffic Controllers via Multi-Agent Reinforcement Learning [link](https://arxiv.org/pdf/2512.07417)
- [arXiv251209] Revolutionizing Mixed Precision Quantization: Towards Training-free Automatic Proxy Discovery via Large Language Models [link](https://arxiv.org/pdf/2512.07419)
- [arXiv251209] KAN-Dreamer: Benchmarking Kolmogorov-Arnold Networks as Function Approximators in World Models [link](https://arxiv.org/pdf/2512.07437)
- [arXiv251209] How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and Failure Scenarios of Various LLMs in Agentic Simulations [link](https://arxiv.org/pdf/2512.07497)
- [arXiv251209] Model-Based Reinforcement Learning Under Confounding [link](https://arxiv.org/pdf/2512.07528)
- [arXiv251209] ReLaX: Reasoning with Latent Exploration for Large Reasoning Models [link](https://arxiv.org/pdf/2512.07558)
- [arXiv251209] Comparative Analysis and Parametric Tuning of PPO, GRPO, and DAPO for LLM Reasoning Enhancement [link](https://arxiv.org/pdf/2512.07611)
- [arXiv251209] The Agent Capability Problem: Predicting Solvability Through Information-Theoretic Bounds [link](https://arxiv.org/pdf/2512.07631)
- [arXiv251209] RL-MTJail: Reinforcement Learning for Automated Black-Box Multi-Turn Jailbreaking of Large Language Models [link](https://arxiv.org/pdf/2512.07761)
- [arXiv251209] Learning to Hedge Swaptions [link](https://arxiv.org/pdf/2512.06639)
- [arXiv251209] Statistical analysis of Inverse Entropy-regularized Reinforcement Learning [link](https://arxiv.org/pdf/2512.06956)

**cs.AI/cs.LG contains "accelerate" total: 25**
- [arXiv251209] GENIUS: An Agentic AI Framework for Autonomous Design and Execution of Simulation Protocols [link](https://arxiv.org/pdf/2512.06404)
- [arXiv251209] Flash Multi-Head Feed-Forward Network [link](https://arxiv.org/pdf/2512.06989)
- [arXiv251209] RLAX: Large-Scale, Distributed Reinforcement Learning for Large Language Models on TPUs [link](https://arxiv.org/pdf/2512.06392)
- [arXiv251209] FedDSR: Federated Deep Supervision and Regularization Towards Autonomous Driving [link](https://arxiv.org/pdf/2512.06676)
- [arXiv251209] Approximate Multiplier Induced Error Propagation in Deep Neural Networks [link](https://arxiv.org/pdf/2512.06537)
- [arXiv251209] Closed-Loop Robotic Manipulation of Transparent Substrates for Self-Driving Laboratories using Deep Learning Micro-Error Correction [link](https://arxiv.org/pdf/2512.06038)
- [arXiv251209] Learning Without Time-Based Embodiment Resets in Soft-Actor Critic [link](https://arxiv.org/pdf/2512.06252)
- [arXiv251209] On The Role of K-Space Acquisition in MRI Reconstruction Domain-Generalization [link](https://arxiv.org/pdf/2512.06530)
- [arXiv251209] BitStopper: An Efficient Transformer Attention Accelerator via Stage-fusion and Early Termination [link](https://arxiv.org/pdf/2512.06457)
- [arXiv251209] Improving the Throughput of Diffusion-based Large Language Models via a Training-Free Confidence-Aware Calibration [link](https://arxiv.org/pdf/2512.07173)
- [arXiv251209] DaGRPO: Rectifying Gradient Conflict in Reasoning via Distinctiveness-Aware Group Relative Policy Optimization [link](https://arxiv.org/pdf/2512.06337)
- [arXiv251209] Block Sparse Flash Attention [link](https://arxiv.org/pdf/2512.07011)
- [arXiv251209] A self-driving lab for solution-processed electrochromic thin films [link](https://arxiv.org/pdf/2512.05989)
- [arXiv251209] Leveraging KV Similarity for Online Structured Pruning in LLMs [link](https://arxiv.org/pdf/2512.07090)
- [arXiv251209] FOAM: Blocked State Folding for Memory-Efficient LLM Training [link](https://arxiv.org/pdf/2512.07112)
- [arXiv251209] Hankel-FNO: Fast Underwater Acoustic Charting Via Physics-Encoded Fourier Neural Operator [link](https://arxiv.org/pdf/2512.06417)
- [arXiv251209] Hardware Software Optimizations for Fast Model Recovery on Reconfigurable Architectures [link](https://arxiv.org/pdf/2512.06113)
- [arXiv251209] A Patient-Doctor-NLP-System to contest inequality for less privileged [link](https://arxiv.org/pdf/2512.06734)
- [arXiv251209] ESPADA: Execution Speedup via Semantics Aware Demonstration Data Downsampling for Imitation Learning [link](https://arxiv.org/pdf/2512.07371)
- [arXiv251209] KAN-Dreamer: Benchmarking Kolmogorov-Arnold Networks as Function Approximators in World Models [link](https://arxiv.org/pdf/2512.07437)
- [arXiv251209] Artificial Intelligence and Nuclear Weapons Proliferation: The Technological Arms Race for (In)visibility [link](https://arxiv.org/pdf/2512.07487)
- [arXiv251209] Efficient Low-Tubal-Rank Tensor Estimation via Alternating Preconditioned Gradient Descent [link](https://arxiv.org/pdf/2512.07490)
- [arXiv251209] Exploring Test-time Scaling via Prediction Merging on Large-Scale Recommendation [link](https://arxiv.org/pdf/2512.07650)
- [arXiv251209] A scalable and real-time neural decoder for topological quantum codes [link](https://arxiv.org/pdf/2512.07737)
- [arXiv251209] LUNA: LUT-Based Neural Architecture for Fast and Low-Cost Qubit Readout [link](https://arxiv.org/pdf/2512.07808)

## 2025-12-10

**cs.DC total: 14**

- **[arXiv251210] Basic Lock Algorithms in Lightweight Thread Environments**
  - **tags:** [sys], [concurrency], [TTAS lock, MCS lock, cohort lock, lightweight threads, mutex]
  - **authors:** Taras Skazhenik, Nikolai Korobenikov, Andrei Churbanov, Anton Malakhov, Vitaly Aksenov
  - **institution:** ITMO University, Huawei
  - **link:** https://arxiv.org/pdf/2512.08563
  - **Simple LLM Summary:** This paper modifies TTAS and MCS lock algorithms for use with lightweight threads, highlighting the importance of yield and sleep context switch mechanisms. It concludes that a cohort lock, which combines multiple MCS queues with a common TTAS, provides a balanced and library-agnostic solution for this environment.

- **[arXiv251210] Model-based Testing of Practical Distributed Systems in Actor Model**
  - **tags:** [sys], [distributed systems testing], [model-based testing, finite-state automaton, actor model, Viewstamped Replication]
  - **authors:** Ilya Kokorin, Evgeny Chernatskiy, Vitaly Aksenov
  - **institution:** ITMO University, VK.com
  - **link:** https://arxiv.org/pdf/2512.08698
  - **Simple LLM Summary:** This paper presents a model-based testing approach for distributed systems in the actor model, generating exhaustive test suites from finite-state automaton models without modifying the system code. The method was successfully applied to verify an implementation of the Viewstamped Replication algorithm, demonstrating its practical utility for real-world systems.

- **[arXiv251210] Magneton: Optimizing Energy Efficiency of ML Systems via Differential Energy Debugging**
  - **tags:** [mlsys], [llm inference], [differential energy debugging, energy profiler, operator-level comparison, software energy waste]
  - **authors:** Yi Pan, Wenbo Qian, Dedong Xie, Ruiyan Hu, Yigong Hu, Baris Kasikci
  - **institution:** University of Washington, Boston University, Shanghai Jiao Tong University
  - **link:** https://arxiv.org/pdf/2512.08365
  - **Simple LLM Summary:** The paper introduces Magneton, a tool that uses differential energy debugging to compare energy consumption between similar ML systems at the operator level, automatically identifying code and configurations causing energy waste. It successfully detected both known and previously unknown software inefficiencies across several popular ML frameworks and applications.

- **[arXiv251210] GPU Memory Prediction for Multimodal Model Training**
  - **tags:** [mlsys], [multi-modal training], [memory prediction, factorization, layer decomposition, profiling, formulation-based modeling]
  - **authors:** Jinwoo Jeong, Minchul Kang, Younghun Go, Changyong Shin, Hyunho Lee, Junho Yoon, Gyeongsik Yang, Chuck Yoo
  - **institution:** Korea University, KT Corporation
  - **link:** https://arxiv.org/pdf/2512.07853
  - **Simple LLM Summary:** The paper proposes a framework to predict peak GPU memory usage for training multimodal models by decomposing the model into layers and applying factorization to estimate each layer's memory consumption. It addresses the limitations of prior unimodal-focused methods. The evaluation shows the framework achieves high prediction accuracy with an average MAPE of ~8.7%.

- **[arXiv251210] Chopper: A Multi-Level GPU Characterization Tool & Derived Insights Into LLM Training Inefficiency**
  - **tags:** [mlsys], [llm training], [profiling framework, hardware performance counters, kernel traces, FSDP, DVFS, frequency overhead, MFMA utilization]
  - **authors:** Marco Kurzynski, Shaizeen Aga, Di Wu
  - **institution:** University of Central Florida, Advanced Micro Devices, Inc.
  - **link:** https://arxiv.org/pdf/2512.08242
  - **Simple LLM Summary:** This paper introduces Chopper, a multi-level GPU profiling tool for characterizing distributed LLM training. Using it to analyze Llama 3 8B training on AMD MI300X GPUs, the authors find that frequency overhead (DVFS effects) is the largest source of inefficiency, surpassing other factors like kernel launch overheads.

- **[arXiv251210] Synergizing Monetization, Orchestration, and Semantics in Computing Continuum**
  - **tags:** [sys], [distributed systems], [computing continuum, resource orchestration, semantic interoperability, distributed marketplace, edge computing, fog computing]
  - **authors:** Chinmaya Kumar Dehury, Lauri Lovén, Praveen Kumar Donta, Ilir Murturi, Schahram Dustdar
  - **institution:** IISER Berhampur, University of Oulu, Stockholm University, University of Prishtina, TU Wien
  - **link:** https://arxiv.org/pdf/2512.08288
  - **Simple LLM Summary:** The paper introduces the HERMES framework, which synergizes resource monetization, orchestration, and semantic interoperability to create a seamless and secure computing continuum from cloud to edge. It aims to overcome current limitations in scalability, interoperability, and trust for hyper-distributed applications. The main conclusion is that HERMES lays a foundation for more efficient, trustworthy, and autonomous distributed applications.

- **[arXiv251210] CapsuleFS A Multi-credential DataCapsule Filesystem**
  - **tags:** [sys], [edge computing filesystem], [DataCapsule, POSIX-compliant, FUSE, Trusted Execution Environment, Global Data Plane]
  - **authors:** Qingyang Hu, Yucheng Huang, Manshi Yang
  - **institution:** University of California, Berkeley
  - **link:** https://arxiv.org/pdf/2512.08067
  - **Simple LLM Summary:** CapsuleFS is a POSIX-compliant filesystem that integrates multi-credential functionality using DataCapsules as storage, built on the Global Data Plane for edge computing. Its architecture consists of a DataCapsule server, a middleware in a Trusted Execution Environment, and a FUSE-based client. While read/write performance is modest, the system achieves high functional correctness, making it a viable candidate for real-world software development.

- **[arXiv251210] A scalable high-order multigrid-FFT Poisson solver for unbounded domains on adaptive multiresolution grids**
  - **tags:** [sys], [computational physics], [multigrid solver, Fourier-based direct solver, high-order compact stencils, adaptive multiresolution grids, unbounded boundary conditions]
  - **authors:** Gilles Poncelet, Jonathan Lambrechts, Thomas Gillis, Philippe Chatelain
  - **institution:** Université catholique de Louvain, Massachusetts Institute of Technology, NVIDIA
  - **link:** https://arxiv.org/pdf/2512.08555
  - **Simple LLM Summary:** This paper presents a scalable high-order multigrid-FFT Poisson solver designed for unbounded domains on adaptive multiresolution grids. The method combines a multigrid solver with a Fourier-based direct solver and high-order compact stencils to handle various boundary conditions efficiently. The solver is validated against analytical solutions and demonstrates scalability up to 16,384 cores on European high-performance computing infrastructures.

- **[arXiv251210] Modeling the Potential of Message-Free Communication via CXL.mem**
  - **tags:** [sys], [high-performance computing], [CXL.mem, MPI, performance modeling, Mitos, memory trace sampling]
  - **authors:** Stepan Vanecek, Matthew Turner, Manisha Gajbe, Matthew Wolf, Martin Schulz
  - **institution:** Technical University of Munich, Hewlett Packard Enterprise
  - **link:** https://arxiv.org/pdf/2512.08005
  - **Simple LLM Summary:** The paper presents a performance evaluation toolchain and extended model that analyzes MPI application data access patterns to predict which inter-node data transfers could benefit from using CXL.mem for direct memory access instead of traditional message passing. The method uses an extended memory trace sampling tool (Mitos) to model performance on a per-MPI-call basis. The main conclusion is that this approach can identify specific MPI calls with high optimization potential for CXL.mem, enabling targeted communication optimizations in HPC applications.

- **[arXiv251210] Emulation of Complex Matrix Multiplication based on the Chinese Remainder Theorem**
  - **tags:** [mlsys], [GPU kernels], [Ozaki-II scheme, Chinese Remainder Theorem, low-precision emulation, INT8 matrix engines, complex matrix multiplication]
  - **authors:** Yuki Uchino, Qianxiang Ma, Toshiyuki Imamura, Katsuhisa Ozaki, Patrick Lars Gutsche
  - **institution:** RIKEN Center for Computational Science, Shibaura Institute of Technology, Ecole Normale Superieure de Lyon
  - **link:** https://arxiv.org/pdf/2512.08321
  - **Simple LLM Summary:** This paper proposes methods to emulate high-precision complex matrix multiplication using low-precision INT8 hardware, based on the Ozaki-II scheme and the Chinese Remainder Theorem. On an NVIDIA B200 GPU, the methods achieve significant speedups (4.0x–6.5x) over native cuBLAS routines for large problem sizes. The approach offers a flexible trade-off between speed and accuracy, making it a promising default algorithm for various applications.

- **[arXiv251210] A Task Parallel Orthonormalization Multigrid Method For Multiphase Elliptic Problems**
  - **tags:** [sys], [high-performance computing], [multigrid, k-cycle, orthonormalization, task-parallel, asynchronous execution]
  - **authors:** Teoman Toprak, Florian Kummer
  - **institution:** TU Darmstadt
  - **link:** https://arxiv.org/pdf/2512.08728
  - **Simple LLM Summary:** This paper presents a task-parallel variant of the K-cycle orthonormalization multigrid method for solving multiphase elliptic problems. The core method leverages asynchronous execution to overcome the scalability limitations of traditional bulk-synchronous implementations. The main conclusion is that this approach improves scalability and performance on modern large-scale parallel high-performance computing systems.

- **[arXiv251210] Spatio-Temporal Shifting to Reduce Carbon, Water, and Land-Use Footprints of Cloud Workloads**
  - **tags:** [sys], [cloud sustainability], [workload shifting, spatial shifting, temporal shifting, simulation, Land Usage Effectiveness (LUE), carbon-aware scheduling]
  - **authors:** Giulio Attenni, Youssef Moawad, Novella Bartolini, Lauritz Thamsen
  - **institution:** University of Glasgow, "La Sapienza" University of Rome
  - **link:** https://arxiv.org/pdf/2512.08725
  - **Simple LLM Summary:** This paper investigates using spatial and temporal shifting of cloud workloads to reduce their environmental footprints. Through simulations with real-world cloud provider data and workload traces, it finds that spatial shifting yields substantial reductions in carbon, water, and land-use footprints, with temporal shifting providing an additional, incremental benefit when combined.

- **[arXiv251210] Skewness-Guided Pruning of Multimodal Swin Transformers for Federated Skin Lesion Classification on Edge Devices**
  - **tags:** [mlsys], [multi-modal inference], [skewness-guided pruning, federated learning, swin transformer, model compression, multi-head self-attention, multi-layer perceptron]
  - **authors:** Kuniko Paxton, Koorosh Aslansefat, Dhavalkumar Thakker, Yiannis Papadopoulos
  - **institution:** University of Hull
  - **link:** https://arxiv.org/pdf/2512.08751
  - **Simple LLM Summary:** This paper proposes a skewness-guided pruning method for multimodal Swin Transformers, selectively pruning layers based on the statistical skewness of their outputs to reduce model size. It validates the method in a federated learning setting for skin lesion classification, achieving a 36% size reduction without accuracy loss, demonstrating efficient compression for edge deployment.

- **[arXiv251210] Parallel Batch Dynamic Vertex Coloring in $O(\log Δ)$ Amortized Update Time**
  - **tags:** [sys], [graph algorithms], [batch-dynamic algorithm, vertex coloring, amortized update time, parallel span, randomized algorithm]
  - **authors:** Chase Hutton, Adam Melrod
  - **institution:** University of Maryland, Harvard University
  - **link:** https://arxiv.org/pdf/2512.08742
  - **Simple LLM Summary:** This paper presents a new parallel batch-dynamic algorithm for maintaining a proper (Δ+1)-vertex coloring in graphs. The method builds on a sequential dynamic algorithm and achieves an expected amortized update time of O(log Δ) with a parallel span of O(polylog b + polylog n) for processing batches of updates. This is the first algorithm to achieve such parallel efficiency for the dynamic vertex coloring problem.


**cs.AI/cs.LG contains "reinforcement learning" total: 19**
- [arXiv251210] Using reinforcement learning to probe the role of feedback in skill acquisition [link](https://arxiv.org/pdf/2512.08463)
- [arXiv251210] Benchmarking Offline Multi-Objective Reinforcement Learning in Critical Care [link](https://arxiv.org/pdf/2512.08012)
- [arXiv251210] rSIM: Incentivizing Reasoning Capabilities of LLMs via Reinforced Strategy Injection [link](https://arxiv.org/pdf/2512.08300)
- [arXiv251210] Multi-Agent Deep Reinforcement Learning for Collaborative UAV Relay Networks under Jamming Atatcks [link](https://arxiv.org/pdf/2512.08341)
- [arXiv251210] ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models [link](https://arxiv.org/pdf/2512.07843)
- [arXiv251210] Scalable Offline Model-Based RL with Action Chunks [link](https://arxiv.org/pdf/2512.08108)
- [arXiv251210] Training LLMs for Honesty via Confessions [link](https://arxiv.org/pdf/2512.08093)
- [arXiv251210] Robust Agents in Open-Ended Worlds [link](https://arxiv.org/pdf/2512.08139)
- [arXiv251210] Empowerment Gain and Causal Model Construction: Children and adults are sensitive to controllability and variability in their causal interventions [link](https://arxiv.org/pdf/2512.08230)
- [arXiv251210] Optimal Perturbation Budget Allocation for Data Poisoning in Offline Reinforcement Learning [link](https://arxiv.org/pdf/2512.08485)
- [arXiv251210] Mind to Hand: Purposeful Robotic Control via Embodied Reasoning [link](https://arxiv.org/pdf/2512.08580)
- [arXiv251210] TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models [link](https://arxiv.org/pdf/2512.08153)
- [arXiv251210] Direct transfer of optimized controllers to similar systems using dimensionless MPC [link](https://arxiv.org/pdf/2512.08667)
- [arXiv251210] From Accuracy to Impact: The Impact-Driven AI Framework (IDAIF) for Aligning Engineering Architecture with Theory of Change [link](https://arxiv.org/pdf/2512.08449)
- [arXiv251210] An Introduction to Deep Reinforcement and Imitation Learning [link](https://arxiv.org/pdf/2512.08052)
- [arXiv251210] Learning and Editing Universal Graph Prompt Tuning via Reinforcement Learning [link](https://arxiv.org/pdf/2512.08763)
- [arXiv251210] Reinforcement Learning From State and Temporal Differences [link](https://arxiv.org/pdf/2512.08855)
- [arXiv251210] No Labels, No Problem: Training Visual Reasoners with Multimodal Verifiers [link](https://arxiv.org/pdf/2512.08889)
- [arXiv251210] Heuristics for Combinatorial Optimization via Value-based Reinforcement Learning: A Unified Framework and Analysis [link](https://arxiv.org/pdf/2512.08601)

**cs.AI/cs.LG contains "accelerate" total: 15**
- [arXiv251210] CarBench: A Comprehensive Benchmark for Neural Surrogates on High-Fidelity 3D Car Aerodynamics [link](https://arxiv.org/pdf/2512.07847)
- [arXiv251210] Empowering smart app development with SolidGPT: an edge-cloud hybrid AI agent framework [link](https://arxiv.org/pdf/2512.08286)
- [arXiv251210] Unveiling Latent Knowledge in Chemistry Language Models through Sparse Autoencoders [link](https://arxiv.org/pdf/2512.08077)
- [arXiv251210] Wavelet-Accelerated Physics-Informed Quantum Neural Network for Multiscale Partial Differential Equations [link](https://arxiv.org/pdf/2512.08256)
- [arXiv251210] DeepCode: Open Agentic Coding [link](https://arxiv.org/pdf/2512.07921)
- [arXiv251210] gHAWK: Local and Global Structure Encoding for Scalable Training of Graph Neural Networks on Knowledge Graphs [link](https://arxiv.org/pdf/2512.08274)
- [arXiv251210] LAPA: Log-Domain Prediction-Driven Dynamic Sparsity Accelerator for Transformer Model [link](https://arxiv.org/pdf/2512.07855)
- [arXiv251210] LayerPipe2: Multistage Pipelining and Weight Recompute via Improved Exponential Moving Average for Training Neural Networks [link](https://arxiv.org/pdf/2512.08160)
- [arXiv251210] Gradient-Informed Monte Carlo Fine-Tuning of Diffusion Models for Low-Thrust Trajectory Design [link](https://arxiv.org/pdf/2512.08705)
- [arXiv251210] Neural Ordinary Differential Equations for Simulating Metabolic Pathway Dynamics from Time-Series Multiomics Data [link](https://arxiv.org/pdf/2512.08732)
- [arXiv251210] A Practical Guide for Designing, Developing, and Deploying Production-Grade Agentic AI Workflows [link](https://arxiv.org/pdf/2512.08769)
- [arXiv251210] InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models [link](https://arxiv.org/pdf/2512.08829)
- [arXiv251210] Refining Diffusion Models for Motion Synthesis with an Acceleration Loss to Generate Realistic IMU Data [link](https://arxiv.org/pdf/2512.08859)
- [arXiv251210] Open Polymer Challenge: Post-Competition Report [link](https://arxiv.org/pdf/2512.08896)
- [arXiv251210] Fast and Robust Diffusion Posterior Sampling for MR Image Reconstruction Using the Preconditioned Unadjusted Langevin Algorithm [link](https://arxiv.org/pdf/2512.05791)

## 2025-12-11

**cs.DC total: 11**

- **[arXiv251211] PHWSOA: A Pareto-based Hybrid Whale-Seagull Scheduling for Multi-Objective Tasks in Cloud Computing**
  - **tags:** [ai], [optimization algorithms], [Whale Optimization Algorithm, Seagull Optimization Algorithm, Pareto dominance, Halton sequence, CloudSim, makespan, load balancing]
  - **authors:** Zhi Zhao, Hang Xiao, Wei Rang
  - **institution:** Shandong Normal University
  - **link:** https://arxiv.org/pdf/2512.09568
  - **Simple LLM Summary:** This paper proposes a hybrid optimization algorithm called PHWSOA, which combines the Whale and Seagull Optimization Algorithms using Pareto dominance to optimize task scheduling in cloud computing. The method improves makespan, load balancing, and cost. Experiments show it significantly outperforms baseline algorithms in these metrics.

- **[arXiv251211] Passing the Baton: High Throughput Distributed Disk-Based Vector Search with BatANN**
  - **tags:** [mlsys], [others], [distributed vector search, disk-based ANN, single global graph, baton passing, scatter-gather baseline]
  - **authors:** Nam Anh Dang, Ben Landrum, Ken Birman
  - **institution:** Cornell University, Cornell Tech
  - **link:** https://arxiv.org/pdf/2512.09331
  - **Simple LLM Summary:** The paper presents BatANN, a distributed disk-based approximate nearest neighbor search system that improves throughput by passing the full query state to another server when accessing remote data, allowing the search to continue locally on that machine. It achieves near-linear throughput scaling and maintains low latency compared to a scatter-gather baseline, while operating over a single global graph.

- **[arXiv251211] SynthPix: A lightspeed PIV images generator**
  - **tags:** [mlsys], [others], [JAX, synthetic data generation, Particle Image Velocimetry (PIV), parallelism, accelerators, reinforcement learning, optical flow]
  - **authors:** Antonio Terpin, Alan Bonomi, Francesco Banelli, Raffaello D'Andrea
  - **institution:** ETH Zürich
  - **link:** https://arxiv.org/pdf/2512.09664
  - **Simple LLM Summary:** SynthPix is a high-performance synthetic image generator for Particle Image Velocimetry (PIV) implemented in JAX, designed for parallelism on accelerators. It achieves orders of magnitude higher throughput in generating image pairs compared to existing tools. The tool aims to support data-intensive training for flow estimation methods and accelerate development in active fluid control studies.

- **[arXiv251211] Ariel-ML: Computing Parallelization with Embedded Rust for Neural Networks on Heterogeneous Multi-core Microcontrollers**
  - **tags:** [mlsys], [others], [TinyML, embedded Rust, multi-core parallelization, microcontroller, inference optimization]
  - **authors:** Zhaolan Huang, Kaspar Schleiser, Gyungmin Myung, Emmanuel Baccelli
  - **institution:** Freie Universität Berlin, KAIST, Inria Saclay
  - **link:** https://arxiv.org/pdf/2512.09800
  - **Simple LLM Summary:** This paper introduces Ariel-ML, a toolkit that combines a generic TinyML pipeline with an embedded Rust software platform to automate parallelized inference computation on heterogeneous multi-core microcontrollers. It shows that Ariel-ML outperforms prior work in inference latency while achieving memory footprints comparable to existing C/C++ toolkits, providing a useful basis for TinyML and embedded Rust development.

- **[arXiv251211] Recoverable Lock-Free Locks**
  - **tags:** [sys], [concurrent algorithms], [lock-freedom, recoverability, transformation, nested locks, non-volatile memory (NVM)]
  - **authors:** Hagit Attiya, Panagiota Fatourou, Eleftherios Kosmas, Yuanhao Wei
  - **institution:** Technion – Israel Institute of Technology, FORTH ICS, University of Crete, Hellenic Mediterranean University, University of British Columbia
  - **link:** https://arxiv.org/pdf/2512.09710
  - **Simple LLM Summary:** This paper introduces a novel transformation that converts lock-based implementations into lock-free and recoverable ones. The method provides a substitution for lock acquire and release operations, supporting nested locks and ensuring correctness. It is the first technique to simultaneously achieve both lock-freedom and recoverability, addressing challenges in concurrent and persistent memory systems.

- **[arXiv251211] Straggler Tolerant and Resilient DL Training on Homogeneous GPUs**
  - **tags:** [mlsys], [fault-tolerance], [straggler mitigation, synchronous SGD, asynchronous SGD, parameter server, all-reduce, time-to-accuracy, resource reallocation]
  - **authors:** Zeyu Zhang, Haiying Shen
  - **institution:** University of Virginia
  - **link:** https://arxiv.org/pdf/2512.09685
  - **Simple LLM Summary:** The paper proposes STAR, a system that introduces new synchronization modes and resource management to mitigate stragglers in homogeneous GPU clusters. It finds that stragglers persist due to CPU/bandwidth imbalances and that simply switching to asynchronous SGD can be counterproductive. Evaluations show STAR significantly reduces Time-To-Accuracy compared to state-of-the-art systems while maintaining accuracy.

- **[arXiv251211] Efficient MoE Serving in the Memory-Bound Regime: Balance Activated Experts, Not Tokens**
  - **tags:** [mlsys], [llm inference], [expert parallelism, token routing, load balancing, memory-bound regime, allGather, Minimum Expert Token Routing (METRO)]
  - **authors:** Yanpeng Yu, Haiyue Ma, Krish Agarwal, Nicolai Oswald, Qijing Huang, Hugo Linsenmaier, Chunhui Mei, Ritchie Zhao, Ritika Borkar, Bita Darvish Rouhani, David Nellans, Ronny Krashinsky, Anurag Khandelwal
  - **institution:** Yale University, Princeton University, Carnegie Mellon University, NVIDIA
  - **link:** https://arxiv.org/pdf/2512.09277
  - **Simple LLM Summary:** The paper proposes METRO, a novel token-routing algorithm for Mixture-of-Experts (MoE) model serving that balances the number of activated experts per GPU instead of the number of tokens. This approach reduces memory pressure and improves performance in memory-bound regimes like the decode phase. Evaluations show METRO significantly reduces decode latency and improves throughput compared to existing token-balancing methods.

- **[arXiv251211] WarmServe: Enabling One-for-Many GPU Prewarming for Multi-LLM Serving**
  - **tags:** [mlsys], [llm inference], [universal GPU workers, one-for-many GPU prewarming, evict-aware model placement, proactive prewarming, zero-overhead memory switching]
  - **authors:** Chiheng Lou, Sheng Qi, Rui Kang, Yong Zhang, Chen Sun, Pengcheng Wang, Bingyang Liu, Xuanzhe Liu, Xin Jin
  - **institution:** Peking University, Huawei Technologies Co., Ltd
  - **link:** https://arxiv.org/pdf/2512.09472
  - **Simple LLM Summary:** The paper proposes WarmServe, a multi-LLM serving system that uses universal GPU workers to enable one-for-many GPU prewarming based on workload predictions. Its key techniques include an evict-aware model placement strategy, proactive prewarming, and a zero-overhead memory switching mechanism. Evaluation shows it significantly improves time-to-first-token and request throughput compared to existing autoscaling and GPU-sharing systems.

- **[arXiv251211] Scalable Construction of Spiking Neural Networks using up to thousands of GPUs**
  - **tags:** [mlsys], [cluster infrastructure], [spiking neural networks, MPI, multi-GPU, exascale computing, collective communication, point-to-point communication]
  - **authors:** Bruno Golosio, Gianmarco Tiddia, José Villamar, Luca Pontisso, Luca Sergi, Francesco Simula, Pooja Babu, Elena Pastorelli, Abigail Morrison, Markus Diesmann, Alessandro Lonardo, Pier Stanislao Paolucci, Johanna Senk
  - **institution:** University of Cagliari, Istituto Nazionale di Fisica Nucleare (INFN), Jülich Research Centre, RWTH Aachen University
  - **link:** https://arxiv.org/pdf/2512.09502
  - **Simple LLM Summary:** This paper presents a novel method for constructing large-scale spiking neural networks on multi-GPU clusters using MPI, where each process builds local connectivity to enable efficient spike exchange. The method demonstrates scalable performance for cortical models using both point-to-point and collective communication strategies. The work enables the simulation of complex, brain-scale neural systems on upcoming exascale supercomputers.

- **[arXiv251211] A Distributed Framework for Privacy-Enhanced Vision Transformers on the Edge**
  - **tags:** [mlsys], [multi-modal inference], [distributed offloading, hierarchical framework, edge-cloud continuum, Vision Transformer (ViT), Segment Anything Model (SAM), privacy protection, data partitioning]
  - **authors:** Zihao Ding, Mufeng Zhu, Zhongze Tang, Sheng Wei, Yao Liu
  - **institution:** Rutgers University
  - **link:** https://arxiv.org/pdf/2512.09309
  - **Simple LLM Summary:** The paper proposes a distributed, hierarchical offloading framework for Vision Transformers that partitions visual data and distributes the pieces across multiple independent cloud servers, with final aggregation performed only on a trusted local edge device. This design prevents any single external server from possessing the complete image, thereby enhancing privacy. The evaluation using the Segment Anything Model shows the framework maintains near-baseline segmentation performance while significantly reducing the risk of content reconstruction and data exposure.

- **[arXiv251211] Link-Sharing Backpressure Routing In Wireless Multi-Hop Networks**
  - **tags:** [sys], [wireless networking], [backpressure routing, Lyapunov drift, MaxWeight scheduling, shortest path-biased BP, Maximum Utility link-sharing]
  - **authors:** Zhongyuan Zhao, Yujun Ming, Ananthram Swami, Kevin Chan, Fikadu Dagefu, Santiago Segarra
  - **institution:** Rice University, US Army DEVCOM Army Research Laboratory
  - **link:** https://arxiv.org/pdf/2512.09902
  - **Simple LLM Summary:** This paper revisits the Lyapunov drift theory of Backpressure routing to show that exclusive commodity selection is unnecessary, and proposes a Maximum Utility (MaxU) link-sharing method. The proposed MaxU SP-BP mitigates the last-packet problem and slightly expands the network capacity region without increasing control message overhead.


**cs.AI/cs.LG contains "reinforcement learning" total: 13**
- [arXiv251211] RouteRAG: Efficient Retrieval-Augmented Generation from Text and Graph via Reinforcement Learning [link](https://arxiv.org/pdf/2512.09487)
- [arXiv251211] Optimizing Algorithms for Mobile Health Interventions with Active Querying Optimization [link](https://arxiv.org/pdf/2512.08950)
- [arXiv251211] Training Multi-Image Vision Agents via End2End Reinforcement Learning [link](https://arxiv.org/pdf/2512.08980)
- [arXiv251211] CFLight: Enhancing Safety with Traffic Signal Control through Counterfactual Learning [link](https://arxiv.org/pdf/2512.09368)
- [arXiv251211] Dynamic one-time delivery of critical data by small and sparse UAV swarms: a model problem for MARL scaling studies [link](https://arxiv.org/pdf/2512.09682)
- [arXiv251211] Learning Unmasking Policies for Diffusion Language Models [link](https://arxiv.org/pdf/2512.09106)
- [arXiv251211] Toward Closed-loop Molecular Discovery via Language Model, Property Alignment and Strategic Search [link](https://arxiv.org/pdf/2512.09566)
- [arXiv251211] Generalizable Collaborative Search-and-Capture in Cluttered Environments via Path-Guided MAPPO and Directional Frontier Allocation [link](https://arxiv.org/pdf/2512.09410)
- [arXiv251211] Financial Instruction Following Evaluation (FIFE) [link](https://arxiv.org/pdf/2512.08965)
- [arXiv251211] RIFT: A Scalable Methodology for LLM Accelerator Fault Assessment using Reinforcement Learning [link](https://arxiv.org/pdf/2512.09829)
- [arXiv251211] FlipLLM: Efficient Bit-Flip Attacks on Multimodal LLMs using Reinforcement Learning [link](https://arxiv.org/pdf/2512.09872)
- [arXiv251211] STACHE: Local Black-Box Explanations for Reinforcement Learning Policies [link](https://arxiv.org/pdf/2512.09909)
- [arXiv251211] Online Inference of Constrained Optimization: Primal-Dual Optimality and Sequential Quadratic Programming [link](https://arxiv.org/pdf/2512.08948)

**cs.AI/cs.LG contains "accelerate" total: 12**
- [arXiv251211] ODMA: On-Demand Memory Allocation Framework for LLM Serving on LPDDR-Class Accelerators [link](https://arxiv.org/pdf/2512.09427)
- [arXiv251211] FBA$^2$D: Frequency-based Black-box Attack for AI-generated Image Detection [link](https://arxiv.org/pdf/2512.09264)
- [arXiv251211] Graph Deep Learning for Intracranial Aneurysm Blood Flow Simulation and Risk Assessment [link](https://arxiv.org/pdf/2512.09013)
- [arXiv251211] HSCP: A Two-Stage Spectral Clustering Framework for Resource-Constrained UAV Identification [link](https://arxiv.org/pdf/2512.08983)
- [arXiv251211] CluCERT: Certifying LLM Robustness via Clustering-Guided Denoising Smoothing [link](https://arxiv.org/pdf/2512.08967)
- [arXiv251211] Llama-based source code vulnerability detection: Prompt engineering vs Fine tuning [link](https://arxiv.org/pdf/2512.09006)
- [arXiv251211] Towards Lossless Ultimate Vision Token Compression for VLMs [link](https://arxiv.org/pdf/2512.09010)
- [arXiv251211] Generalizable Collaborative Search-and-Capture in Cluttered Environments via Path-Guided MAPPO and Directional Frontier Allocation [link](https://arxiv.org/pdf/2512.09410)
- [arXiv251211] 3DID: Direct 3D Inverse Design for Aerodynamics with Physics-Aware Optimization [link](https://arxiv.org/pdf/2512.08987)
- [arXiv251211] Tensor-Compressed and Fully-Quantized Training of Neural PDE Solvers [link](https://arxiv.org/pdf/2512.09202)
- [arXiv251211] RIFT: A Scalable Methodology for LLM Accelerator Fault Assessment using Reinforcement Learning [link](https://arxiv.org/pdf/2512.09829)
- [arXiv251211] Fast Factorized Learning: Powered by In-Memory Database Systems [link](https://arxiv.org/pdf/2512.09836)

## 2025-12-12

**cs.DC total: 18**

- **[arXiv251212] CloudFix: Automated Policy Repair for Cloud Access Control Policies Using Large Language Models**
  - **tags:** [mlsys], [llm inference], [formal methods, fault localization, SMT solvers, automated program repair, access control policies]
  - **authors:** Bethel Hall, Owen Ungaro, William Eiers
  - **institution:** Stevens Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.09957
  - **Simple LLM Summary:** The paper introduces CloudFix, a framework that combines formal methods for fault localization with Large Language Models to generate and verify repairs for cloud access control policies. It demonstrates improved repair accuracy over a baseline on a dataset of real-world AWS policies, showing the effectiveness of LLMs for this automated security task.

- **[arXiv251212] ELANA: A Simple Energy and Latency Analyzer for LLMs**
  - **tags:** [mlsys], [llm inference], [profiling, latency analysis, energy measurement, key-value cache, quantization, compression]
  - **authors:** Hung-Yueh Chiang, Bokun Wang, Diana Marculescu
  - **institution:** The University of Texas at Austin
  - **link:** https://arxiv.org/pdf/2512.09946
  - **Simple LLM Summary:** The paper introduces ELANA, a lightweight profiling tool for benchmarking the latency and energy consumption of large language models (LLMs) during inference. It supports metrics like prefilling and generation latency, model size, and KV cache footprint across various hardware platforms. The tool is designed to fill a gap in standardized, academic-friendly evaluation for efficient LLM deployment and research.

- **[arXiv251212] Hybrid Learning and Optimization-Based Dynamic Scheduling for DL Workloads on Heterogeneous GPU Clusters**
  - **tags:** [mlsys], [cluster infrastructure], [reinforcement learning, MILP, job scheduling, GPU utilization, heterogeneous clusters]
  - **authors:** Shruti Dongare, Redwan Ibne Seraj Khan, Hadeel Albahar, Nannan Zhao, Diego Melendez Maita, Ali R. Butt
  - **institution:** Virginia Tech, Kuwait University, Northwestern Polytechnical University
  - **link:** https://arxiv.org/pdf/2512.10271
  - **Simple LLM Summary:** This paper presents RLTune, a hybrid scheduling framework that uses reinforcement learning for job prioritization and Mixed-Integer Linear Programming (MILP) for job-to-node mapping on heterogeneous GPU clusters. It improves GPU utilization by up to 20%, reduces queueing delay by up to 81%, and shortens job completion time by as much as 70% without requiring per-job profiling.

- **[arXiv251212] A study of the spectrum resource leasing method based on ERC4907 extension**
  - **tags:** [sys], [blockchain], [ERC4907, smart contract, NFT, dynamic spectrum sharing, multi-slot authorization, gas optimization]
  - **authors:** Zhiming Liang, Bin Chen, Litao Ye, Chen Sun, Shuo Wang, Zhe Peng
  - **institution:** Shenzhen University, Sony (China) Limited, The Hong Kong Polytechnic University
  - **link:** https://arxiv.org/pdf/2512.09942
  - **Simple LLM Summary:** This paper proposes a Multi-slot ERC4907 (M-ERC4907) extension method to enhance blockchain-based spectrum resource leasing by enabling batch configuration of multiple time slots and simultaneous multi-user authorization. The method was tested on the Remix platform and was shown to significantly reduce on-chain transactions and gas consumption, thereby improving scalability and allocation efficiency.

- **[arXiv251212] When Quantum Federated Learning Meets Blockchain in 6G Networks**
  - **tags:** [mlsys], [others], [quantum federated learning, blockchain, 6G networks, decentralized learning, smart contracts]
  - **authors:** Dinh C. Nguyen, Md Bokhtiar Al Zami, Ratun Rahman, Shaba Shaon, Tuy Tan Nguyen, Fatemeh Afghah
  - **institution:** University of Alabama in Huntsville, Florida State University, Clemson University
  - **link:** https://arxiv.org/pdf/2512.09958
  - **Simple LLM Summary:** This paper proposes QFLchain, a framework that integrates Quantum Federated Learning (QFL) with blockchain technology to enable secure and scalable distributed AI model training in 6G networks. It investigates key challenges like communication overhead and security vulnerabilities. The authors conclude that this integration offers potential advantages for training performance in future decentralized, data-intensive environments.

- **[arXiv251212] GoodSpeed: Optimizing Fair Goodput with Adaptive Speculative Decoding in Distributed Edge Inference**
  - **tags:** [mlsys], [llm inference], [speculative decoding, gradient scheduling algorithm, resource allocation, fluid sample path analysis, distributed edge inference]
  - **authors:** Phuong Tran, Tzu-Hao Liu, Long Tan Le, Tung-Anh Nguyen, Van Quan La, Eason Yu, Han Shu, Choong Seon Hong, Nguyen H. Tran
  - **institution:** Kyung Hee University
  - **link:** https://arxiv.org/pdf/2512.09963
  - **Simple LLM Summary:** The paper introduces GOODSPEED, a distributed inference framework that uses adaptive speculative decoding with a central verification server coordinating heterogeneous draft servers. It employs a gradient scheduling algorithm to dynamically allocate verification tasks, maximizing a logarithmic utility function for proportional fairness. The analysis shows GOODSPEED converges to optimal goodput allocation and maintains near-optimal performance with bounded error, providing a scalable and fair solution for distributed LLM inference.

- **[arXiv251212] TDC-Cache: A Trustworthy Decentralized Cooperative Caching Framework for Web3.0**
  - **tags:** [mlsys], [others], [decentralized caching, deep reinforcement learning, proof of cooperative learning, decentralized oracle network]
  - **authors:** Jinyu Chen, Long Shi, Taotao Wang, Jiaheng Wang, Wei Zhang
  - **institution:** Nanjing University of Science and Technology, Shenzhen University, Southeast University, Purple Mountain Laboratories, The University of New South Wales
  - **link:** https://arxiv.org/pdf/2512.09961
  - **Simple LLM Summary:** The paper proposes a Trustworthy Decentralized Cooperative Caching (TDC-Cache) framework for Web3.0, featuring a two-layer architecture with a Decentralized Oracle Network and employing Deep Reinforcement Learning to optimize caching strategies alongside a Proof of Cooperative Learning consensus mechanism. Experimental results show the framework reduces access latency by 20%, increases cache hit rate by up to 18%, and improves consensus success rate by 10% compared to existing approaches.

- **[arXiv251212] Design Space Exploration of DMA based Finer-Grain Compute Communication Overlap**
  - **tags:** [mlsys], [cluster infrastructure], [FiCCO, DMA offloading, compute-communication overlap, sharding, execution schedules, heuristics]
  - **authors:** Shagnik Pal, Shaizeen Aga, Suchita Pati, Mahzabeen Islam, Lizy K. John
  - **institution:** Advanced Micro Devices Inc., The University of Texas at Austin
  - **link:** https://arxiv.org/pdf/2512.10236
  - **Simple LLM Summary:** This paper proposes FiCCO, a finer-grain compute-communication overlap technique that decomposes operations beyond the shard level to enable more flexible scheduling and performance gains in distributed ML. It characterizes inefficiency losses, explores a design space of FiCCO schedules with heuristics for selection, and offloads communication to GPU DMA engines to reduce contention. The approach achieves up to 1.6x speedup in realistic ML deployments, with heuristics providing accurate guidance in 81% of unseen scenarios.

- **[arXiv251212] A Comparative Analysis of zk-SNARKs and zk-STARKs: Theory and Practice**
  - **tags:** [sys], [cryptography], [zk-SNARKs, zk-STARKs, Groth16, zero-knowledge proofs, trusted setup, post-quantum security, proof generation, verification latency]
  - **authors:** Ayush Nainwal, Atharva Kamble, Nitin Awathare
  - **institution:** Indian Institute of Technology, Jodhpur
  - **link:** https://arxiv.org/pdf/2512.10020
  - **Simple LLM Summary:** This paper presents an empirical, implementation-level comparison of zk-SNARKs (specifically Groth16) and zk-STARKs on a consumer-grade ARM platform, evaluating proof generation time, verification latency, and proof size. The main conclusion is that zk-SNARKs generate proofs much faster and with a much smaller size, but require a trusted setup and verify slower, while zk-STARKs verify faster, are transparent, and are post-quantum secure, but have larger proofs and slower generation.

- **[arXiv251212] High-Dimensional Data Processing: Benchmarking Machine Learning and Deep Learning Architectures in Local and Distributed Environments**
  - **tags:** [mlsys], [cluster infrastructure], [Apache Spark, PyTorch, XGBoost, TF-IDF, Multi-Layer Perceptron, Batch Normalization, Dropout, Adam optimizer, Grid Search]
  - **authors:** Julian Rodriguez, Piotr Lopez, Emiliano Lerma, Rafael Medrano, Jacobo Hernandez
  - **institution:** Universidad de Guanajuato
  - **link:** https://arxiv.org/pdf/2512.10312
  - **Simple LLM Summary:** This paper benchmarks machine learning and deep learning architectures for high-dimensional data processing across three case studies (Epsilon, Rest-Mex, IMDb) using techniques like MLP, TF-IDF, and XGBoost, implemented in both local (PyTorch) and distributed (Apache Spark) environments. It concludes that this methodological progression demonstrates the versatility of ML techniques for big data analysis across different scales and data types, establishing a robust framework. The work includes the technical implementation of a distributed computing cluster with Apache Spark.

- **[arXiv251212] Making Wide Stripes Practical: Cascaded Parity LRCs for Efficient Repair and High Reliability**
  - **tags:** [sys], [distributed storage systems], [cascaded parity LRCs, wide stripes, erasure coding, fault tolerance, repair algorithms]
  - **authors:** Fan Yu, Guodong Li, Si Wu, Weijun Fang, Sihuang Hu
  - **institution:** Shandong University, Quan Cheng Laboratory
  - **link:** https://arxiv.org/pdf/2512.10425
  - **Simple LLM Summary:** The paper introduces Cascaded Parity LRCs (CP-LRCs), a new family of erasure codes that embed structured dependencies between local and global parity blocks to improve repair efficiency in wide-stripe storage systems. This design enables low-bandwidth repairs for both single-node and multi-node failures while maintaining high reliability. Evaluations on Alibaba Cloud show significant reductions in repair time compared to existing approaches.

- **[arXiv251212] Bit of a Close Talker: A Practical Guide to Serverless Cloud Co-Location Attacks**
  - **tags:** [sys], [cloud security], [serverless computing, co-location attacks, micro-architectural side-channel, cloud scheduler vulnerabilities, mitigation strategy]
  - **authors:** Wei Shao, Najmeh Nazari, Behnam Omidi, Setareh Rafatirad, Houman Homayoun, Khaled N. Khasawneh, Chongzhou Fang
  - **institution:** University of California, Davis, George Mason University, Rochester Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.10361
  - **Simple LLM Summary:** The paper presents a methodology to uncover vulnerabilities in serverless cloud scheduling algorithms and demonstrates how to construct co-location attacks through normal user interfaces. It successfully achieves instance co-location on open-source infrastructures and Microsoft Azure Functions. The work concludes by highlighting critical areas for security enhancements in cloud schedulers and offers a mitigation strategy to defend against such attacks.

- **[arXiv251212] Clustered Federated Learning with Hierarchical Knowledge Distillation**
  - **tags:** [mlsys], [cluster infrastructure], [clustered federated learning, hierarchical knowledge distillation, multi-teacher knowledge distillation, bi-level aggregation]
  - **authors:** Sabtain Ahmad, Meerzhan Kanatbekova, Ivona Brandic, Atakan Aral
  - **institution:** TU Wien, University of Vienna
  - **link:** https://arxiv.org/pdf/2512.10443
  - **Simple LLM Summary:** This paper proposes CFLHKD, a hierarchical clustered federated learning method that uses multi-teacher knowledge distillation to enable inter-cluster knowledge sharing while preserving cluster-specific personalization. It employs bi-level aggregation to bridge local and global learning. The method outperforms baselines, achieving a 3.32-7.57% improvement in accuracy for both cluster-specific and global models.

- **[arXiv251212] A Proof of Success and Reward Distribution Protocol for Multi-bridge Architecture in Cross-chain Communication**
  - **tags:** [sys], [blockchain interoperability], [multi-bridge architecture, reward distribution, Gini index, Nakamoto coefficient, Proof of Success and Reward Distribution (PSCRD)]
  - **authors:** Damilare Peter Oyinloye, Mohd Sameen Chishti, Jingyue Li
  - **institution:** Norwegian University of Science and Technology (NTNU)
  - **link:** https://arxiv.org/pdf/2512.10667
  - **Simple LLM Summary:** This paper proposes the Proof of Success and Reward Distribution (PSCRD) protocol, a novel multi-bridge coordination and incentive system for cross-chain communication. It uses a fair reward distribution mechanism to incentivize honest bridge participation, thereby improving decentralization and fault tolerance. Mathematical analysis and simulations show that PSCRD significantly enhances reward fairness and decentralization without substantially increasing user costs.

- **[arXiv251212] ESS: An Offload-Centric Latent-Cache Management Architecture for DeepSeek-V3.2-Exp**
  - **tags:** [mlsys], [llm inference], [offload-centric system, latent-cache management, sparse attention, batch-size scaling, PD disaggregation]
  - **authors:** Xinhang Chen, Chao Zhang, Jiahuan He, Wei Liu, Jianming Zhang, Wenlong Zhou, Xiao Li, Pai Zeng, Shiyong Li, Yuanpan Qian, Dong Li, Zhaogeng Li
  - **institution:** Baidu Inc.
  - **link:** https://arxiv.org/pdf/2512.10576
  - **Simple LLM Summary:** The paper proposes ESS, an offload-centric system architecture for DeepSeek-V3.2-Exp that selectively offloads the Latent-Cache to CPU memory to free up GPU capacity. This decouples batch-size scaling from GPU memory constraints, significantly improving Decode-stage throughput for long-context inference. High-fidelity simulations show throughput improvements of 69.4% at 32K context length and up to 123% at 128K.

- **[arXiv251212] D2M: A Decentralized, Privacy-Preserving, Incentive-Compatible Data Marketplace for Collaborative Learning**
  - **tags:** [mlsys], [fault-tolerance], [federated learning, blockchain, smart contracts, Byzantine robustness, incentive compatibility, YODA protocol, CONE]
  - **authors:** Yash Srivastava, Shalin Jain, Sneha Awathare, Nitin Awathare
  - **institution:** Indian Institute of Technology Jodhpur, Eastern University Pennsylvania
  - **link:** https://arxiv.org/pdf/2512.10372
  - **Simple LLM Summary:** The paper proposes D2M, a decentralized data marketplace that integrates federated learning, blockchain smart contracts for auctions and arbitration, and an off-chain compute network (CONE) to enable privacy-preserving collaborative learning. It introduces mechanisms like a modified YODA protocol and Corrected OSMD to ensure Byzantine robustness and incentive compatibility. The system demonstrates high accuracy and resilience on benchmark datasets, making it a practical framework for secure, decentralized data sharing.

- **[arXiv251212] Differential Privacy for Secure Machine Learning in Healthcare IoT-Cloud Systems**
  - **tags:** [mlsys], [others], [differential privacy, laplace noise, gaussian noise, hybrid noise mechanism, blockchain, edge computing, iot-cloud architecture]
  - **authors:** N Mangala, Murtaza Rangwala, S Aishwarya, B Eswara Reddy, Rajkumar Buyya, KR Venugopal, SS Iyengar, LM Patnaik
  - **institution:** JNTU Anantapur, University of Melbourne, UVCE Bangalore, Florida International University, National Institute of Advanced Studies Bangalore
  - **link:** https://arxiv.org/pdf/2512.10426
  - **Simple LLM Summary:** The paper proposes a multi-layer IoT-Edge-Cloud architecture for healthcare, integrating a differential privacy framework with machine learning models to protect patient data. It introduces a hybrid Laplace-Gaussian noise mechanism with adaptive budget allocation, which provides a balanced privacy-utility trade-off, reducing inference attacks and data reconstruction correlation while maintaining model accuracy. The hierarchical architecture, secured with blockchain, significantly reduces latency for emergency responses, validating its effectiveness for time-critical healthcare operations.

- **[arXiv251212] TriHaRd: Higher Resilience for TEE Trusted Time**
  - **tags:** [sys], [trusted execution environments], [TriHaRd, Byzantine-resilient clock updates, consistency checks, TEE trusted time protocol]
  - **authors:** Matthieu Bettinger, Sonia Ben Mokhtar, Pascal Felber, Etienne Rivière, Valerio Schiavoni, Anthony Simonet-Boulogne
  - **institution:** INSA Lyon, University of Neuchâtel, UCLouvain, iExec Blockchain Tech
  - **link:** https://arxiv.org/pdf/2512.10732
  - **Simple LLM Summary:** The paper proposes TriHaRd, a new trusted time protocol for Trusted Execution Environments (TEEs) that uses Byzantine-resilient clock updates and consistency checks to achieve high resilience against clock speed and offset manipulations. It is designed to mitigate attacks on previous protocols like Triad. The authors empirically demonstrate that TriHaRd successfully counters known attacks.


**cs.AI/cs.LG contains "reinforcement learning" total: 19**
- [arXiv251212] An exploration for higher efficiency in multi objective optimisation with reinforcement learning [link](https://arxiv.org/pdf/2512.10208)
- [arXiv251212] Latent Action World Models for Control with Unlabeled Trajectories [link](https://arxiv.org/pdf/2512.10016)
- [arXiv251212] SEMDICE: Off-policy State Entropy Maximization via Stationary Distribution Correction Estimation [link](https://arxiv.org/pdf/2512.10042)
- [arXiv251212] Push Smarter, Not Harder: Hierarchical RL-Diffusion Policy for Efficient Nonprehensile Manipulation [link](https://arxiv.org/pdf/2512.10099)
- [arXiv251212] A Privacy-Preserving Cloud Architecture for Distributed Machine Learning at Scale [link](https://arxiv.org/pdf/2512.10341)
- [arXiv251212] Boosting RL-Based Visual Reasoning with Selective Adversarial Entropy Intervention [link](https://arxiv.org/pdf/2512.10414)
- [arXiv251212] UACER: An Uncertainty-Aware Critic Ensemble Framework for Robust Adversarial Reinforcement Learning [link](https://arxiv.org/pdf/2512.10492)
- [arXiv251212] Adaptive Replay Buffer for Offline-to-Online Reinforcement Learning [link](https://arxiv.org/pdf/2512.10510)
- [arXiv251212] Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning [link](https://arxiv.org/pdf/2512.10534)
- [arXiv251212] Multi-Objective Reward and Preference Optimization: Theory and Algorithms [link](https://arxiv.org/pdf/2512.10601)
- [arXiv251212] Enhancing Radiology Report Generation and Visual Grounding using Reinforcement Learning [link](https://arxiv.org/pdf/2512.10691)
- [arXiv251212] How to Brake? Ethical Emergency Braking with Deep Reinforcement Learning [link](https://arxiv.org/pdf/2512.10698)
- [arXiv251212] Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving [link](https://arxiv.org/pdf/2512.10739)
- [arXiv251212] OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification [link](https://arxiv.org/pdf/2512.10756)
- [arXiv251212] Learning Controllable and Diverse Player Behaviors in Multi-Agent Environments [link](https://arxiv.org/pdf/2512.10835)
- [arXiv251212] Iterative Compositional Data Generation for Robot Control [link](https://arxiv.org/pdf/2512.10891)
- [arXiv251212] Digital Twin Supervised Reinforcement Learning Framework for Autonomous Underwater Navigation [link](https://arxiv.org/pdf/2512.10925)
- [arXiv251212] Curriculum-Based Reinforcement Learning for Autonomous UAV Navigation in Unknown Curved Tubular Conduit [link](https://arxiv.org/pdf/2512.10934)
- [arXiv251212] Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation [link](https://arxiv.org/pdf/2512.10949)

**cs.AI/cs.LG contains "accelerate" total: 6**
- [arXiv251212] Mind the Gap! Pathways Towards Unifying AI Safety and Ethics Research [link](https://arxiv.org/pdf/2512.10058)
- [arXiv251212] Robust Gradient Descent via Heavy-Ball Momentum with Predictive Extrapolation [link](https://arxiv.org/pdf/2512.10033)
- [arXiv251212] RoboNeuron: A Modular Framework Linking Foundation Models and ROS for Embodied AI [link](https://arxiv.org/pdf/2512.10394)
- [arXiv251212] Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning [link](https://arxiv.org/pdf/2512.10534)
- [arXiv251212] BabyVLM-V2: Toward Developmentally Grounded Pretraining and Benchmarking of Vision Foundation Models [link](https://arxiv.org/pdf/2512.10932)
- [arXiv251212] Bidirectional Normalizing Flow: From Data to Noise and Back [link](https://arxiv.org/pdf/2512.10953)
