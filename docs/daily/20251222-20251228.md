# 20251222-20251228

## 2025-12-22

**cs.DC total: 13**

- **[arXiv251222] Dion2: A Simple Method to Shrink Matrix in Muon**
  - **tags:** [mlsys], [llm training], [Muon optimizer, orthonormalization, Newton-Schulz iterations, matrix shrinking, sampling, sparse updates]
  - **authors:** Kwangjun Ahn, Noah Amsel, John Langford
  - **institution:** Microsoft Research, AI Frontiers, NYU
  - **link:** https://arxiv.org/pdf/2512.16928
  - **Simple LLM Summary:** The paper introduces Dion2, a simple method to improve the scalability of the Muon optimizer by reducing the cost of its orthonormalization step. It works by sampling a fraction of rows or columns at each iteration to orthonormalize, creating sparse updates that lower computation and communication costs. The method maintains update quality close to full Muon while significantly speeding up training steps.

- **[arXiv251222] Sedna: Sharding transactions in multiple concurrent proposer blockchains**
  - **tags:** [sys], [blockchain consensus], [multi-proposer consensus, verifiable rateless coding, sharding, until-decode privacy, bandwidth overhead]
  - **authors:** Alejandro Ranchal-Pedrosa, Benjamin Marsh, Lefteris Kokoris-Kogias, Alberto Sonnino
  - **institution:** Sei Labs, University of Portsmouth, Mysten Labs, University College London
  - **link:** https://arxiv.org/pdf/2512.17045
  - **Simple LLM Summary:** The paper introduces Sedna, a user-facing protocol that uses verifiable, rateless coding to shard and disseminate transactions across multiple concurrent proposers in a blockchain. This approach reduces bandwidth overhead and MEV exposure while maintaining liveness and privacy. The protocol achieves a 2-3x efficiency improvement over naive replication without requiring modifications to the underlying consensus mechanism.

- **[arXiv251222] Taming the Memory Footprint Crisis: System Design for Production Diffusion LLM Serving**
  - **tags:** [mlsys], [llm inference], [diffusion large language models, parallel decoding, logit-aware activation budgeting, phase-multiplexed scheduler, head-centric sparse attention, memory footprint optimization]
  - **authors:** Jiakun Fan, Yanglin Zhang, Xiangchen Li, Dimitrios S. Nikolopoulos
  - **institution:** Virginia Tech
  - **link:** https://arxiv.org/pdf/2512.17077
  - **Simple LLM Summary:** This paper introduces dLLM-Serve, a serving system that addresses the memory footprint crisis in diffusion LLMs by co-optimizing memory, scheduling, and generation quality through techniques like Logit-Aware Activation Budgeting and a Phase-Multiplexed Scheduler. It demonstrates significant throughput improvements and latency reductions on various GPUs, establishing a blueprint for scalable dLLM inference.

- **[arXiv251222] Practical Framework for Privacy-Preserving and Byzantine-robust Federated Learning**
  - **tags:** [mlsys], [fault-tolerance], [federated learning, byzantine-robust aggregation, privacy-preserving, dimensionality reduction, secure multi-party computation]
  - **authors:** Baolei Zhang, Minghong Fang, Zhuqing Liu, Biao Yi, Peizhao Zhou, Yuan Wang, Tong Li, Zheli Liu
  - **institution:** Nankai University, University of Louisville, University of North Texas
  - **link:** https://arxiv.org/pdf/2512.17254
  - **Simple LLM Summary:** The paper proposes ABBR, a practical framework for federated learning that combines Byzantine-robust aggregation with privacy-preserving techniques. Its core innovation uses dimensionality reduction to speed up the private computation of complex filtering rules, minimizing performance overhead. The evaluation shows that ABBR runs significantly faster with minimal communication cost while maintaining strong resilience against attacks.

- **[arXiv251222] Democratizing Scalable Cloud Applications: Transactional Stateful Functions on Streaming Dataflows**
  - **tags:** [sys], [distributed systems], [streaming dataflow, stateful functions, serializable transactions, fault tolerance, serverless, Apache Flink]
  - **authors:** Kyriakos Psarakis
  - **institution:** Unknown (Inference from email domain not possible from provided text)
  - **link:** https://arxiv.org/pdf/2512.17429
  - **Simple LLM Summary:** This thesis introduces Stateflow and Styx, a programming model and distributed engine that compiles cloud applications into transactional stateful dataflow graphs. It demonstrates that this approach enables scalable, fault-tolerant, serializable transactions on streaming dataflows, significantly improving performance and programmability over prior systems.

- **[arXiv251222] Fixed-Priority and EDF Schedules for ROS2 Graphs on Uniprocessor**
  - **tags:** [sys], [real-time systems], [fixed-priority scheduling, EDF scheduling, DAG task models, events executor, ROS2, uniprocessor]
  - **authors:** Oren Bell, Harun Teper, Mario Günzel, Chris Gill, Jian-Jia Chen
  - **institution:** Washington University in St Louis, TU Dortmund University
  - **link:** https://arxiv.org/pdf/2512.16926
  - **Simple LLM Summary:** This paper proposes a novel scheduling method for ROS2 applications by using an events executor to implement fixed-job-level-priority schedulers for arbitrary Directed Acyclic Graphs (DAGs) on uniprocessor systems. It demonstrates that ROS2 applications can be mapped to traditional real-time DAG task models, and shows that the implementation generates schedules equivalent to a conventional fixed-priority DAG scheduler, thereby bridging the gap between real-time systems theory and ROS2 scheduling.

- **[arXiv251222] The HEAL Data Platform**
  - **tags:** [sys], [data platform], [Gen3 platform, federated system, FAIR principles, cloud-based compute, mesh architecture, persistent identifiers, STRIDES]
  - **authors:** Brienna M. Larrick, L. Philip Schumm, Mingfei Shao, Craig Barnes, Anthony Juehne, Hara Prasad Juvvla, Michael B. Kranz, Michael Lukowski, Clint Malson, Jessica N. Mazerik, Christopher G. Meyer, Jawad Qureshi, Erin Spaniol, Andrea Tentner, Alexander VanTol, Peter Vassilatos, Sara Volk de Garcia, Robert L. Grossman
  - **institution:** University of Chicago
  - **link:** https://arxiv.org/pdf/2512.17506
  - **Simple LLM Summary:** The paper presents the HEAL Data Platform, a cloud-based federated system built on the open-source Gen3 platform to serve as a single point of search, discovery, and analysis for data from the NIH HEAL Initiative. It interoperates with multiple data repositories using a mesh architecture to provide rich metadata and secure compute environments. The platform maximizes the value of HEAL data by making it Findable, Accessible, Interoperable, and Reusable (FAIR).

- **[arXiv251222] Scalable Distributed Vector Search via Accuracy Preserving Index Construction**
  - **tags:** [mlsys], [others], [distributed vector index, approximate nearest neighbor search, hierarchical clustering, partition granularity, accuracy-preserving construction]
  - **authors:** Yuming Xu, Qianxi Zhang, Qi Chen, Baotong Lu, Menghao Li, Philip Adams, Mingqin Li, Zengzhong Li, Jing Liu, Cheng Li, Fan Yang
  - **institution:** University of Science and Technology of China, Microsoft Research, Shopify, Microsoft AI and Research
  - **link:** https://arxiv.org/pdf/2512.17264
  - **Simple LLM Summary:** The paper presents SPIRE, a scalable vector index system for Approximate Nearest Neighbor Search (ANNS) that uses a balanced partition granularity and an accuracy-preserving recursive construction to build a multi-level index. It achieves high scalability and throughput by controlling cross-node communication and vector read costs. In experiments with up to 8 billion vectors, SPIRE demonstrates up to 9.64x higher throughput than state-of-the-art systems.

- **[arXiv251222] Adaptive Graph Pruning with Sudden-Events Evaluation for Traffic Prediction using Online Semi-Decentralized ST-GNNs**
  - **tags:** [mlsys], [others], [adaptive graph pruning, online semi-decentralized training, spatio-temporal graph neural networks (ST-GNNs), federated learning, gossip learning, sudden event prediction accuracy (SEPA)]
  - **authors:** Ivan Kralj, Lodovico Giaretta, Gordan Ježić, Ivana Podnar Žarko, Šarūnas Girdzijauskas
  - **institution:** University of Zagreb, Faculty of Electrical Engineering and Computing; RISE Research Institutes of Sweden; KTH Royal Institute of Technology
  - **link:** https://arxiv.org/pdf/2512.17352
  - **Simple LLM Summary:** This paper proposes an adaptive graph pruning algorithm for online semi-decentralized ST-GNNs to reduce communication overhead in traffic prediction across distributed edge nodes (cloudlets). It also introduces a novel event-focused metric, SEPA, to evaluate responsiveness to sudden traffic changes. The method maintains prediction accuracy while significantly lowering communication costs, demonstrating effective communication reduction without compromising on critical event responsiveness.

- **[arXiv251222] LLM-HPC++: Evaluating LLM-Generated Modern C++ and MPI+OpenMP Codes for Scalable Mandelbrot Set Computation**
  - **tags:** [mlsys], [llm inference], [C++ parallel algorithms, OpenMP, MPI, code generation, scalability evaluation, Mandelbrot set]
  - **authors:** Patrick Diehl, Noujoud Nader, Deepti Gupta
  - **institution:** Los Alamos National Laboratory, Louisiana State University, Texas A&M University-Central Texas
  - **link:** https://arxiv.org/pdf/2512.17023
  - **Simple LLM Summary:** This paper systematically evaluates large language models (LLMs) like ChatGPT and Claude on generating correct and scalable parallel C++ code using OpenMP and MPI for Mandelbrot set computation. The core method involves prompting LLMs to generate implementations, then compiling and executing the code to assess correctness and performance. The main conclusion is that ChatGPT-4 and ChatGPT-5 achieve strong syntactic precision and scalable performance in this HPC code generation task.

- **[arXiv251222] Torrent: A Distributed DMA for Efficient and Flexible Point-to-Multipoint Data Movement**
  - **tags:** [mlsys], [cluster infrastructure], [distributed DMA, Chainwrite, point-to-multipoint, Network-on-Chip, AXI protocol, scheduling algorithms]
  - **authors:** Yunhao Deng, Fanchen Kong, Xiaoling Yi, Ryan Antonio, Marian Verhelst
  - **institution:** KU Leuven
  - **link:** https://arxiv.org/pdf/2512.17589
  - **Simple LLM Summary:** This paper introduces Torrent, a distributed DMA architecture that performs efficient point-to-multipoint data transfers by forming logical chains over a Network-on-Chip, a method called Chainwrite, without modifying the hardware or protocol. It demonstrates significant performance improvements, achieving up to a 7.88x speedup over unicast, while maintaining minimal area and power overhead.

- **[arXiv251222] Enabling Disaggregated Multi-Stage MLLM Inference via GPU-Internal Scheduling and Resource Sharing**
  - **tags:** [mlsys], [multi-modal inference], [GPU-internal scheduling, resource sharing, collaborative multi-GPU video decoding, logically decoupled execution, FlashCodec, UnifiedServe]
  - **authors:** Lingxiao Zhao, Haoran Zhou, Yuezhi Che, Dazhao Cheng
  - **institution:** Wuhan University
  - **link:** https://arxiv.org/pdf/2512.17574
  - **Simple LLM Summary:** The paper proposes FlashCodec and UnifiedServe, a framework that optimizes MLLM serving by accelerating video decoding and enabling resource sharing between the vision encoding and LLM inference stages. This approach reduces latency and increases throughput by eliminating inter-stage blocking and improving GPU utilization. The system achieves significantly higher throughput and can serve more requests compared to existing state-of-the-art systems.

- **[arXiv251222] Asymptotic behaviour of galactic small-scale dynamos at modest magnetic Prandtl number**
  - **tags:** [sys], [astrophysics], [magnetohydrodynamics (MHD), Pencil Code, Astaroth, GPU acceleration, magnetic Prandtl number, supernova-driven dynamo]
  - **authors:** Frederick A. Gent, Mordecai-Mark Mac Low, Maarit J. Korpi-Lagg, Touko Puro, Matthias Reinhardt
  - **institution:** Nordita, KTH Royal Institute of Technology and Stockholm University, Aalto University, Newcastle University, American Museum of Natural History
  - **link:** https://arxiv.org/pdf/2512.17885
  - **Simple LLM Summary:** This paper uses high-resolution magnetohydrodynamic simulations with the GPU-accelerated Pencil Code (Astaroth) to model a supernova-driven galactic dynamo. The main finding is that the turbulent magnetic field strength from the small-scale dynamo reaches an asymptotic limit at a modest magnetic Prandtl number of only a few hundred, which is far below the physical values in the interstellar medium. This result helps characterize the small-scale magnetic field for inclusion in larger galactic models.


**cs.AI/cs.LG contains "reinforcement learning" total: 22**
- [arXiv251222] UniRel-R1: RL-tuned LLM Reasoning for Knowledge Graph Relational Question Answering [link](https://arxiv.org/pdf/2512.17043)
- [arXiv251222] Assessing Long-Term Electricity Market Design for Ambitious Decarbonization Targets using Multi-Agent Reinforcement Learning [link](https://arxiv.org/pdf/2512.17444)
- [arXiv251222] Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows [link](https://arxiv.org/pdf/2512.16969)
- [arXiv251222] Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs [link](https://arxiv.org/pdf/2512.17008)
- [arXiv251222] A Theoretical Analysis of State Similarity Between Markov Decision Processes [link](https://arxiv.org/pdf/2512.17265)
- [arXiv251222] Understanding Generalization in Role-Playing Models via Information Theory [link](https://arxiv.org/pdf/2512.17270)
- [arXiv251222] GB-DQN: Gradient Boosted DQN Models for Non-stationary Reinforcement Learning [link](https://arxiv.org/pdf/2512.17034)
- [arXiv251222] Value Under Ignorance in Universal Artificial Intelligence [link](https://arxiv.org/pdf/2512.17086)
- [arXiv251222] Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making [link](https://arxiv.org/pdf/2512.17091)
- [arXiv251222] CheXPO-v2: Preference Optimization for Chest X-ray VLMs with Knowledge Graph Consistency [link](https://arxiv.org/pdf/2512.17213)
- [arXiv251222] Large Language Models as Pokémon Battle Agents: Strategic Play and Content Generation [link](https://arxiv.org/pdf/2512.17308)
- [arXiv251222] Reinforcement Learning for Self-Improving Agent with Skill Library [link](https://arxiv.org/pdf/2512.17102)
- [arXiv251222] MMRAG-RFT: Two-stage Reinforcement Fine-tuning for Explainable Multi-modal Retrieval-augmented Generation [link](https://arxiv.org/pdf/2512.17194)
- [arXiv251222] Conservative Bias in Multi-Teacher Learning: Why Agents Prefer Low-Reward Advisors [link](https://arxiv.org/pdf/2512.17180)
- [arXiv251222] Learning Safe Autonomous Driving Policies Using Predictive Safety Representations [link](https://arxiv.org/pdf/2512.17586)
- [arXiv251222] SCOPE: Sequential Causal Optimization of Process Interventions [link](https://arxiv.org/pdf/2512.17629)
- [arXiv251222] Trust-Region Adaptive Policy Optimization [link](https://arxiv.org/pdf/2512.17636)
- [arXiv251222] About Time: Model-free Reinforcement Learning with Timed Reward Machines [link](https://arxiv.org/pdf/2512.17637)
- [arXiv251222] Planning as Descent: Goal-Conditioned Latent Trajectory Synthesis in Learned Energy Landscapes [link](https://arxiv.org/pdf/2512.17846)
- [arXiv251222] AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning [link](https://arxiv.org/pdf/2512.17853)
- [arXiv251222] Distributionally Robust Imitation Learning: Layered Control Architecture for Certifiable Autonomy [link](https://arxiv.org/pdf/2512.17899)
- [arXiv251222] HydroGym: A Reinforcement Learning Platform for Fluid Dynamics [link](https://arxiv.org/pdf/2512.17534)

**cs.AI/cs.LG contains "accelerate" total: 8**
- [arXiv251222] Fair Voting Methods as a Catalyst for Democratic Resilience: A Trilogy on Legitimacy, Impact and AI Safeguarding [link](https://arxiv.org/pdf/2512.17461)
- [arXiv251222] AutoMetrics: Approximate Human Judgements with Automatically Generated Evaluators [link](https://arxiv.org/pdf/2512.17267)
- [arXiv251222] MINPO: Memory-Informed Neural Pseudo-Operator to Resolve Nonlocal Spatiotemporal Dynamics [link](https://arxiv.org/pdf/2512.17273)
- [arXiv251222] LibriVAD: A Scalable Open Dataset with Deep Learning Benchmarks for Voice Activity Detection [link](https://arxiv.org/pdf/2512.17281)
- [arXiv251222] Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making [link](https://arxiv.org/pdf/2512.17091)
- [arXiv251222] SDUM: A Scalable Deep Unrolled Model for Universal MRI Reconstruction [link](https://arxiv.org/pdf/2512.17137)
- [arXiv251222] M2RU: Memristive Minion Recurrent Unit for Continual Learning at the Edge [link](https://arxiv.org/pdf/2512.17299)
- [arXiv251222] Generative Multi-Objective Bayesian Optimization with Scalable Batch Evaluations for Sample-Efficient De Novo Molecular Design [link](https://arxiv.org/pdf/2512.17659)

## 2025-12-23

**cs.DC total: 21**

- **[arXiv251223] QAISim: A Toolkit for Modeling and Simulation of AI in Quantum Cloud Computing Environments**
  - **tags:** [mlsys], [cluster infrastructure], [quantum reinforcement learning, parameterized quantum circuits, policy gradient, deep q-learning, quantum cloud computing]
  - **authors:** Irwindeep Singh, Sukhpal Singh Gill, Jinzhao Sun, Jan Mol
  - **institution:** Indian Institute of Technology Jodhpur, Queen Mary University of London
  - **link:** https://arxiv.org/pdf/2512.17918
  - **Simple LLM Summary:** This paper proposes QAISim, a Python toolkit for simulating Quantum Artificial Intelligence models to design resource management policies in quantum cloud environments. It employs quantum reinforcement learning based on parameterized quantum circuits to address the resource allocation problem for IoT applications. The toolkit demonstrates a substantial reduction in model complexity compared to classical counterparts.

- **[arXiv251223] Accelerated Digital Twin Learning for Edge AI: A Comparison of FPGA and Mobile GPU**
  - **tags:** [mlsys], [GPU kernels], [FPGA acceleration, mobile GPU, model recovery, digital twin, edge AI, performance-per-watt, DRAM footprint]
  - **authors:** Bin Xu, Ayan Banerjee, Midhat Urooj, Sandeep K.S. Gupta
  - **institution:** Arizona State University
  - **link:** https://arxiv.org/pdf/2512.17941
  - **Simple LLM Summary:** The paper presents a digital twin learning framework accelerated on reconfigurable hardware (FPGA) and compares it with a mobile GPU implementation. The FPGA approach achieves superior performance-per-watt and lower memory footprint compared to both mobile GPU and cloud GPU baselines, making it suitable for resource-efficient, real-time edge AI in precision healthcare applications.

- **[arXiv251223] Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems**
  - **tags:** [mlsys], [multi-modal inference], [pipeline parallelism, asynchronous execution, message-queue decoupling, graph compilation, mixed-precision quantization, kernel fusion, silence-detection, transformer]
  - **authors:** Eren Caglar, Amirkia Rafiei Oskooei, Mehmet Kutanoglu, Mustafa Keles, Mehmet S. Aktas
  - **institution:** Yildiz Technical University, Aktif Investment Bank Inc.
  - **link:** https://arxiv.org/pdf/2512.18318
  - **Simple LLM Summary:** This paper proposes an asynchronous pipeline-parallel Transformer framework for real-time multilingual lip synchronization, decoupling modules via message queues and optimizing inference with techniques like graph compilation and quantization. It concludes that this architecture significantly reduces latency and improves resource efficiency compared to sequential pipelines, making it suitable for AIoT communication systems.

- **[arXiv251223] Fast Online Digital Twinning on FPGA for Mission Critical Applications**
  - **tags:** [mlsys], [others], [FPGA acceleration, gated recurrent units (GRU), dense layers, model recovery (MR), digital twin, edge computing, hardware acceleration]
  - **authors:** Bin Xu, Ayan Banerjee, Sandeep K. S. Gupta
  - **institution:** Arizona State University
  - **link:** https://arxiv.org/pdf/2512.17942
  - **Simple LLM Summary:** This paper introduces an FPGA-accelerated digital twinning framework that offloads key neural components like GRUs and dense layers to reconfigurable hardware for parallel execution. It demonstrates that this approach enables real-time, low-latency predictive modeling for mission-critical applications, operating significantly faster than human reaction times and proving viable for deployment on edge platforms.

- **[arXiv251223] Byzantine Fault-Tolerant Multi-Agent System for Healthcare: A Gossip Protocol Approach to Secure Medical Message Propagation**
  - **tags:** [mlsys], [fault-tolerance], [byzantine fault tolerance, gossip protocol, cryptographic signatures, consensus algorithms, multi-agent system]
  - **authors:** Nihir Chadderwala
  - **institution:** Independent researcher (based on email domain)
  - **link:** https://arxiv.org/pdf/2512.17913
  - **Simple LLM Summary:** This paper proposes a Byzantine fault-tolerant multi-agent system for healthcare that uses a gossip protocol for decentralized message propagation and a consensus algorithm to tolerate malicious nodes. The system integrates cryptographic validation to secure medical messages and maintains 100% consensus accuracy even with up to 33% faulty nodes, providing a secure framework for collaborative medical decision-making in untrusted environments.

- **[arXiv251223] Faster Vertex Cover Algorithms on GPUs with Component-Aware Parallel Branching**
  - **tags:** [sys], [graph algorithms], [GPU, vertex cover, branch-and-reduce, component-aware branching, load balancing, non-tail-recursive branching]
  - **authors:** Hussein Amro, Basel Fakhri, Amer E. Mouawad, Izzat El Hajj
  - **institution:** American University of Beirut, University of Waterloo
  - **link:** https://arxiv.org/pdf/2512.18334
  - **Simple LLM Summary:** This paper proposes a new GPU algorithm for the vertex cover problem that improves scalability by detecting when the graph splits into independent components and branching on them separately to avoid redundant work. It overcomes the load balancing challenge of non-tail-recursive branches by delegating post-processing to the last descendant. The method significantly outperforms the state-of-the-art GPU solution, reducing runtime from hours to seconds.

- **[arXiv251223] TraCT: Disaggregated LLM Serving with CXL Shared Memory KV Cache at Rack-Scale**
  - **tags:** [mlsys], [llm inference], [CXL shared memory, KV cache, disaggregated serving, two-tier synchronization, Dynamo framework]
  - **authors:** Dongha Yoon, Younghoon Min, Hoshik Kim, Sam H. Noh, Jongryool Kim
  - **institution:** Virginia Tech, SK Hynix America
  - **link:** https://arxiv.org/pdf/2512.18194
  - **Simple LLM Summary:** This paper presents TraCT, a system for disaggregated LLM serving that uses CXL shared memory as a direct transfer substrate and rack-wide KV cache, eliminating network hops. It addresses challenges like synchronization on non-coherent CXL memory with software mechanisms like a two-tier inter-node synchronization scheme. The implementation shows significant improvements in latency and throughput compared to RDMA-based baselines.

- **[arXiv251223] Constrained Cuts, Flows, and Lattice-Linearity**
  - **tags:** [sys], [combinatorial optimization], [lattice-linearity, min-cuts, max-flow, distributive lattices, parallel algorithms, poset slicing]
  - **authors:** Robert Streit, Vijay K. Garg
  - **institution:** The University of Texas at Austin
  - **link:** https://arxiv.org/pdf/2512.18141
  - **Simple LLM Summary:** The paper introduces methods using lattice-linear predicates and poset slicing to compute min-cuts under additional constraints in capacitated directed graphs. It shows that while the general problem is NP-hard, these techniques enable parallel algorithms and exact solutions with better complexity than exhaustive search for certain constraint types. The work also presents new concepts like k-transition predicates to improve parallel complexity analyses.

- **[arXiv251223] ACE-Sync: An Adaptive Cloud-Edge Synchronization Framework for Communication-Efficient Large-Scale Distributed Model Training**
  - **tags:** [mlsys], [cluster infrastructure], [attention-based gradient importance predictor, differentiated parameter compression, knapsack-based optimization, hierarchical cloud-edge coordination, residual-based error compensation, device clustering]
  - **authors:** Yi Yang, Ziyu Lin, Liesheng Wei
  - **institution:** Sichuan Agricultural University, Google LLC, College of Information Technology, Shanghai Ocean University
  - **link:** https://arxiv.org/pdf/2512.18127
  - **Simple LLM Summary:** ACE-Sync is an adaptive synchronization framework for distributed model training that uses an attention-based predictor to select important gradients and a knapsack-based strategy to compress them under bandwidth constraints. It significantly reduces communication overhead by 60% while maintaining model accuracy, demonstrating an efficient solution for large-scale cloud-edge training.

- **[arXiv251223] Efficient Multi-Adapter LLM Serving via Cross-Model KV-Cache Reuse with Activated LoRA**
  - **tags:** [mlsys], [llm inference], [KV-cache reuse, Activated LoRA (aLoRA), vLLM, base-aligned block hashing, activation-aware masking]
  - **authors:** Allison Li, Kristjan Greenewald, Thomas Parnell, Navid Azizan
  - **institution:** Massachusetts Institute of Technology, IBM Research
  - **link:** https://arxiv.org/pdf/2512.17910
  - **Simple LLM Summary:** This paper introduces a new LLM serving engine that enables efficient cross-model KV-cache reuse between a base model and its LoRA adapters, called Activated LoRA (aLoRA). By extending the vLLM framework with base-aligned block hashing and activation-aware masking, it drastically reduces recomputation overhead when switching adapters during multi-turn inference. The evaluation shows significant latency reductions, bridging parameter-efficient adaptation with high-performance serving.

- **[arXiv251223] Simulations between Strongly Sublinear MPC and Node-Capacitated Clique**
  - **tags:** [sys], [distributed computing], [MPC, Node-Capacitated Clique, round-preserving simulation, strongly sublinear regime]
  - **authors:** Philipp Schneider, Julian Werthmann
  - **institution:** CISPA Helmholtz Center for Information Security, Paderborn University
  - **link:** https://arxiv.org/pdf/2512.19326
  - **Simple LLM Summary:** This paper studies round-preserving simulations between the strongly sublinear Massively Parallel Computation (MPC) model and the Node-Capacitated Clique (NCC) model when total resources are matched. It provides techniques for efficient simulations with constant overhead and proves impossibility results that show the limitations of these simulations are inherent.

- **[arXiv251223] Evidential Trust-Aware Model Personalization in Decentralized Federated Learning for Wearable IoT**
  - **tags:** [mlsys], [others], [decentralized federated learning, evidential deep learning, Dirichlet distribution, epistemic uncertainty, trust-aware aggregation, model personalization]
  - **authors:** Murtaza Rangwala, Richard O. Sinnott, Rajkumar Buyya
  - **institution:** The University of Melbourne
  - **link:** https://arxiv.org/pdf/2512.19131
  - **Simple LLM Summary:** The paper presents Murmura, a framework that uses evidential deep learning to enable trust-aware model personalization in decentralized federated learning. It uses epistemic uncertainty from Dirichlet-based models to measure peer compatibility, allowing nodes to selectively collaborate. The method reduces performance degradation in non-IID conditions and achieves faster convergence compared to baselines.

- **[arXiv251223] QoS-Aware Load Balancing in the Computing Continuum via Multi-Player Bandits**
  - **tags:** [mlsys], [cluster infrastructure], [Multi-Player Multi-Armed Bandit, Kernel Density Estimation, QoS-aware load balancing, Kubernetes, K3s]
  - **authors:** Ivan Čilić, Ivana Podnar Žarko, Pantelis Frangoudis, Schahram Dustdar
  - **institution:** University of Zagreb, TU Wien
  - **link:** https://arxiv.org/pdf/2512.18915
  - **Simple LLM Summary:** The paper proposes QEdgeProxy, a decentralized load balancer for the Computing Continuum that uses a Multi-Player Multi-Armed Bandit approach with Kernel Density Estimation to autonomously select service instances, maximizing per-client QoS satisfaction. It demonstrates superior performance over baseline methods in adapting to dynamic conditions like load surges and instance changes in a Kubernetes-based testbed.

- **[arXiv251223] Faster Distributed Inference-Only Recommender Systems via Bounded Lag Synchronous Collectives**
  - **tags:** [mlsys], [others], [bounded lag synchronous, alltoallv, distributed inference, embedding table lookup, PyTorch Distributed]
  - **authors:** Kiril Dichev, Filip Pawlowski, Albert-Jan Yzelman
  - **institution:** Huawei Technologies Switzerland AG
  - **link:** https://arxiv.org/pdf/2512.19342
  - **Simple LLM Summary:** The paper proposes a bounded lag synchronous (BLS) version of the all-to-all collective operation to reduce synchronization overhead in distributed deep learning recommender model inference. This method allows slower processes to lag behind within a bounded number of iterations, preserving accuracy. The technique improves latency and throughput for unbalanced inference runs, effectively masking process delays in the best case.

- **[arXiv251223] Snowveil: A Framework for Decentralised Preference Discovery**
  - **tags:** [sys], [decentralised governance], [gossip-based protocol, potential function, submartingale theory, Constrained Hybrid Borda (CHB), Positive Responsiveness]
  - **authors:** Grammateia Kotsialou
  - **institution:** King's College London
  - **link:** https://arxiv.org/pdf/2512.18444
  - **Simple LLM Summary:** The paper proposes Snowveil, a gossip-based protocol for Decentralised Preference Discovery where voters iteratively sample random peers to converge on a collective outcome. It demonstrates the framework's modularity with a novel aggregation rule and proves the system almost surely converges to a stable winner in finite time. The work shows how stable consensus can emerge from diverse preferences in large, decentralised systems.

- **[arXiv251223] A Real-Time Digital Twin for Adaptive Scheduling**
  - **tags:** [sys], [cluster scheduling], [digital twin, discrete-event simulator, what-if evaluation, PBS scheduler, adaptive scheduling]
  - **authors:** Yihe Zhang, Yash Kurkure, Yiheng Tao, Michael E. Papka, Zhiling Lan
  - **institution:** University of Illinois Chicago, Argonne National Laboratory
  - **link:** https://arxiv.org/pdf/2512.18894
  - **Simple LLM Summary:** This paper presents SchedTwin, a real-time digital twin for adaptive HPC scheduling that uses a high-fidelity discrete-event simulator to perform rapid what-if evaluations of multiple scheduling policies and dynamically select the best one. Preliminary results show that SchedTwin consistently outperforms static scheduling policies while maintaining low overhead, demonstrating that real-time digital twins offer a practical path toward adaptive scheduling.

- **[arXiv251223] L4: Low-Latency and Load-Balanced LLM Serving via Length-Aware Scheduling**
  - **tags:** [mlsys], [llm inference], [length-aware scheduling, dynamic programming, decentralized load balancing, multi-instance serving, continuous batching]
  - **authors:** Yitao Yuan, Chenqi Zhao, Bohan Zhao, Zane Cao, Yongchao He, Wenfei Wu
  - **institution:** Peking University, ScitiX AI
  - **link:** https://arxiv.org/pdf/2512.19179
  - **Simple LLM Summary:** The paper introduces L4, a runtime system that improves LLM serving by dynamically rescheduling requests across multiple instances based on their length. It partitions instances into length-specialized groups to reduce heterogeneity in attention computation, using a dynamic programming algorithm for optimal grouping and decentralized load balancing. The evaluation shows L4 significantly reduces latency and increases throughput compared to existing multi-instance schedulers.

- **[arXiv251223] Timely Parameter Updating in Over-the-Air Federated Learning**
  - **tags:** [mlsys], [others], [over-the-air computation, federated learning, parameter selection, FAIR-k, gradient staleness, Markov analysis, convergence rate]
  - **authors:** Jiaqi Zhu, Zhongyuan Zhao, Xiao Li, Ruihao Du, Shi Jin, Howard H.Yang
  - **institution:** Zhejiang University, Beijing University of Posts and Telecommunications, Southeast University
  - **link:** https://arxiv.org/pdf/2512.19103
  - **Simple LLM Summary:** This paper proposes the FAIR-k algorithm for Over-the-Air Federated Learning, which selects the most impactful subset of gradients for transmission by balancing update timeliness and gradient magnitude. The analysis shows that FAIR-k accelerates convergence and enhances communication efficiency by reducing parameter staleness and enabling longer local training periods.

- **[arXiv251223] Remoe: Towards Efficient and Low-Cost MoE Inference in Serverless Computing**
  - **tags:** [mlsys], [llm inference], [Similar Prompts Searching (SPS), Main Model Pre-allocation (MMP), Lagrangian duality, Longest Processing Time (LPT), expert offloading, heterogeneous inference]
  - **authors:** Wentao Liu, Yuhao Hu, Ruiting Zhou, Baochun Li, Ne Wang
  - **institution:** Southeast University, University of Toronto, The Hong Kong Polytechnic University
  - **link:** https://arxiv.org/pdf/2512.18674
  - **Simple LLM Summary:** The paper proposes Remoe, a heterogeneous inference system for Mixture-of-Experts (MoE) models in serverless computing that assigns non-expert modules to GPUs and offloads infrequently activated experts to CPU-based serverless functions. It uses techniques like semantic similarity prediction for expert activation and joint memory-replica optimization to manage resources. The system reduces inference cost by up to 57% and cold start latency by 47% compared to baselines.

- **[arXiv251223] RAPID-LLM: Resilience-Aware Performance analysis of Infrastructure for Distributed LLM Training and Inference**
  - **tags:** [mlsys], [llm training], [performance modeling, hybrid parallelism, congestion-aware routing, tile-based latency model, ZeRO/FDSP sharding, Astra-Sim, DeepFlow, Chakra traces]
  - **authors:** George Karfakis, Faraz Tahmasebi, Binglu Chen, Lime Yao, Saptarshi Mitra, Tianyue Pan, Hyoukjun Kwon, Puneet Gupta
  - **institution:** University of California, Los Angeles
  - **link:** https://arxiv.org/pdf/2512.19606
  - **Simple LLM Summary:** RAPID-LLM is a performance modeling framework that combines a DeepFlow-based frontend for generating hardware-aware operator traces with an extended Astra-Sim backend for simulating execution on network topologies. It accurately predicts LLM training and inference performance, enabling exploration of hybrid parallelism, fault sensitivity, and hardware design variants.

- **[arXiv251223] EuroHPC SPACE CoE: Redesigning Scalable Parallel Astrophysical Codes for Exascale**
  - **tags:** [sys], [astrophysics simulation], [exascale computing, code re-engineering, parallel programming, high-performance data analysis, machine learning, visualization]
  - **authors:** Nitin Shukla, Alessandro Romeo, Caterina Caravita, Lubomir Riha, Ondrej Vysocky, Petr Strakos, Milan Jaros, João Barbosa, Radim Vavrik, Andrea Mignone, Marco Rossazza, Stefano Truzzi, Vittoria Berta, Iacopo Colonnelli, Doriana Medić, Elisabetta Boella, Daniele Gregori, Eva Sciacca, Luca Tornatore, Giuliano Taffoni, Pranab J. Deka, Fabio Bacchini, Rostislav-Paul Wilhelm, Georgios Doulis, Khalil Pierre, Luciano Rezzolla, Tine Colman, Benoît Commerçon, Othman Bouizi, Matthieu Kuhn, Erwan Raffin, Marc Sergent, Robert Wissing, Guillermo Marin, Klaus Dolag, Geray S. Karademir, Gino Perna, Marisa Zanotti, Sebastian Trujillo-Gomez
  - **institution:** CINECA, IT4Innovations, VSB-TU Ostrava, University of Turin, E4 COMPUTER ENGINEERING SpA, INAF, KU Leuven, Goethe-Universität, CRAL, CNRS, ENS Lyon, Eviden, University of Oslo, Barcelona Supercomputing Center, ENGINSOFT SpA, Ludwig-Maximilians-Universität, Heidelberg Institute for Theoretical Studies
  - **link:** https://arxiv.org/pdf/2512.18883
  - **Simple LLM Summary:** The SPACE Centre of Excellence aims to redesign legacy astrophysical simulation codes for exascale systems by adopting innovative programming paradigms and software solutions. It concludes that a collaborative effort involving scientists, developers, and hardware manufacturers is essential to overcome architectural complexities and enable high-performance data analysis for future simulations and observations.


**cs.AI/cs.LG contains "reinforcement learning" total: 40**
- [arXiv251223] Graph-O1 : Monte Carlo Tree Search with Reinforcement Learning for Text-Attributed Graph Reasoning [link](https://arxiv.org/pdf/2512.17912)
- [arXiv251223] On Swarm Leader Identification using Probing Policies [link](https://arxiv.org/pdf/2512.18146)
- [arXiv251223] ReGal: A First Look at PPO-based Legal AI for Judgment Prediction and Summarization in India [link](https://arxiv.org/pdf/2512.18014)
- [arXiv251223] NystagmusNet: Explainable Deep Learning for Photosensitivity Risk Prediction [link](https://arxiv.org/pdf/2512.17943)
- [arXiv251223] NL2CA: Auto-formalizing Cognitive Decision-Making from Natural Language Using an Unsupervised CriticNL2LTL Framework [link](https://arxiv.org/pdf/2512.18189)
- [arXiv251223] Monitoring Monitorability [link](https://arxiv.org/pdf/2512.18311)
- [arXiv251223] Unifying Causal Reinforcement Learning: Survey, Taxonomy, Algorithms and Applications [link](https://arxiv.org/pdf/2512.18135)
- [arXiv251223] Adaptive Agents in Spatial Double-Auction Markets: Modeling the Emergence of Industrial Symbiosis [link](https://arxiv.org/pdf/2512.17979)
- [arXiv251223] Stable and Efficient Single-Rollout RL for Multimodal Reasoning [link](https://arxiv.org/pdf/2512.18215)
- [arXiv251223] Reinforcement Learning Position Control of a Quadrotor Using Soft Actor-Critic (SAC) [link](https://arxiv.org/pdf/2512.18333)
- [arXiv251223] Embedded Safety-Aligned Intelligence via Differentiable Internal Alignment Embeddings [link](https://arxiv.org/pdf/2512.18309)
- [arXiv251223] Trustworthy and Explainable Deep Reinforcement Learning for Safe and Energy-Efficient Process Control: A Use Case in Industrial Compressed Air Systems [link](https://arxiv.org/pdf/2512.18317)
- [arXiv251223] Dynamic Entropy Tuning in Reinforcement Learning Low-Level Quadcopter Control: Stochasticity vs Determinism [link](https://arxiv.org/pdf/2512.18336)
- [arXiv251223] On the Universality of Transformer Architectures; How Much Attention Is Enough? [link](https://arxiv.org/pdf/2512.18445)
- [arXiv251223] Scaling up Stability: Reinforcement Learning for Distributed Control of Networked Systems in the Space of Stabilizing Policies [link](https://arxiv.org/pdf/2512.18540)
- [arXiv251223] Toward Training Superintelligent Software Agents through Self-Play SWE-RL [link](https://arxiv.org/pdf/2512.18552)
- [arXiv251223] Vox Deorum: A Hybrid LLM Architecture for 4X / Grand Strategy Game AI -- Lessons from Civilization V [link](https://arxiv.org/pdf/2512.18564)
- [arXiv251223] Trajectory Planning for UAV-Based Smart Farming Using Imitation-Based Triple Deep Q-Learning [link](https://arxiv.org/pdf/2512.18604)
- [arXiv251223] A Multi-agent Text2SQL Framework using Small Language Models and Execution Feedback [link](https://arxiv.org/pdf/2512.18622)
- [arXiv251223] LLM-CAS: Dynamic Neuron Perturbation for Real-Time Hallucination Correction [link](https://arxiv.org/pdf/2512.18623)
- [arXiv251223] Demonstration-Guided Continual Reinforcement Learning in Dynamic Environments [link](https://arxiv.org/pdf/2512.18670)
- [arXiv251223] A Theoretical Lens for RL-Tuned Language Models via Energy-Based Models [link](https://arxiv.org/pdf/2512.18730)
- [arXiv251223] InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search [link](https://arxiv.org/pdf/2512.18745)
- [arXiv251223] Gaussian-Mixture-Model Q-Functions for Policy Iteration in Reinforcement Learning [link](https://arxiv.org/pdf/2512.18763)
- [arXiv251223] CORE: Concept-Oriented Reinforcement for Bridging the Definition-Application Gap in Mathematical Reasoning [link](https://arxiv.org/pdf/2512.18857)
- [arXiv251223] Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage Framework for Long Chain-of-Thought Synthesis and Selection [link](https://arxiv.org/pdf/2512.18956)
- [arXiv251223] Scaling Online Distributionally Robust Reinforcement Learning: Sample-Efficient Guarantees with General Function Approximation [link](https://arxiv.org/pdf/2512.18957)
- [arXiv251223] ORPR: An OR-Guided Pretrain-then-Reinforce Learning Model for Inventory Management [link](https://arxiv.org/pdf/2512.19001)
- [arXiv251223] Tool-Augmented Hybrid Ensemble Reasoning with Distillation for Bilingual Mathematical Problem Solving [link](https://arxiv.org/pdf/2512.19093)
- [arXiv251223] First-Order Representation Languages for Goal-Conditioned RL [link](https://arxiv.org/pdf/2512.19355)
- [arXiv251223] Interpretable Hybrid Deep Q-Learning Framework for IoT-Based Food Spoilage Prediction with Synthetic Data Generation and Hardware Validation [link](https://arxiv.org/pdf/2512.19361)
- [arXiv251223] Learning General Policies with Policy Gradient Methods [link](https://arxiv.org/pdf/2512.19366)
- [arXiv251223] LacaDM: A Latent Causal Diffusion Model for Multiobjective Reinforcement Learning [link](https://arxiv.org/pdf/2512.19516)
- [arXiv251223] CARE What Fails: Contrastive Anchored-REflection for Verifiable Multimodal [link](https://arxiv.org/pdf/2512.19554)
- [arXiv251223] LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller [link](https://arxiv.org/pdf/2512.19576)
- [arXiv251223] Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies [link](https://arxiv.org/pdf/2512.19673)
- [arXiv251223] Scalably Enhancing the Clinical Validity of a Task Benchmark with Physician Oversight [link](https://arxiv.org/pdf/2512.19691)
- [arXiv251223] Reinforcement Learning for Monetary Policy Under Macroeconomic Uncertainty: Analyzing Tabular and Function Approximation Methods [link](https://arxiv.org/pdf/2512.17929)
- [arXiv251223] Structural Reinforcement Learning for Heterogeneous Agent Macroeconomics [link](https://arxiv.org/pdf/2512.18892)
- [arXiv251223] Explicit and Non-asymptotic Query Complexities of Rank-Based Zeroth-order Algorithm on Stochastic Smooth Functions [link](https://arxiv.org/pdf/2512.19104)

**cs.AI/cs.LG contains "accelerate" total: 12**
- [arXiv251223] Conflict-Driven Clause Learning with VSIDS Heuristics for Discrete Facility Layout [link](https://arxiv.org/pdf/2512.18034)
- [arXiv251223] Real-Time Human-Robot Interaction Intent Detection Using RGB-based Pose and Emotion Cues with Cross-Camera Model Generalization [link](https://arxiv.org/pdf/2512.17958)
- [arXiv251223] SoK: Understanding (New) Security Issues Across AI4Code Use Cases [link](https://arxiv.org/pdf/2512.18456)
- [arXiv251223] Trajectory Planning for UAV-Based Smart Farming Using Imitation-Based Triple Deep Q-Learning [link](https://arxiv.org/pdf/2512.18604)
- [arXiv251223] Demonstration-Guided Continual Reinforcement Learning in Dynamic Environments [link](https://arxiv.org/pdf/2512.18670)
- [arXiv251223] Generative Modeling through Spectral Analysis of Koopman Operator [link](https://arxiv.org/pdf/2512.18837)
- [arXiv251223] Merging of Kolmogorov-Arnold networks trained on disjoint datasets [link](https://arxiv.org/pdf/2512.18921)
- [arXiv251223] Context-Aware Initialization for Reducing Generative Path Length in Diffusion Language Models [link](https://arxiv.org/pdf/2512.19004)
- [arXiv251223] Phase-space entropy at acquisition reflects downstream learnability [link](https://arxiv.org/pdf/2512.19223)
- [arXiv251223] An Agentic Framework for Autonomous Materials Computation [link](https://arxiv.org/pdf/2512.19458)
- [arXiv251223] QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models [link](https://arxiv.org/pdf/2512.19526)
- [arXiv251223] The Subject of Emergent Misalignment in Superintelligence: An Anthropological, Cognitive Neuropsychological, Machine-Learning, and Ontological Perspective [link](https://arxiv.org/pdf/2512.17989)
