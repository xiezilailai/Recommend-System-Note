# 20251229-20260104

## 2025-12-29

**cs.DC total: 16**

- **[arXiv251229] LIME:Accelerating Collaborative Lossless LLM Inference on Memory-Constrained Edge Devices**
  - **tags:** [mlsys], [llm inference], [collaborative inference, pipeline parallelism, model offloading, memory adaptation, edge computing]
  - **authors:** Mingyu Sun, Xiao Zhang, Shen Qu, Yan Li, Mengbai Xiao, Yuan Yuan, Dongxiao Yu
  - **institution:** Shandong University
  - **link:** https://arxiv.org/pdf/2512.21835
  - **Simple LLM Summary:** The paper proposes LIME, a collaborative system that uses interleaved pipeline parallelism and model offloading to enable lossless LLM inference across multiple memory-constrained edge devices. It introduces fine-grained scheduling and online memory adaptation to optimize resource use and minimize latency. Experiments show LIME achieves significant speedups over baselines on heterogeneous edge devices without compromising model accuracy.

- **[arXiv251229] Embedding Samples Dispatching for Recommendation Model Training in Edge Environments**
  - **tags:** [mlsys], [others], [embedding cache, parameter server, edge computing, sample dispatching, HybridDis]
  - **authors:** Guopeng Li, Haisheng Tan, Chi Zhang, Hongqiu Ni, Zilong Wang, Xinyue Zhang, Yang Xu, Han Tian
  - **institution:** University of Science and Technology of China (USTC), Hefei University of Technology
  - **link:** https://arxiv.org/pdf/2512.21615
  - **Simple LLM Summary:** The paper proposes ESD, a mechanism that dispatches input embedding samples to edge workers to minimize transmission costs in distributed DLRM training. It introduces HybridDis, a hybrid dispatch method combining an optimal and a heuristic algorithm. Experiments show ESD reduces embedding transmission cost by up to 36.76% and speeds up training by up to 1.74x.

- **[arXiv251229] LEFT-RS: A Lock-Free Fault-Tolerant Resource Sharing Protocol for Multicore Real-Time Systems**
  - **tags:** [sys], [real-time systems], [lock-free protocol, fault tolerance, resource sharing, worst-case response time analysis, multicore]
  - **authors:** Nan Chen, Xiaotian Dai, Tong Cheng, Alan Burns, Iain Bate, Shuai Zhao
  - **institution:** University of York, Sun Yat-sen University
  - **link:** https://arxiv.org/pdf/2512.21701
  - **Simple LLM Summary:** The paper proposes LEFT-RS, a lock-free fault-tolerant resource sharing protocol for multicore real-time systems that allows tasks to concurrently read global resources and enter critical sections in parallel, improving efficiency and fault resilience. It includes a worst-case response time analysis to ensure timing guarantees. Evaluation shows the method significantly outperforms existing approaches, achieving up to an 84.5% average improvement in schedulability.

- **[arXiv251229] Harnessing Data Spaces to Build Intelligent Smart City Infrastructures Across the Cloud-Edge Continuum**
  - **tags:** [mlsys], [others], [data spaces, cloud-edge continuum, edge computing, containerized microservices, AI/ML services, IoT, sensor integration, data privacy]
  - **authors:** Dimitrios Amaxilatis, Themistoklis Sarantakos, Nikolaos Tsironis, Souvik Sengupta, Kostas Ramantas, Jhofre Ojeda
  - **institution:** Spark Works Ltd., IONOS SE, Iquadrat Informática S.L.
  - **link:** https://arxiv.org/pdf/2512.21340
  - **Simple LLM Summary:** The paper presents a real-world use case of intelligent infrastructure monitoring using a data space-enabled cloud-edge framework. It demonstrates how edge computing, containerized microservices, and interoperable data sharing address challenges like sensor integration and data privacy. The implementation highlights the transformative potential of combining AI, edge computing, and data spaces for scalable and resilient smart city ecosystems.

- **[arXiv251229] nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures**
  - **tags:** [mlsys], [llm inference], [e-graph, term rewriting, equality saturation, auto vectorize, auto distribution, auto schedule, buffer-aware codegen, NUMA abstraction, roofline model]
  - **authors:** Hui Guo, Qihang Zheng, Chenghai Huo, Dongliang Guo, Haoqi Yang, Yang Zhang
  - **institution:** Canaan Inc.
  - **link:** https://arxiv.org/pdf/2512.21571
  - **Simple LLM Summary:** The paper presents nncase, an end-to-end compiler framework that uses an e-graph-based term rewriting engine to optimize large language model deployment across heterogeneous memory architectures. It unifies optimization through modules for vectorization, parallel distribution, and scheduling, achieving performance comparable to hand-optimized systems and outperforming other mainstream frameworks.

- **[arXiv251229] Smart IoT-Based Leak Forecasting and Detection for Energy-Efficient Liquid Cooling in AI Data Centers**
  - **tags:** [mlsys], [fault-tolerance], [LSTM, Random Forest, MQTT, InfluxDB, Streamlit, IoT monitoring, probabilistic forecasting]
  - **authors:** Krishna Chaitanya Sunkara, Rambabu Konakanchi
  - **institution:** Oracle, Charles Schwab
  - **link:** https://arxiv.org/pdf/2512.21801
  - **Simple LLM Summary:** This paper presents a smart IoT monitoring system that uses LSTM neural networks for probabilistic leak forecasting and Random Forest classifiers for instant detection in liquid-cooled AI data centers. The system, tested on synthetic data, achieved high accuracy and can forecast leaks 2-4 hours in advance, potentially preventing significant energy waste. The work establishes a proof-of-concept for proactive maintenance to enhance sustainability in data center operations.

- **[arXiv251229] Hyperion: Low-Latency Ultra-HD Video Analytics via Collaborative Vision Transformer Inference**
  - **tags:** [mlsys], [multi-modal inference], [vision transformer, cloud-device collaboration, patch-level importance scoring, dynamic scheduling, weighted ensembling]
  - **authors:** Linyi Jiang, Yifei Zhu, Hao Yin, Bo Li
  - **institution:** Shanghai Jiao Tong University, Tsinghua University, Hong Kong University of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.21730
  - **Simple LLM Summary:** Hyperion is a cloud-device collaborative framework that enables low-latency inference on Ultra-HD videos using vision transformers by identifying critical patches, dynamically adjusting transmission quality, and fusing edge-cloud results. It improves frame processing rate by up to 1.61× and accuracy by up to 20.2% compared to baselines under dynamic network conditions.

- **[arXiv251229] Demystifying ARM SME to Optimize General Matrix Multiplications**
  - **tags:** [mlsys], [GPU kernels], [ARM SME, cache-aware partitioning, data packing, micro-kernels, multi-vector loads, tile registers]
  - **authors:** Chencheng Deng, Weiling Yang, Jianbin Fang, Dezun Dong
  - **institution:** National University of Defense Technology
  - **link:** https://arxiv.org/pdf/2512.21473
  - **Simple LLM Summary:** This paper presents MpGEMM, an open-source library that optimizes General Matrix Multiplication (GEMM) by leveraging ARM's Scalable Matrix Extension (SME) through techniques like cache-aware partitioning, efficient data packing, and specialized micro-kernels. It demonstrates superior performance, achieving an average speedup of 1.23x over Apple's Accelerate library on real-world workloads from models like DeepSeek and LLaMA.

- **[arXiv251229] Proceedings First Workshop on Adaptable Cloud Architectures**
  - **tags:** TBD
  - **authors:** Giuseppe De Palma, Saverio Giallorenzo
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.22054

- **[arXiv251229] DeepCQ: General-Purpose Deep-Surrogate Framework for Lossy Compression Quality Prediction**
  - **tags:** [mlsys], [others], [deep-surrogate model, two-stage design, mixture-of-experts, error-bounded lossy compression, quality prediction]
  - **authors:** Khondoker Mirazul Mumenin, Robert Underwood, Dong Dai, Jinzhen Wang, Sheng Di, Zarija Lukić, Franck Cappello
  - **institution:** University of North Carolina at Charlotte, Argonne National Laboratory, University of Delaware, Lawrence Berkeley National Laboratory
  - **link:** https://arxiv.org/pdf/2512.21433
  - **Simple LLM Summary:** This paper presents DeepCQ, a general-purpose deep-surrogate framework for predicting the quality of lossy compressed scientific data. Its key innovations include a two-stage design separating feature extraction from prediction and a mixture-of-experts approach for handling time-evolving data. The framework demonstrates high predictive accuracy (errors generally under 10%), significantly reducing the computational overhead of quality assessment.

- **[arXiv251229] Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism**
  - **tags:** [mlsys], [llm inference], [mixture-of-experts, disaggregated expert parallelism, fine-grained task scheduling, FinDEP, KV cache, expert group, attention group]
  - **authors:** Xinglin Pan, Shaohuai Shi, Wenxiang Lin, Yuxin Wang, Zhenheng Tang, Wei Wang, Xiaowen Chu
  - **institution:** The Hong Kong University of Science and Technology (Guangzhou), Harbin Institute of Technology, Shenzhen, Hong Kong Baptist University, The Hong Kong University of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.21487
  - **Simple LLM Summary:** This paper proposes FinDEP, a fine-grained task scheduling algorithm for disaggregated expert parallelism (DEP) to improve the inference throughput of MoE models. It partitions computation and communication tasks for better pipelining and solves a scheduling optimization problem. Experiments show FinDEP achieves up to 1.61x higher throughput compared to prior methods.

- **[arXiv251229] Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models**
  - **tags:** [mlsys], [llm inference], [model parallelism, block placement, request routing, mixed integer linear programming, performance modeling, distributed inference]
  - **authors:** Tingyang Sun, Ting He, Bo Ji, Parimal Parag
  - **institution:** Pennsylvania State University, Virginia Tech, Indian Institute of Science
  - **link:** https://arxiv.org/pdf/2512.21884
  - **Simple LLM Summary:** This paper presents the first systematic study of resource allocation for distributed large language model (LLM) inference, focusing on optimizing block placement and request routing. The core method involves formulating the problem as a mixed integer linear program, proving its NP-hardness, and developing a polynomial-time algorithm with performance guarantees for both offline and online settings. The proposed solution is shown to substantially reduce inference time compared to the state-of-the-art in geographically-distributed server environments.

- **[arXiv251229] Agentic Structured Graph Traversal for Root Cause Analysis of Code-related Incidents in Cloud Applications**
  - **tags:** [mlsys], [fault-tolerance], [agentic workflow, structured graph traversal, service dependency graph (SDG), program dependence graph (PDG), hammock-block, root-cause analysis (RCA), LLM-driven traversal]
  - **authors:** Shengkun Cui, Rahul Krishna, Saurabh Jha, Ravishankar K. Iyer
  - **institution:** University of Illinois at Urbana-Champaign, IBM Research
  - **link:** https://arxiv.org/pdf/2512.22113
  - **Simple LLM Summary:** This paper introduces PRAXIS, an orchestrator that uses an LLM-driven agent to traverse service dependency graphs and program dependence graphs for diagnosing code-related cloud incidents. It demonstrates that this structured, agentic approach significantly improves root-cause analysis accuracy and reduces token consumption compared to baseline methods.

- **[arXiv251229] Robust Federated Fine-Tuning in Heterogeneous Networks with Unreliable Connections: An Aggregation View**
  - **tags:** [mlsys], [fault-tolerance], [federated fine-tuning, adaptive aggregation, connection failures, data heterogeneity, convergence guarantee, LoRA]
  - **authors:** Yanmeng Wang, Zhiwen Dai, Shuai Wang, Jian Zhou, Fu Xiao, Tony Q. S. Quek, Tsung-Hui Chang
  - **institution:** Nanjing University of Posts and Telecommunications, The Chinese University of Hong Kong, Shenzhen, Singapore University of Technology and Design
  - **link:** https://arxiv.org/pdf/2512.22035
  - **Simple LLM Summary:** The paper proposes FedAuto, a robust federated fine-tuning framework that uses adaptive aggregation to handle unreliable network connections and heterogeneous client data without prior knowledge of network conditions. It provides a strong convergence guarantee for each training round. Experiments show FedAuto outperforms existing methods under various failure scenarios for both full and partial-parameter fine-tuning like LoRA.

- **[arXiv251229] BLEST: Blazingly Efficient BFS using Tensor Cores**
  - **tags:** [mlsys], [GPU kernels], [Binarised Virtual Slice Sets (BVSS), graph reordering, batched SpMSpV multiplication, kernel fusion, lazy vertex update, Tensor Cores]
  - **authors:** Deniz Elbek, Kamer Kaya
  - **institution:** Sabanci University
  - **link:** https://arxiv.org/pdf/2512.21967
  - **Simple LLM Summary:** This paper presents BLEST, a framework that accelerates Breadth-First Search (BFS) on GPUs by efficiently mapping the algorithm onto Tensor Cores. Its key innovations include a bitmap-oriented structure with Binarised Virtual Slice Sets for load balancing, complementary graph reordering strategies, and a batched sparse matrix-sparse vector multiplication pattern. The experiments show that BLEST achieves significant speedups over state-of-the-art BFS implementations on a variety of real-world graphs.

- **[arXiv251229] FUSCO: High-Performance Distributed Data Shuffling via Transformation-Communication Fusion**
  - **tags:** [mlsys], [llm training], [expert parallelism, data shuffling, transformation-communication fusion, pipelined communication engine, load-balancing, Mixture-of-Experts (MoE)]
  - **authors:** Zhuoran Zhu, Chunyang Zhu, Hao Lin, Xu Fu, Yiming Zhou, Quanlu Zhang, Zhenhua Li, Feng Qian, Chao Yu, Boxun Li, Guohao Dai, Yu Wang
  - **institution:** Tsinghua University, Infinigence AI, University of Southern California, Zhongguancun Academy, Shanghai Jiaotong University
  - **link:** https://arxiv.org/pdf/2512.22036
  - **Simple LLM Summary:** The paper introduces FUSCO, a communication library that fuses data transformation and communication to optimize distributed data shuffling for Mixture-of-Experts (MoE) model training. It addresses the layout mismatch between expert-major data and device-major communication by using a pipelined engine and lightweight planning. The method significantly reduces training and inference latency compared to existing libraries like NCCL and DeepEP.


**cs.AI/cs.LG contains "reinforcement learning" total: 12**
- [arXiv251229] Variance-Aware Prior-Based Tree Policies for Monte Carlo Tree Search [link](https://arxiv.org/pdf/2512.21648)
- [arXiv251229] Multiconnectivity for SAGIN: Current Trends, Challenges, AI-driven Solutions, and Opportunities [link](https://arxiv.org/pdf/2512.21717)
- [arXiv251229] DiverseGRPO: Mitigating Mode Collapse in Image Generation via Diversity-Aware GRPO [link](https://arxiv.org/pdf/2512.21514)
- [arXiv251229] CosmoCore-Evo: Evolutionary Dream-Replay Reinforcement Learning for Adaptive Code Generation [link](https://arxiv.org/pdf/2512.21351)
- [arXiv251229] Videos are Sample-Efficient Supervisions: Behavior Cloning from Videos via Latent Representations [link](https://arxiv.org/pdf/2512.21586)
- [arXiv251229] A Reinforcement Learning Approach to Synthetic Data Generation [link](https://arxiv.org/pdf/2512.21395)
- [arXiv251229] Generative Actor Critic [link](https://arxiv.org/pdf/2512.21527)
- [arXiv251229] A Survey of Freshness-Aware Wireless Networking with Reinforcement Learning [link](https://arxiv.org/pdf/2512.21412)
- [arXiv251229] Leash: Adaptive Length Penalty and Reward Shaping for Efficient Large Reasoning Model [link](https://arxiv.org/pdf/2512.21540)
- [arXiv251229] dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning [link](https://arxiv.org/pdf/2512.21446)
- [arXiv251229] A Comedy of Estimators: On KL Regularization in RL Training of LLMs [link](https://arxiv.org/pdf/2512.21852)
- [arXiv251229] Meta-Learning-Based Handover Management in NextG O-RAN [link](https://arxiv.org/pdf/2512.22022)

**cs.AI/cs.LG contains "accelerate" total: 12**
- [arXiv251229] Variance-Aware Prior-Based Tree Policies for Monte Carlo Tree Search [link](https://arxiv.org/pdf/2512.21648)
- [arXiv251229] How Do Agents Perform Code Optimization? An Empirical Study [link](https://arxiv.org/pdf/2512.21757)
- [arXiv251229] BertsWin: Resolving Topological Sparsity in 3D Masked Autoencoders via Component-Balanced Structural Optimization [link](https://arxiv.org/pdf/2512.21769)
- [arXiv251229] Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism [link](https://arxiv.org/pdf/2512.21452)
- [arXiv251229] AVP-Fusion: Adaptive Multi-Modal Fusion and Contrastive Learning for Two-Stage Antiviral Peptide Identification [link](https://arxiv.org/pdf/2512.21544)
- [arXiv251229] dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning [link](https://arxiv.org/pdf/2512.21446)
- [arXiv251229] Discovering Sparse Recovery Algorithms Using Neural Architecture Search [link](https://arxiv.org/pdf/2512.21563)
- [arXiv251229] Hybrid Combinatorial Multi-armed Bandits with Probabilistically Triggered Arms [link](https://arxiv.org/pdf/2512.21925)
- [arXiv251229] DuaDeep-SeqAffinity: Dual-Stream Deep Learning Framework for Sequence-Only Antigen-Antibody Affinity Prediction [link](https://arxiv.org/pdf/2512.22007)
- [arXiv251229] StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars [link](https://arxiv.org/pdf/2512.22065)
- [arXiv251229] Prefill vs. Decode Bottlenecks: SRAM-Frequency Tradeoffs and the Memory-Bandwidth Ceiling [link](https://arxiv.org/pdf/2512.22066)
- [arXiv251229] Enabling Ultra-Fast Cardiovascular Imaging Across Heterogeneous Clinical Environments with a Generalist Foundation Model and Multimodal Database [link](https://arxiv.org/pdf/2512.21652)

## 2025-12-30

**cs.DC total: 42**

- **[arXiv251230] Adaptive GPU Resource Allocation for Multi-Agent Collaborative Reasoning in Serverless Environments**
  - **tags:** [mlsys], [llm inference], [adaptive GPU resource allocation, serverless computing, multi-agent systems, workload scheduling, O(N) algorithm]
  - **authors:** Guilin Zhang, Wulan Guo, Ziqi Tan
  - **institution:** George Washington University
  - **link:** https://arxiv.org/pdf/2512.22149
  - **Simple LLM Summary:** This paper proposes an adaptive GPU resource allocation framework for deploying multi-agent LLM systems in serverless environments. It uses a real-time O(N) complexity algorithm to dynamically allocate resources based on workload characteristics, reducing latency by 85% compared to round-robin scheduling while maintaining throughput and efficient GPU utilization.

- **[arXiv251230] HybridFlow: Adaptive Task Scheduling for Fast and Token-Efficient LLM Inference in Edge-Cloud Collaboration**
  - **tags:** [mlsys], [llm inference], [edge-cloud collaboration, task decomposition, parallel execution, resource-aware routing, adaptive scheduling]
  - **authors:** Jiangwen Dong, Jiayu Li, Wanyu Lin
  - **institution:** The Hong Kong Polytechnic University
  - **link:** https://arxiv.org/pdf/2512.22137
  - **Simple LLM Summary:** The paper proposes HybridFlow, a framework that splits complex queries into interdependent subtasks for parallel execution and uses a learned router to adaptively assign subtasks between edge and cloud LLMs. This approach aims to reduce inference latency and token usage while maintaining accuracy. Evaluations on reasoning benchmarks show it effectively achieves faster and more token-efficient inference.

- **[arXiv251230] GPU Kernel Optimization Beyond Full Builds: An LLM Framework with Minimal Executable Programs**
  - **tags:** [mlsys], [GPU kernels], [Minimal Executable Program (MEP), Automatic Error Repair, Performance Pattern Inheritance, iterative optimization]
  - **authors:** Ruifan Chu, Anbang Wang, Xiuxiu Bai, Shuai Liu, Xiaoshe Dong
  - **institution:** Xi'an Jiaotong University
  - **link:** https://arxiv.org/pdf/2512.22147
  - **Simple LLM Summary:** This paper presents an LLM-based framework for GPU kernel optimization that avoids expensive full application builds by using automatically generated Minimal Executable Programs (MEPs) for iterative optimization and evaluation. The framework incorporates Automatic Error Repair and Performance Pattern Inheritance to fix errors and reuse effective optimization strategies. The method achieves significant speedups across multiple platforms and benchmarks, demonstrating a practical, low-cost approach to GPU kernel optimization.

- **[arXiv251230] GPU-Virt-Bench: A Comprehensive Benchmarking Framework for Software-Based GPU Virtualization Systems**
  - **tags:** [mlsys], [llm inference], [GPU virtualization, benchmarking, CUDA, Multi-Instance GPU (MIG), HAMi-core, BUD-FCSP, container isolation, multi-tenancy]
  - **authors:** Jithin VG, Ditto PS
  - **institution:** Bud Ecosystem Inc
  - **link:** https://arxiv.org/pdf/2512.22125
  - **Simple LLM Summary:** This paper introduces GPU-Virt-Bench, a comprehensive benchmarking framework with 56 metrics across 10 categories for evaluating software-based GPU virtualization systems. It demonstrates the framework by comparing solutions like HAMi-core and BUD-FCSP against ideal MIG behavior. The main conclusion is that the framework provides actionable insights for deploying GPU resources in multi-tenant environments by revealing critical performance characteristics.

- **[arXiv251230] TL: Automatic End-to-End Compiler of Tile-Based Languages for Spatial Dataflow Architectures**
  - **tags:** [mlsys], [GPU kernels], [tile-based compilation, spatial dataflow architectures, MLIR, Triton, on-chip network, distributed memory, data reuse]
  - **authors:** Wei Li, Zhenyu Bai, Heru Wang, Pranav Dangi, Zhiqiang Zhang, Cheng Tan, Huiying Lan, Weng-Fai Wong, Tulika Mitra
  - **institution:** National University of Singapore, Arizona State University, Google, Lumai Ltd.
  - **link:** https://arxiv.org/pdf/2512.22168
  - **Simple LLM Summary:** This paper presents TL, an end-to-end compiler framework that compiles tile-based programs (like Triton kernels) onto spatial dataflow accelerators. It focuses on distributing tile instances across cores and exploiting the on-chip network to optimize data reuse and reduce communication. The framework, built on MLIR, achieves performance comparable to or better than vendor libraries on systems like Tenstorrent-Wormhole.

- **[arXiv251230] SlimEdge: Lightweight Distributed DNN Deployment on Constrained Hardware**
  - **tags:** [mlsys], [others], [structured model pruning, multi-objective optimization, view-adaptive compression, distributed inference]
  - **authors:** Mahadev Sunil Kumar, Arnab Raha, Debayan Das, Gopakumar G, Amitava Mukherjee
  - **institution:** Accenture PLC, Intel Corporation, Indian Institute of Science, Amrita Vishwa Vidyapeetham, Birla Institute of Technology and Science
  - **link:** https://arxiv.org/pdf/2512.22136
  - **Simple LLM Summary:** This paper presents SlimEdge, a method for deploying deep neural networks on resource-constrained edge devices by integrating structured model pruning with multi-objective optimization to tailor network capacity to hardware constraints. The framework is demonstrated on a Multi-View CNN for 3D object recognition, using view-adaptive compression to allocate pruning budgets. The results show that the optimized models meet specified accuracy and memory bounds while significantly reducing inference latency across diverse hardware platforms.

- **[arXiv251230] AiiDAlab: on the route to accelerate science**
  - **tags:** [sys], [scientific workflow management], [AiiDAlab, AiiDA, computational workflows, provenance tracking, FAIR principles, electronic laboratory notebooks, web interface]
  - **authors:** Aliaksandr V.Yakutovich, Jusong Yu, Daniel Hollas, Edan Bainglass, Corsin Battaglia, Miki Bonacci, Lucas Fernandez Vilanova, Stephan Henne, Anders Kaestner, Michel Kenzelmann, Graham Kimbell, Jakob Lass, Fabio Lopes, Daniel G. Mazzone, Andres Ortega-Guerrero, Xing Wang, Nicola Marzari, Carlo A. Pignedoli, Giovanni Pizzi
  - **institution:** Empa-Swiss Federal Laboratories for Materials Science and Technology, Paul Scherrer Institute, École Polytechnique Fédérale de Lausanne, University of Bristol, ETH Zurich
  - **link:** https://arxiv.org/pdf/2512.22173
  - **Simple LLM Summary:** The paper presents AiiDAlab, a web-based platform designed to simplify and automate complex computational workflows for scientific research. It allows researchers to execute and orchestrate simulations through an intuitive interface while the underlying AiiDA engine automatically tracks full provenance to ensure reproducibility. The platform has matured to accelerate discovery across multiple disciplines by enabling scientists to focus on research rather than computational details.

- **[arXiv251230] On Harnessing Idle Compute at the Edge for Foundation Model Training**
  - **tags:** [mlsys], [llm training], [selective hybrid tensor parallelism, parameter server, cost optimization model, decentralized training, edge computing]
  - **authors:** Leyang Xue, Meghana Madhyastha, Myungjin Lee, Amos Storkey, Randal Burns, Mahesh K. Marina
  - **institution:** The University of Edinburgh, Johns Hopkins University, Cisco Research
  - **link:** https://arxiv.org/pdf/2512.22142
  - **Simple LLM Summary:** The paper introduces Cleave, a new paradigm for decentralized foundation model training at the edge, which uses a selective hybrid tensor parallelism method and a parameter server framework to partition training operations and manage device memory and communication. It also employs a cost optimization model to handle device heterogeneity and churn. The evaluation shows that Cleave matches cloud-based GPU training performance, scales efficiently to thousands of devices, and significantly outperforms existing edge-training methods in speed and failure recovery.

- **[arXiv251230] HLS4PC: A Parametrizable Framework For Accelerating Point-Based 3D Point Cloud Models on FPGA**
  - **tags:** [mlsys], [others], [FPGA acceleration, High-Level Synthesis (HLS), parameter quantization, layer fusion, input-points pruning, Uniform Random Sampling (URS), fixed-point implementation]
  - **authors:** Amur Saqib Pal, Muhammad Mohsin Ghaffar, Faisal Shafait, Christian Weis, Norbert Wehn
  - **institution:** National University of Sciences and Technology, RPTU Kaiserslautern-Landau
  - **link:** https://arxiv.org/pdf/2512.22139
  - **Simple LLM Summary:** This paper introduces HLS4PC, a parameterizable High-Level Synthesis framework for accelerating point-based 3D point cloud models on FPGAs. The method applies hardware-aware compression techniques like quantization and sampling replacement to create a lighter model variant, PointMLP-Lite, and leverages FPGA parallelization for efficient fixed-point execution. The main conclusion is that this FPGA acceleration achieves significantly higher throughput compared to previous works, GPUs, and CPUs.

- **[arXiv251230] SoDA: An Efficient Interaction Paradigm for the Agentic Web**
  - **tags:** [mlsys], [others], [Sovereign Digital Avatar, intent-permission handshake, A2A protocols, dual-factor adaptive routing, orthogonal decoupling, data as a persistent asset, model as a transient tool]
  - **authors:** Zicai Cui, Zhouyuan Jian, Weiwen Liu, Weinan Zhang
  - **institution:** Shanghai Jiao Tong University, Shanghai Innovation Institute
  - **link:** https://arxiv.org/pdf/2512.22135
  - **Simple LLM Summary:** This paper proposes the Sovereign Digital Avatar (SoDA), a new interaction paradigm for the Agentic Web that decouples storage, computation, and interaction to give users control over their data and reduce cognitive load. It introduces an intent-permission handshake mechanism for secure, adaptive task routing in zero-trust environments. Empirical results show SoDA significantly reduces token consumption and user cognitive load compared to existing methods, positioning it as essential infrastructure for a decentralized and efficient web.

- **[arXiv251230] BitFlipScope: Scalable Fault Localization and Recovery for Bit-Flip Corruptions in LLMs**
  - **tags:** [mlsys], [fault-tolerance], [differential analysis, residual-path perturbation, loss-sensitivity profiling, bit-flip fault localization, transformer architectures]
  - **authors:** Muhammad Zeeshan Karamat, Sadman Saif, Christiana Chamon Garcia
  - **institution:** Virginia Tech
  - **link:** https://arxiv.org/pdf/2512.22174
  - **Simple LLM Summary:** This paper introduces BitFlipScope, a framework for localizing and recovering from bit-flip corruptions in LLMs. It uses differential analysis with a reference model or residual-path perturbation and loss-sensitivity profiling without one to pinpoint faults. The method enables lightweight performance recovery, contributing to more fault-resilient LLM deployment.

- **[arXiv251230] iOS as Acceleration**
  - **tags:** [mlsys], [others], [distributed pipeline parallelism, model partitioning, iOS acceleration, mobile computing, agentic LRM]
  - **authors:** Alexander K. Chen
  - **institution:** Independent High School Researcher
  - **link:** https://arxiv.org/pdf/2512.22180
  - **Simple LLM Summary:** This paper proposes a proof-of-concept system that uses distributed pipeline parallelism to harness iOS devices as computational accelerators for machine learning tasks, circumventing their memory limitations. It demonstrates benefits like reduced training time for modest models and explores use-cases in batch inference and agentic tool usage. The main conclusion is that ubiquitous mobile devices have significant potential to contribute to machine learning, improving local compute setups at zero additional hardware cost.

- **[arXiv251230] SPUMA: a minimally invasive approach to the GPU porting of OPENFOAM**
  - **tags:** [sys], [computational fluid dynamics], [GPU porting, unified memory, memory pool manager, portable programming model, strong scalability, weak scalability]
  - **authors:** Simone Bnà, Giuseppe Giaquinto, Ettore Fadiga, Tommaso Zanelli, Francesco Bottau
  - **institution:** Cineca Supercomputing Centre, Università degli Studi di Napoli Federico II
  - **link:** https://arxiv.org/pdf/2512.22215
  - **Simple LLM Summary:** The paper presents SPUMA, a minimally invasive GPU porting of OPENFOAM using a portable programming model and a memory pool manager that leverages unified memory. Performance tests on pre-exascale clusters show good scalability and up to an 82% reduction in energy consumption compared to CPU simulations.

- **[arXiv251230] MatKV: Trading Compute for Flash Storage in LLM Inference**
  - **tags:** [mlsys], [llm inference], [retrieval augmented generation, key-value vectors, precomputation, materialization, flash storage, prefill phase]
  - **authors:** Kun-Woo Shin, Jay H. Park, Moonwook Oh, Yohan Jo, Jaeyoung Do, Sang-Won Lee
  - **institution:** Seoul National University, Samsung Electronics
  - **link:** https://arxiv.org/pdf/2512.22195
  - **Simple LLM Summary:** The paper proposes MatKV, a method that precomputes and stores key-value vectors for RAG documents in flash storage to avoid recomputing them on GPUs during inference. This approach reduces inference time and power consumption by half for RAG workloads while maintaining accuracy. It also enables optimizations like overlapping KV loading with decoding and using lower-end GPUs for decoding.

- **[arXiv251230] Efficient Multi-Model Orchestration for Self-Hosted Large Language Models**
  - **tags:** [mlsys], [llm inference], [kubernetes, helm, scale-to-zero, hybrid routing, distilbert classifier, workload orchestration]
  - **authors:** Bhanu Prakash Vangala, Tanu Malik
  - **institution:** University of Missouri
  - **link:** https://arxiv.org/pdf/2512.22402
  - **Simple LLM Summary:** The paper introduces "Pick and Spin," a framework for orchestrating self-hosted LLMs using Kubernetes, Helm-based deployment, adaptive scaling, and a hybrid routing module. It demonstrates that this intelligent orchestration achieves higher success rates, lower latency, and reduced GPU costs compared to static deployments, making high-capacity AI more affordable on private infrastructure.

- **[arXiv251230] Scalable Cloud-Native Architectures for Intelligent PMU Data Processing**
  - **tags:** [mlsys], [cluster infrastructure], [cloud-native architecture, distributed stream processing, containerized microservices, elastic resource orchestration, edge computing, machine learning, time-series analysis, anomaly detection]
  - **authors:** Nachiappan Chockalingam, Akshay Deshpande, Lokesh Butra, Ram Sekhar Bodala, Nitin Saksena, Adithya Parthasarathy, Balakrishna Pothineni, Akash Kumar Agarwal
  - **institution:** IEEE, NTT Data, Amtrak, Albertsons Companies
  - **link:** https://arxiv.org/pdf/2512.22231
  - **Simple LLM Summary:** This paper proposes a scalable cloud-native architecture that integrates AI with edge and cloud computing, using distributed stream processing and containerized microservices for intelligent PMU data processing. It demonstrates that the framework can achieve sub-second response times and scale to large deployments, providing a robust foundation for next-generation smart grid analytics.

- **[arXiv251230] Mirage Persistent Kernel: A Compiler and Runtime for Mega-Kernelizing Tensor Programs**
  - **tags:** [mlsys], [llm inference], [megakernel, kernel fusion, SM-level task graph, cross-operator software pipelining, decentralized scheduling, CUDA]
  - **authors:** Xinhao Cheng, Zhihao Zhang, Yu Zhou, Jianan Ji, Jinchen Jiang, Zepeng Zhao, Ziruo Xiao, Zihao Ye, Yingyi Huang, Ruihang Lai, Hongyi Jin, Bohan Hou, Mengdi Wu, Yixin Dong, Anthony Yip, Zihao Ye, Songting Wang, Wenqin Yang, Xupeng Miao, Tianqi Chen, Zhihao Jia
  - **institution:** Carnegie Mellon University, Tsinghua University, NVIDIA, University of Michigan, Purdue University
  - **link:** https://arxiv.org/pdf/2512.22219
  - **Simple LLM Summary:** MPK is a compiler and runtime system that transforms multi-GPU model inference into a single megakernel by introducing an SM-level graph representation for fine-grained task scheduling. This enables cross-operator optimizations like software pipelining and reduces kernel launch overhead. The evaluation shows MPK reduces LLM inference latency by up to 1.7x compared to existing systems.

- **[arXiv251230] Nightjar: Dynamic Adaptive Speculative Decoding for Large Language Models Serving**
  - **tags:** [mlsys], [llm inference], [speculative decoding, adaptive inference, multi-armed bandit, dynamic batch size, verification overhead]
  - **authors:** Rui Li, Zhaoning Zhang, Libo Zhang, Huaimin Wang, Xiang Fu, Zhiquan Lai
  - **institution:** National University of Defense Technology
  - **link:** https://arxiv.org/pdf/2512.22420
  - **Simple LLM Summary:** The paper proposes Nightjar, a learning-based algorithm that dynamically adjusts the speculative length (or disables speculative decoding) based on real-time request load to optimize LLM serving. It overcomes the limitation of fixed-length speculative decoding, which suffers from verification overhead under high load. Experiments show Nightjar achieves higher throughput and lower latency compared to standard speculative decoding.

- **[arXiv251230] Valori: A Deterministic Memory Substrate for AI Systems**
  - **tags:** [mlsys], [others], [fixed-point arithmetic, Q16.16, state machine, vector embeddings, approximate nearest neighbor search, deterministic memory]
  - **authors:** Varshith Gudur
  - **institution:** Valori Kernel Project, Independent Researcher
  - **link:** https://arxiv.org/pdf/2512.22280
  - **Simple LLM Summary:** This paper introduces Valori, a deterministic AI memory substrate that replaces floating-point operations with fixed-point arithmetic (Q16.16) and models memory as a replayable state machine. It guarantees bit-identical memory states and search results across different hardware platforms. The authors conclude that deterministic memory is a necessary primitive for building trustworthy and auditable AI systems.

- **[arXiv251230] Cost-Aware Text-to-SQL: An Empirical Study of Cloud Compute Costs for LLM-Generated Queries**
  - **tags:** [mlsys], [llm inference], [text-to-sql, cloud compute cost, google bigquery, query cost optimization, reasoning models, bytes processed, slot utilization]
  - **authors:** Saurabh Deochake, Debajyoti Mukhopadhyay
  - **institution:** SentinelOne, WIDiCoReL Research Lab
  - **link:** https://arxiv.org/pdf/2512.22364
  - **Simple LLM Summary:** This paper presents an empirical study evaluating the cloud compute costs of SQL queries generated by six different LLMs on Google BigQuery. The core method involves measuring bytes processed, slot utilization, and estimated cost for 180 query executions on a 230GB dataset. The main conclusion is that reasoning models achieve significantly lower costs (processing 44.5% fewer bytes) than standard models while maintaining high correctness, and that execution time is a poor proxy for cloud query cost.

- **[arXiv251230] Modality Inflation: Energy Characterization and Optimization Opportunities for MLLM Inference**
  - **tags:** [mlsys], [multi-modal inference], [dynamic voltage and frequency scaling (DVFS), stage-level energy analysis, GPU power traces]
  - **authors:** Mona Moghadampanah, Adib Rezaei Shahmirzadi, Farhana Amin, Dimitrios S. Nikolopoulos
  - **institution:** Virginia Tech
  - **link:** https://arxiv.org/pdf/2512.22695
  - **Simple LLM Summary:** This paper provides a stage-level energy analysis of multimodal large language model (MLLM) inference, identifying modality inflation as a key inefficiency. It demonstrates that stage-wise dynamic voltage and frequency scaling (DVFS) is an effective optimization for reducing energy consumption with only a modest performance impact.

- **[arXiv251230] Role-Based Fault Tolerance System for LLM RL Post-Training**
  - **tags:** [mlsys], [fault-tolerance], [role-based fault isolation, role-aware monitoring, non-disruptive recovery, dynamic point-to-point communication, UCX, warm standbys]
  - **authors:** Zhenqian Chen, Baoquan Zhong, Xiang Li, Qing Dai, Xinkui Zhao, Miao Ye, Ren Cheng, Lufei Zhang, Jianwei Yin
  - **institution:** Zhejiang University, State Key Laboratory of Mathematical Engineering and Advanced Computing
  - **link:** https://arxiv.org/pdf/2512.22492
  - **Simple LLM Summary:** The paper introduces RobustRL, a fault tolerance system for RL post-training of LLMs that uses a role-based approach to isolate and recover from GPU failures. Its method involves detecting failures per role, restarting only the failed component, and reconnecting it via dynamic communication. The system significantly improves the Effective Training Time Ratio and reduces end-to-end training time compared to baseline methods.

- **[arXiv251230] RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure**
  - **tags:** [mlsys], [llm training], [disaggregated infrastructure, hardware-affinity workload mapping, fine-grained asynchrony, statefulness-aware computation, agentic RL, trajectory-level execution]
  - **authors:** Wei Gao, Yuheng Zhao, Tianyuan Wu, Shaopan Xiong, Weixun Wang, Dakai An, Lunxi Cao, Dilxat Muhtar, Zichen Liu, Haizhou Zhao, Ju Huang, Siran Yang, Yongbin Li, Wenbo Su, Jiamang Wang, Lin Qu, Bo Zheng, Wei Wang
  - **institution:** HKUST, Alibaba Group, Tongyi Lab
  - **link:** https://arxiv.org/pdf/2512.22560
  - **Simple LLM Summary:** The paper presents RollArt, a distributed system designed to scale agentic reinforcement learning training by using disaggregated infrastructure. Its core methods include hardware-affinity workload mapping, fine-grained asynchrony, and statefulness-aware computation to manage heterogeneous workloads efficiently. The results show that RollArt significantly improves training throughput and reduces end-to-end training time compared to monolithic baselines.

- **[arXiv251230] OptiNIC: A Resilient and Tail-Optimal RDMA NIC for Distributed ML Workloads**
  - **tags:** [mlsys], [cluster infrastructure], [RDMA, reliability, in-order delivery, adaptive timeouts, best-effort transport, Hadamard Transform, Erasure Coding, tail latency, collective communication]
  - **authors:** Ertza Warraich, Ali Imran, Annus Zulfiqar, Shay Vargaftik, Sonia Fahmy, Muhammad Shahbaz
  - **institution:** Purdue University, Broadcom, University of Michigan
  - **link:** https://arxiv.org/pdf/2512.22743
  - **Simple LLM Summary:** The paper presents OptiNIC, a domain-specific RDMA transport that eliminates retransmissions and in-order delivery from the NIC, using adaptive timeouts and shifting loss recovery to the ML pipeline. It concludes that this approach significantly improves time-to-accuracy, throughput, and tail latency for distributed ML workloads, while increasing NIC resilience.

- **[arXiv251230] Object Abstraction To Streamline Edge-Cloud-Native Application Development**
  - **tags:** [sys], [cloud-native computing], [Object-as-a-Service, edge-cloud continuum, serverless, FaaS, SLA-driven management, Oparaca, EdgeWeaver]
  - **authors:** Pawissanutt Lertpongrujikorn
  - **institution:** University of North Texas
  - **link:** https://arxiv.org/pdf/2512.22534
  - **Simple LLM Summary:** This dissertation introduces the Object-as-a-Service (OaaS) paradigm, a unified approach to cloud-native development that abstracts infrastructure complexity by consolidating resource, state, and workflow management. It demonstrates through empirical studies and prototypes like Oparaca and EdgeWeaver that OaaS reduces developer effort and improves performance compared to traditional serverless models. The main conclusion is that OaaS establishes a foundation for platforms that hide infrastructure complexity, allowing developers to focus on innovation.

- **[arXiv251230] Two-Robot Computational Landscape: A Complete Characterization of Model Power in Minimal Mobile Robot Systems**
  - **tags:** [sys], [distributed computing], [Look-Compute-Move, OBLOT, FSTA, FCOM, LUMI, Fsynch, Rsynch, Ssynch, Asynch, simulation-free method]
  - **authors:** Naoki Kitamura, Yuichi Sudo, Koichi Wada
  - **institution:** The University of Osaka, Hosei University
  - **link:** https://arxiv.org/pdf/2512.22770
  - **Simple LLM Summary:** This paper provides a complete characterization of the computational power of two autonomous mobile robots across major models and schedulers using a novel simulation-free method. It concludes that the two-robot landscape fundamentally differs from the general case, showing that perfect synchrony can substitute for memory and communication, and that certain models are orthogonal.

- **[arXiv251230] Argus: Token Aware Distributed LLM Inference Optimization**
  - **tags:** [mlsys], [llm inference], [task offloading, Lyapunov optimization, integer nonlinear programming, length prediction, edge-cloud system]
  - **authors:** Panlong Wu, Yifei Zhong, Danyang Chen, Ting Wang, Fangxin Wang
  - **institution:** The Chinese University of Hong Kong, Shenzhen (CUHK(SZ))
  - **link:** https://arxiv.org/pdf/2512.22925
  - **Simple LLM Summary:** This paper introduces Argus, a token-aware distributed inference framework that uses a Length-Aware Semantics module to predict output token lengths and a Lyapunov-guided Offloading Optimization module for long-term Quality-of-Experience optimization. It proposes an Iterative Offloading Algorithm with Damping and Congestion Control to solve the resulting optimization problem. The evaluations show that Argus achieves robust and efficient performance in dynamic, heterogeneous edge-cloud environments.

- **[arXiv251230] A Domain Decomposition-based Solver for Acoustic Wave propagation in Two-Dimensional Random Media**
  - **tags:** [sys], [computational mathematics], [domain decomposition, stochastic Galerkin method, polynomial chaos expansion, Neumann-Neumann preconditioner, conjugate gradient]
  - **authors:** Sudhi Sharma Padillath Vasudevan
  - **institution:** Carleton University
  - **link:** https://arxiv.org/pdf/2512.23027
  - **Simple LLM Summary:** This paper presents a domain decomposition-based solver for acoustic wave propagation in random media, using an intrusive stochastic Galerkin method with polynomial chaos expansion to transform the stochastic PDE into deterministic systems. The method employs a conjugate gradient iterative solver with a two-level Neumann-Neumann preconditioner to manage computational costs. The results demonstrate the approach's efficient scalability for handling large mesh sizes, time steps, and random parameters.

- **[arXiv251230] Viability and Performance of a Private LLM Server for SMBs: A Benchmark Analysis of Qwen3-30B on Consumer-Grade Hardware**
  - **tags:** [mlsys], [llm inference], [quantization, mixture-of-experts (MoE), on-premise deployment, benchmarking, consumer-grade hardware]
  - **authors:** Alex Khalil, Guillaume Heilles, Maria Parraga, Simon Heilles
  - **institution:** UCLouvain, Universidad Espíritu Santo, DENEM Labs
  - **link:** https://arxiv.org/pdf/2512.23029
  - **Simple LLM Summary:** This paper benchmarks a private LLM server for SMBs by deploying a quantized Qwen3-30B Mixture-of-Experts model on consumer-grade hardware (NVIDIA RTX 5090). It concludes that this on-premise setup can achieve performance comparable to cloud services at a lower cost while maintaining data privacy and control.

- **[arXiv251230] Osmotic Learning: A Self-Supervised Paradigm for Decentralized Contextual Data Representation**
  - **tags:** [mlsys], [others], [osmotic learning, self-supervised learning, representation learning, distributed learning, decentralized clustering, contextual data representation]
  - **authors:** Mario Colosi, Reza Farahani, Maria Fazio, Radu Prodan, Massimo Villari
  - **institution:** University of Messina, University of Klagenfurt, University of Innsbruck
  - **link:** https://arxiv.org/pdf/2512.23096
  - **Simple LLM Summary:** The paper introduces Osmotic Learning (OSM-L), a self-supervised distributed learning paradigm that iteratively aligns local data representations to extract higher-level contextual knowledge without exchanging raw data. It functions as a decentralized clustering mechanism by allowing information diffusion to converge into a dynamic equilibrium. Experimental results demonstrate its convergence and high accuracy in aligning local information while preserving contextual integrity.

- **[arXiv251230] FairGFL: Privacy-Preserving Fairness-Aware Federated Learning with Overlapping Subgraphs**
  - **tags:** [mlsys], [others], [federated learning, graph neural networks, fairness, overlapping subgraphs, weighted aggregation, privacy-preserving estimation]
  - **authors:** Zihao Zhou, Shusen Yang, Fangyuan Zhao, Xuebin Ren
  - **institution:** Xi'an Jiaotong University
  - **link:** https://arxiv.org/pdf/2512.23235
  - **Simple LLM Summary:** The paper proposes FairGFL, a fairness-aware federated learning algorithm for graph data that addresses unfairness from imbalanced overlapping subgraphs. It uses a privacy-preserving weighted aggregation method and a regularizer in the loss function to balance model utility and fairness. Experiments show it outperforms baseline methods in both utility and fairness.

- **[arXiv251230] Splitwise: Collaborative Edge-Cloud Inference for LLMs via Lyapunov-Assisted DRL**
  - **tags:** [mlsys], [llm inference], [lyapunov optimization, deep reinforcement learning, edge-cloud partitioning, transformer layer decomposition, queue stability]
  - **authors:** Abolfazl Younesi, Abbas Shabrang Maryan, Elyas Oustad, Zahra Najafabadi Samani, Mohsen Ansari, Thomas Fahringer
  - **institution:** University of Innsbruck, Sharif University of Technology
  - **link:** https://arxiv.org/pdf/2512.23310
  - **Simple LLM Summary:** The paper proposes Splitwise, a framework that uses Lyapunov-assisted deep reinforcement learning to dynamically partition LLM inference tasks between edge and cloud devices at a fine-grained sub-layer level. It aims to jointly optimize latency, energy, and accuracy while handling variable network conditions. Experiments show it significantly reduces latency and energy consumption compared to existing methods while maintaining model accuracy.

- **[arXiv251230] An SLO Driven and Cost-Aware Autoscaling Framework for Kubernetes**
  - **tags:** [mlsys], [cluster infrastructure], [Kubernetes autoscaling, AIOps, SLO-aware control, cost optimization, demand forecasting, multi-signal framework]
  - **authors:** Vinoth Punniyamoorthy, Bikesh Kumar, Sumit Saha, Lokesh Butra, Mayilsamy Palanigounder, Akash Kumar Agarwal, Kabilan Kannan
  - **institution:** IEEE, East West Bank, NTT Data, Albertsons
  - **link:** https://arxiv.org/pdf/2512.23415
  - **Simple LLM Summary:** This paper proposes an AIOps-driven autoscaling framework for Kubernetes that integrates SLO-aware and cost-conscious control with lightweight demand forecasting. The method improves upon native Kubernetes autoscalers by using multiple workload signals to reduce SLO violations and infrastructure costs. The results show significant improvements in reliability, efficiency, and operational trustworthiness for cloud-native platforms.

- **[arXiv251230] Optimal Configuration of API Resources in Cloud Native Computing**
  - **tags:** [sys], [cloud computing, DevOps], [black-box optimization, Bayesian optimization, factor screening, Kubernetes, microservices, resource allocation, TeaStore]
  - **authors:** Eddy Truyen, Wouter Joosen
  - **institution:** DistriNet, KU Leuven
  - **link:** https://arxiv.org/pdf/2512.23494
  - **Simple LLM Summary:** This paper applies a black-box optimization framework to fine-tune CPU and memory resource configurations for microservices during the DevOps Release phase. It evaluates different optimization algorithms using the TeaStore application, concluding that factor screening is beneficial for finding an optimal configuration with a limited sampling budget, while Bayesian optimization without screening is better for finding a near-optimal solution.

- **[arXiv251230] Decoupling Adaptive Control in TeaStore**
  - **tags:** [sys], [cloud-native systems], [MAPE-K control loop, microservices, Operator pattern, self-adaptation, software architecture]
  - **authors:** Eddy Truyen
  - **institution:** KU Leuven
  - **link:** https://arxiv.org/pdf/2512.23495
  - **Simple LLM Summary:** The paper examines how software architectural methods, the cloud-native Operator pattern, and legacy programming techniques can be used to decouple self-adaptive control logic from the TeaStore microservice application. It concludes that these approaches are not mutually exclusive and can be combined into a multi-tiered architecture for self-adaptive microservices.

- **[arXiv251230] Local Rendezvous Hashing: Bounded Loads and Minimal Churn via Cache-Local Candidates**
  - **tags:** [sys], [distributed systems], [consistent hashing, rendezvous hashing, highest random weight, load balancing, minimal churn, cache locality]
  - **authors:** Yongjie Guan
  - **institution:** Zhejiang University of Technology
  - **link:** https://arxiv.org/pdf/2512.23434
  - **Simple LLM Summary:** This paper introduces Local Rendezvous Hashing (LRH), a new consistent hashing method that restricts candidate selection for a key to a cache-local window of neighboring nodes on a ring. It achieves better load balance than basic ring hashing and significantly higher lookup throughput than multi-probe consistent hashing, while ensuring minimal key movement during node failures.

- **[arXiv251230] Synthesis of signal processing algorithms with constraints on minimal parallelism and memory space**
  - **tags:** [sys], [signal processing hardware], [piecewise-polynomial approximation, conflict-free data placement, mixed-radix FFT, fast Schur algorithm, energy consumption model]
  - **authors:** Sergey Salishev
  - **institution:** Saint Petersburg State University
  - **link:** https://arxiv.org/pdf/2512.22676
  - **Simple LLM Summary:** This thesis develops signal processing algorithms and implementation schemes to improve energy efficiency under constraints of minimal parallelism and memory. It proposes methods including integer-friendly function approximations, conflict-free memory access for FFTs, and parallelism analysis for Toeplitz solvers. The results provide constructive theorems and design trade-offs for creating efficient specialized accelerators.

- **[arXiv251230] Bitcoin-IPC: Scaling Bitcoin with a Network of Proof-of-Stake Subnets**
  - **tags:** [sys], [blockchain scaling], [Bitcoin, Layer-2, Proof-of-Stake, subnet, SegWit, SWIFT messaging, virtual-byte cost]
  - **authors:** Marko Vukolić, Orestis Alpos, Jakov Mitrovski, Themis Papameletiou, Nikola Ristić, Dionysis Zindros
  - **institution:** Bitcoin Scaling Labs, Common Prefix
  - **link:** https://arxiv.org/pdf/2512.23439
  - **Simple LLM Summary:** The paper introduces Bitcoin-IPC, a protocol that scales Bitcoin by enabling the permissionless creation of Proof-of-Stake Layer-2 subnets whose security is backed by L1 BTC. Its core method uses a design inspired by SWIFT messaging and embedded in Bitcoin's SegWit to route value transfers across subnets via Bitcoin L1. The main conclusion is that this approach can reduce transaction costs by up to 23x and increase throughput to over 160 tps without modifying Bitcoin's base layer.

- **[arXiv251230] AdaptiFlow: An Extensible Framework for Event-Driven Autonomy in Cloud Microservices**
  - **tags:** [sys], [autonomic computing], [MAPE-K loop, event-driven adaptation, rule-based mechanism, decentralized adaptation, self-healing, self-protection, self-optimization]
  - **authors:** Brice Arléon Zemtsop Ndadji, Simon Bliudze, Clément Quinton
  - **institution:** Univ. Lille, CNRS, Inria, Centrale Lille
  - **link:** https://arxiv.org/pdf/2512.23499
  - **Simple LLM Summary:** The paper presents AdaptiFlow, a framework that decouples metrics collection and action execution from adaptation logic using an event-driven, rule-based mechanism to enable decentralized autonomy in cloud microservices. It validates the approach by demonstrating self-healing, self-protection, and self-optimization scenarios with minimal service modification. The work concludes that decentralized adaptation can emerge from localized decisions without global coordination, bridging autonomic computing theory with cloud-native practice.

- **[arXiv251230] Fancy Some Chips for Your TeaStore? Modeling the Control of an Adaptable Discrete System**
  - **tags:** [sys], [distributed systems modeling], [Chips language, control theory, component-based models, BIP translation, Adaptable TeaStore]
  - **authors:** Anna Gallone, Simon Bliudze, Sophie Cerf, Olga Kouchnarenko
  - **institution:** Université Marie et Louis Pasteur, CNRS UMR6174, Institut FEMTO-ST; Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189 CRIStAL
  - **link:** https://arxiv.org/pdf/2512.23496
  - **Simple LLM Summary:** The paper introduces Chips, a language for designing and modeling complex distributed systems by combining control theory with programming constructs to create robust component-based models. It demonstrates the application of Chips using a variation of the Adaptable TeaStore as a case study to systematically design and analyze such systems. The main conclusion is that Chips facilitates the systematic design and analysis of adaptable discrete systems, enhancing robustness and quality of service.

- **[arXiv251230] Revisiting finite Abelian hidden subgroup problem and its distributed exact quantum algorithm**
  - **tags:** [ai], [quantum algorithms], [amplitude amplification, Chinese Remainder Theorem, exact quantum algorithm, distributed quantum algorithm, hidden subgroup problem]
  - **authors:** Ziyuan Dong, Xiang Fan, Tengxun Zhong, Daowen Qiu
  - **institution:** Sun Yat-sen University
  - **link:** https://arxiv.org/pdf/2512.22959
  - **Simple LLM Summary:** This paper revisits the finite Abelian hidden subgroup problem, presenting a new exact quantum algorithm using amplitude amplification and a distributed exact quantum algorithm based on the Chinese Remainder Theorem. The distributed algorithm requires fewer qudits, lower query complexity, and no quantum communication. The authors also extend the distributed approach to some non-Abelian groups and develop a parallel classical algorithm with reduced query complexity.

- **[arXiv251230] Federated Learning With L0 Constraint Via Probabilistic Gates For Sparsity**
  - **tags:** [mlsys], [others], [federated learning, L0 constraint, probabilistic gates, sparsity, federated stochastic gradient descent, magnitude pruning]
  - **authors:** Krishna Harsha Kovelakuntla Huthasana, Alireza Olama, Andreas Lundell
  - **institution:** Åbo Akademi University
  - **link:** https://arxiv.org/pdf/2512.23071
  - **Simple LLM Summary:** The paper proposes a federated learning method that enforces an L0 constraint on model parameters using probabilistic gates and continuous relaxation to achieve sparsity. The approach is shown to effectively reach target parameter densities under data and client heterogeneity with minimal performance loss across various models. Experiments demonstrate it is more communication-efficient and statistically superior compared to magnitude pruning-based sparsity methods in FL.


**cs.AI/cs.LG contains "reinforcement learning" total: 37**
- [arXiv251230] Unbiased Visual Reasoning with Controlled Visual Inputs [link](https://arxiv.org/pdf/2512.22183)
- [arXiv251230] Learning Tennis Strategy Through Curriculum-Based Dueling Double Deep Q-Networks [link](https://arxiv.org/pdf/2512.22186)
- [arXiv251230] Physics-Informed Machine Learning for Transformer Condition Monitoring -- Part I: Basic Concepts, Neural Networks, and Variants [link](https://arxiv.org/pdf/2512.22190)
- [arXiv251230] Emotion-Inspired Learning Signals (EILS): A Homeostatic Framework for Adaptive Autonomous Agents [link](https://arxiv.org/pdf/2512.22200)
- [arXiv251230] DiRL: An Efficient Post-Training Framework for Diffusion Language Models [link](https://arxiv.org/pdf/2512.22234)
- [arXiv251230] Masking Teacher and Reinforcing Student for Distilling Vision-Language Models [link](https://arxiv.org/pdf/2512.22238)
- [arXiv251230] Agentic Software Issue Resolution with Large Language Models: A Survey [link](https://arxiv.org/pdf/2512.22256)
- [arXiv251230] VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning [link](https://arxiv.org/pdf/2512.22315)
- [arXiv251230] SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents [link](https://arxiv.org/pdf/2512.22322)
- [arXiv251230] PHANTOM: Physics-Aware Adversarial Attacks against Federated Learning-Coordinated EV Charging Management System [link](https://arxiv.org/pdf/2512.22381)
- [arXiv251230] AFA-LoRA: Enabling Non-Linear Adaptations in LoRA with Activation Function Annealing [link](https://arxiv.org/pdf/2512.22455)
- [arXiv251230] Memento-II: Learning by Stateful Reflective Memory [link](https://arxiv.org/pdf/2512.22716)
- [arXiv251230] FoldAct: Efficient and Stable Context Folding for Long-Horizon Search Agents [link](https://arxiv.org/pdf/2512.22733)
- [arXiv251230] ReDiF: Reinforced Distillation for Few Step Diffusion [link](https://arxiv.org/pdf/2512.22802)
- [arXiv251230] TEACH: Temporal Variance-Driven Curriculum for Reinforcement Learning [link](https://arxiv.org/pdf/2512.22824)
- [arXiv251230] AutoForge: Automated Environment Synthesis for Agentic Reinforcement Learning [link](https://arxiv.org/pdf/2512.22857)
- [arXiv251230] Adaptive Trust Consensus for Blockchain IoT: Comparing RL, DRL, and MARL Against Naive, Collusive, Adaptive, Byzantine, and Sleeper Attacks [link](https://arxiv.org/pdf/2512.22860)
- [arXiv251230] Reinforcement Networks: novel framework for collaborative Multi-Agent Reinforcement Learning tasks [link](https://arxiv.org/pdf/2512.22876)
- [arXiv251230] SAMP-HDRL: Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning [link](https://arxiv.org/pdf/2512.22895)
- [arXiv251230] Sat-EnQ: Satisficing Ensembles of Weak Q-Learners for Reliable and Compute-Efficient Reinforcement Learning [link](https://arxiv.org/pdf/2512.22910)
- [arXiv251230] Heterogeneity in Multi-Agent Reinforcement Learning [link](https://arxiv.org/pdf/2512.22941)
- [arXiv251230] APO: Alpha-Divergence Preference Optimization [link](https://arxiv.org/pdf/2512.22953)
- [arXiv251230] Taming the Tail: Stable LLM Reinforcement Learning via Dynamic Vocabulary Pruning [link](https://arxiv.org/pdf/2512.23087)
- [arXiv251230] Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients [link](https://arxiv.org/pdf/2512.23090)
- [arXiv251230] A Note on Hybrid Online Reinforcement and Imitation Learning for LLMs: Formulations and Algorithms [link](https://arxiv.org/pdf/2512.23097)
- [arXiv251230] Evaluating Parameter Efficient Methods for RLVR [link](https://arxiv.org/pdf/2512.23165)
- [arXiv251230] ViLaCD-R1: A Vision-Language Framework for Semantic Change Detection in Remote Sensing [link](https://arxiv.org/pdf/2512.23244)
- [arXiv251230] AGRO-SQL: Agentic Group-Relative Optimization with High-Fidelity Data Synthesis [link](https://arxiv.org/pdf/2512.23366)
- [arXiv251230] The World Is Bigger! A Computationally-Embedded Perspective on the Big World Hypothesis [link](https://arxiv.org/pdf/2512.23419)
- [arXiv251230] Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following [link](https://arxiv.org/pdf/2512.23457)
- [arXiv251230] Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance [link](https://arxiv.org/pdf/2512.23461)
- [arXiv251230] HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation [link](https://arxiv.org/pdf/2512.23464)
- [arXiv251230] Agentic AI for Autonomous Defense in Software Supply Chain Security: Beyond Provenance to Vulnerability Mitigation [link](https://arxiv.org/pdf/2512.23480)
- [arXiv251230] PathFound: An Agentic Multimodal Model Activating Evidence-seeking Pathological Diagnosis [link](https://arxiv.org/pdf/2512.23545)
- [arXiv251230] Le Cam Distortion: A Decision-Theoretic Framework for Robust Transfer Learning [link](https://arxiv.org/pdf/2512.23617)
- [arXiv251230] Training AI Co-Scientists Using Rubric Rewards [link](https://arxiv.org/pdf/2512.23707)
- [arXiv251230] Alpha-R1: Alpha Screening with LLM Reasoning via Reinforcement Learning [link](https://arxiv.org/pdf/2512.23515)

**cs.AI/cs.LG contains "accelerate" total: 20**
- [arXiv251230] HookMIL: Revisiting Context Modeling in Multiple Instance Learning for Computational Pathology [link](https://arxiv.org/pdf/2512.22188)
- [arXiv251230] DiRL: An Efficient Post-Training Framework for Diffusion Language Models [link](https://arxiv.org/pdf/2512.22234)
- [arXiv251230] Graph Attention-based Adaptive Transfer Learning for Link Prediction [link](https://arxiv.org/pdf/2512.22252)
- [arXiv251230] LuxIA: A Lightweight Unitary matriX-based Framework Built on an Iterative Algorithm for Photonic Neural Network Training [link](https://arxiv.org/pdf/2512.22264)
- [arXiv251230] DBAW-PIKAN: Dynamic Balance Adaptive Weight Kolmogorov-Arnold Neural Network for Solving Partial Differential Equations [link](https://arxiv.org/pdf/2512.22283)
- [arXiv251230] LLA: Enhancing Security and Privacy for Generative Models with Logic-Locked Accelerators [link](https://arxiv.org/pdf/2512.22307)
- [arXiv251230] AI-Generated Code Is Not Reproducible (Yet): An Empirical Study of Dependency Gaps in LLM-Based Coding Agents [link](https://arxiv.org/pdf/2512.22387)
- [arXiv251230] Quantum Generative Models for Computational Fluid Dynamics: A First Exploration of Latent Space Learning in Lattice Boltzmann Simulations [link](https://arxiv.org/pdf/2512.22672)
- [arXiv251230] Reach-Avoid Differential game with Reachability Analysis for UAVs: A decomposition approach [link](https://arxiv.org/pdf/2512.22793)
- [arXiv251230] TEACH: Temporal Variance-Driven Curriculum for Reinforcement Learning [link](https://arxiv.org/pdf/2512.22824)
- [arXiv251230] SE-MLP Model for Predicting Prior Acceleration Features in Penetration Signals [link](https://arxiv.org/pdf/2512.23131)
- [arXiv251230] HELM-BERT: A Transformer for Medium-sized Peptide Property Prediction [link](https://arxiv.org/pdf/2512.23175)
- [arXiv251230] KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta [link](https://arxiv.org/pdf/2512.23236)
- [arXiv251230] Explainable Neural Inverse Kinematics for Obstacle-Aware Robotic Manipulation: A Comparative Analysis of IKNet Variants [link](https://arxiv.org/pdf/2512.23312)
- [arXiv251230] SoulX-LiveTalk Technical Report [link](https://arxiv.org/pdf/2512.23379)
- [arXiv251230] AKG kernel Agent: A Multi-Agent Framework for Cross-Platform Kernel Synthesis [link](https://arxiv.org/pdf/2512.23424)
- [arXiv251230] Fuzzy-Logic and Deep Learning for Environmental Condition-Aware Road Surface Classification [link](https://arxiv.org/pdf/2512.23436)
- [arXiv251230] HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation [link](https://arxiv.org/pdf/2512.23464)
- [arXiv251230] Space AI: Leveraging Artificial Intelligence for Space to Improve Life on Earth [link](https://arxiv.org/pdf/2512.22399)
- [arXiv251230] Constraint programming model and biased random-key genetic algorithm for the single-machine coupled task scheduling problem with exact delays to minimize the makespan [link](https://arxiv.org/pdf/2512.23150)
