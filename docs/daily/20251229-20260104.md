# 20251229-20260104

## 2025-12-29

**cs.DC total: 16**

- **[arXiv251229] LIME:Accelerating Collaborative Lossless LLM Inference on Memory-Constrained Edge Devices**
  - **tags:** [mlsys], [llm inference], [collaborative inference, pipeline parallelism, model offloading, memory adaptation, edge computing]
  - **authors:** Mingyu Sun, Xiao Zhang, Shen Qu, Yan Li, Mengbai Xiao, Yuan Yuan, Dongxiao Yu
  - **institution:** Shandong University
  - **link:** https://arxiv.org/pdf/2512.21835
  - **Simple LLM Summary:** The paper proposes LIME, a collaborative system that uses interleaved pipeline parallelism and model offloading to enable lossless LLM inference across multiple memory-constrained edge devices. It introduces fine-grained scheduling and online memory adaptation to optimize resource use and minimize latency. Experiments show LIME achieves significant speedups over baselines on heterogeneous edge devices without compromising model accuracy.

- **[arXiv251229] Embedding Samples Dispatching for Recommendation Model Training in Edge Environments**
  - **tags:** [mlsys], [others], [embedding cache, parameter server, edge computing, sample dispatching, HybridDis]
  - **authors:** Guopeng Li, Haisheng Tan, Chi Zhang, Hongqiu Ni, Zilong Wang, Xinyue Zhang, Yang Xu, Han Tian
  - **institution:** University of Science and Technology of China (USTC), Hefei University of Technology
  - **link:** https://arxiv.org/pdf/2512.21615
  - **Simple LLM Summary:** The paper proposes ESD, a mechanism that dispatches input embedding samples to edge workers to minimize transmission costs in distributed DLRM training. It introduces HybridDis, a hybrid dispatch method combining an optimal and a heuristic algorithm. Experiments show ESD reduces embedding transmission cost by up to 36.76% and speeds up training by up to 1.74x.

- **[arXiv251229] LEFT-RS: A Lock-Free Fault-Tolerant Resource Sharing Protocol for Multicore Real-Time Systems**
  - **tags:** [sys], [real-time systems], [lock-free protocol, fault tolerance, resource sharing, worst-case response time analysis, multicore]
  - **authors:** Nan Chen, Xiaotian Dai, Tong Cheng, Alan Burns, Iain Bate, Shuai Zhao
  - **institution:** University of York, Sun Yat-sen University
  - **link:** https://arxiv.org/pdf/2512.21701
  - **Simple LLM Summary:** The paper proposes LEFT-RS, a lock-free fault-tolerant resource sharing protocol for multicore real-time systems that allows tasks to concurrently read global resources and enter critical sections in parallel, improving efficiency and fault resilience. It includes a worst-case response time analysis to ensure timing guarantees. Evaluation shows the method significantly outperforms existing approaches, achieving up to an 84.5% average improvement in schedulability.

- **[arXiv251229] Harnessing Data Spaces to Build Intelligent Smart City Infrastructures Across the Cloud-Edge Continuum**
  - **tags:** [mlsys], [others], [data spaces, cloud-edge continuum, edge computing, containerized microservices, AI/ML services, IoT, sensor integration, data privacy]
  - **authors:** Dimitrios Amaxilatis, Themistoklis Sarantakos, Nikolaos Tsironis, Souvik Sengupta, Kostas Ramantas, Jhofre Ojeda
  - **institution:** Spark Works Ltd., IONOS SE, Iquadrat Informática S.L.
  - **link:** https://arxiv.org/pdf/2512.21340
  - **Simple LLM Summary:** The paper presents a real-world use case of intelligent infrastructure monitoring using a data space-enabled cloud-edge framework. It demonstrates how edge computing, containerized microservices, and interoperable data sharing address challenges like sensor integration and data privacy. The implementation highlights the transformative potential of combining AI, edge computing, and data spaces for scalable and resilient smart city ecosystems.

- **[arXiv251229] nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures**
  - **tags:** [mlsys], [llm inference], [e-graph, term rewriting, equality saturation, auto vectorize, auto distribution, auto schedule, buffer-aware codegen, NUMA abstraction, roofline model]
  - **authors:** Hui Guo, Qihang Zheng, Chenghai Huo, Dongliang Guo, Haoqi Yang, Yang Zhang
  - **institution:** Canaan Inc.
  - **link:** https://arxiv.org/pdf/2512.21571
  - **Simple LLM Summary:** The paper presents nncase, an end-to-end compiler framework that uses an e-graph-based term rewriting engine to optimize large language model deployment across heterogeneous memory architectures. It unifies optimization through modules for vectorization, parallel distribution, and scheduling, achieving performance comparable to hand-optimized systems and outperforming other mainstream frameworks.

- **[arXiv251229] Smart IoT-Based Leak Forecasting and Detection for Energy-Efficient Liquid Cooling in AI Data Centers**
  - **tags:** [mlsys], [fault-tolerance], [LSTM, Random Forest, MQTT, InfluxDB, Streamlit, IoT monitoring, probabilistic forecasting]
  - **authors:** Krishna Chaitanya Sunkara, Rambabu Konakanchi
  - **institution:** Oracle, Charles Schwab
  - **link:** https://arxiv.org/pdf/2512.21801
  - **Simple LLM Summary:** This paper presents a smart IoT monitoring system that uses LSTM neural networks for probabilistic leak forecasting and Random Forest classifiers for instant detection in liquid-cooled AI data centers. The system, tested on synthetic data, achieved high accuracy and can forecast leaks 2-4 hours in advance, potentially preventing significant energy waste. The work establishes a proof-of-concept for proactive maintenance to enhance sustainability in data center operations.

- **[arXiv251229] Hyperion: Low-Latency Ultra-HD Video Analytics via Collaborative Vision Transformer Inference**
  - **tags:** [mlsys], [multi-modal inference], [vision transformer, cloud-device collaboration, patch-level importance scoring, dynamic scheduling, weighted ensembling]
  - **authors:** Linyi Jiang, Yifei Zhu, Hao Yin, Bo Li
  - **institution:** Shanghai Jiao Tong University, Tsinghua University, Hong Kong University of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.21730
  - **Simple LLM Summary:** Hyperion is a cloud-device collaborative framework that enables low-latency inference on Ultra-HD videos using vision transformers by identifying critical patches, dynamically adjusting transmission quality, and fusing edge-cloud results. It improves frame processing rate by up to 1.61× and accuracy by up to 20.2% compared to baselines under dynamic network conditions.

- **[arXiv251229] Demystifying ARM SME to Optimize General Matrix Multiplications**
  - **tags:** [mlsys], [GPU kernels], [ARM SME, cache-aware partitioning, data packing, micro-kernels, multi-vector loads, tile registers]
  - **authors:** Chencheng Deng, Weiling Yang, Jianbin Fang, Dezun Dong
  - **institution:** National University of Defense Technology
  - **link:** https://arxiv.org/pdf/2512.21473
  - **Simple LLM Summary:** This paper presents MpGEMM, an open-source library that optimizes General Matrix Multiplication (GEMM) by leveraging ARM's Scalable Matrix Extension (SME) through techniques like cache-aware partitioning, efficient data packing, and specialized micro-kernels. It demonstrates superior performance, achieving an average speedup of 1.23x over Apple's Accelerate library on real-world workloads from models like DeepSeek and LLaMA.

- **[arXiv251229] Proceedings First Workshop on Adaptable Cloud Architectures**
  - **tags:** TBD
  - **authors:** Giuseppe De Palma, Saverio Giallorenzo
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.22054

- **[arXiv251229] DeepCQ: General-Purpose Deep-Surrogate Framework for Lossy Compression Quality Prediction**
  - **tags:** [mlsys], [others], [deep-surrogate model, two-stage design, mixture-of-experts, error-bounded lossy compression, quality prediction]
  - **authors:** Khondoker Mirazul Mumenin, Robert Underwood, Dong Dai, Jinzhen Wang, Sheng Di, Zarija Lukić, Franck Cappello
  - **institution:** University of North Carolina at Charlotte, Argonne National Laboratory, University of Delaware, Lawrence Berkeley National Laboratory
  - **link:** https://arxiv.org/pdf/2512.21433
  - **Simple LLM Summary:** This paper presents DeepCQ, a general-purpose deep-surrogate framework for predicting the quality of lossy compressed scientific data. Its key innovations include a two-stage design separating feature extraction from prediction and a mixture-of-experts approach for handling time-evolving data. The framework demonstrates high predictive accuracy (errors generally under 10%), significantly reducing the computational overhead of quality assessment.

- **[arXiv251229] Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism**
  - **tags:** [mlsys], [llm inference], [mixture-of-experts, disaggregated expert parallelism, fine-grained task scheduling, FinDEP, KV cache, expert group, attention group]
  - **authors:** Xinglin Pan, Shaohuai Shi, Wenxiang Lin, Yuxin Wang, Zhenheng Tang, Wei Wang, Xiaowen Chu
  - **institution:** The Hong Kong University of Science and Technology (Guangzhou), Harbin Institute of Technology, Shenzhen, Hong Kong Baptist University, The Hong Kong University of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.21487
  - **Simple LLM Summary:** This paper proposes FinDEP, a fine-grained task scheduling algorithm for disaggregated expert parallelism (DEP) to improve the inference throughput of MoE models. It partitions computation and communication tasks for better pipelining and solves a scheduling optimization problem. Experiments show FinDEP achieves up to 1.61x higher throughput compared to prior methods.

- **[arXiv251229] Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models**
  - **tags:** [mlsys], [llm inference], [model parallelism, block placement, request routing, mixed integer linear programming, performance modeling, distributed inference]
  - **authors:** Tingyang Sun, Ting He, Bo Ji, Parimal Parag
  - **institution:** Pennsylvania State University, Virginia Tech, Indian Institute of Science
  - **link:** https://arxiv.org/pdf/2512.21884
  - **Simple LLM Summary:** This paper presents the first systematic study of resource allocation for distributed large language model (LLM) inference, focusing on optimizing block placement and request routing. The core method involves formulating the problem as a mixed integer linear program, proving its NP-hardness, and developing a polynomial-time algorithm with performance guarantees for both offline and online settings. The proposed solution is shown to substantially reduce inference time compared to the state-of-the-art in geographically-distributed server environments.

- **[arXiv251229] Agentic Structured Graph Traversal for Root Cause Analysis of Code-related Incidents in Cloud Applications**
  - **tags:** [mlsys], [fault-tolerance], [agentic workflow, structured graph traversal, service dependency graph (SDG), program dependence graph (PDG), hammock-block, root-cause analysis (RCA), LLM-driven traversal]
  - **authors:** Shengkun Cui, Rahul Krishna, Saurabh Jha, Ravishankar K. Iyer
  - **institution:** University of Illinois at Urbana-Champaign, IBM Research
  - **link:** https://arxiv.org/pdf/2512.22113
  - **Simple LLM Summary:** This paper introduces PRAXIS, an orchestrator that uses an LLM-driven agent to traverse service dependency graphs and program dependence graphs for diagnosing code-related cloud incidents. It demonstrates that this structured, agentic approach significantly improves root-cause analysis accuracy and reduces token consumption compared to baseline methods.

- **[arXiv251229] Robust Federated Fine-Tuning in Heterogeneous Networks with Unreliable Connections: An Aggregation View**
  - **tags:** [mlsys], [fault-tolerance], [federated fine-tuning, adaptive aggregation, connection failures, data heterogeneity, convergence guarantee, LoRA]
  - **authors:** Yanmeng Wang, Zhiwen Dai, Shuai Wang, Jian Zhou, Fu Xiao, Tony Q. S. Quek, Tsung-Hui Chang
  - **institution:** Nanjing University of Posts and Telecommunications, The Chinese University of Hong Kong, Shenzhen, Singapore University of Technology and Design
  - **link:** https://arxiv.org/pdf/2512.22035
  - **Simple LLM Summary:** The paper proposes FedAuto, a robust federated fine-tuning framework that uses adaptive aggregation to handle unreliable network connections and heterogeneous client data without prior knowledge of network conditions. It provides a strong convergence guarantee for each training round. Experiments show FedAuto outperforms existing methods under various failure scenarios for both full and partial-parameter fine-tuning like LoRA.

- **[arXiv251229] BLEST: Blazingly Efficient BFS using Tensor Cores**
  - **tags:** [mlsys], [GPU kernels], [Binarised Virtual Slice Sets (BVSS), graph reordering, batched SpMSpV multiplication, kernel fusion, lazy vertex update, Tensor Cores]
  - **authors:** Deniz Elbek, Kamer Kaya
  - **institution:** Sabanci University
  - **link:** https://arxiv.org/pdf/2512.21967
  - **Simple LLM Summary:** This paper presents BLEST, a framework that accelerates Breadth-First Search (BFS) on GPUs by efficiently mapping the algorithm onto Tensor Cores. Its key innovations include a bitmap-oriented structure with Binarised Virtual Slice Sets for load balancing, complementary graph reordering strategies, and a batched sparse matrix-sparse vector multiplication pattern. The experiments show that BLEST achieves significant speedups over state-of-the-art BFS implementations on a variety of real-world graphs.

- **[arXiv251229] FUSCO: High-Performance Distributed Data Shuffling via Transformation-Communication Fusion**
  - **tags:** [mlsys], [llm training], [expert parallelism, data shuffling, transformation-communication fusion, pipelined communication engine, load-balancing, Mixture-of-Experts (MoE)]
  - **authors:** Zhuoran Zhu, Chunyang Zhu, Hao Lin, Xu Fu, Yiming Zhou, Quanlu Zhang, Zhenhua Li, Feng Qian, Chao Yu, Boxun Li, Guohao Dai, Yu Wang
  - **institution:** Tsinghua University, Infinigence AI, University of Southern California, Zhongguancun Academy, Shanghai Jiaotong University
  - **link:** https://arxiv.org/pdf/2512.22036
  - **Simple LLM Summary:** The paper introduces FUSCO, a communication library that fuses data transformation and communication to optimize distributed data shuffling for Mixture-of-Experts (MoE) model training. It addresses the layout mismatch between expert-major data and device-major communication by using a pipelined engine and lightweight planning. The method significantly reduces training and inference latency compared to existing libraries like NCCL and DeepEP.


**cs.AI/cs.LG contains "reinforcement learning" total: 12**
- [arXiv251229] Variance-Aware Prior-Based Tree Policies for Monte Carlo Tree Search [link](https://arxiv.org/pdf/2512.21648)
- [arXiv251229] Multiconnectivity for SAGIN: Current Trends, Challenges, AI-driven Solutions, and Opportunities [link](https://arxiv.org/pdf/2512.21717)
- [arXiv251229] DiverseGRPO: Mitigating Mode Collapse in Image Generation via Diversity-Aware GRPO [link](https://arxiv.org/pdf/2512.21514)
- [arXiv251229] CosmoCore-Evo: Evolutionary Dream-Replay Reinforcement Learning for Adaptive Code Generation [link](https://arxiv.org/pdf/2512.21351)
- [arXiv251229] Videos are Sample-Efficient Supervisions: Behavior Cloning from Videos via Latent Representations [link](https://arxiv.org/pdf/2512.21586)
- [arXiv251229] A Reinforcement Learning Approach to Synthetic Data Generation [link](https://arxiv.org/pdf/2512.21395)
- [arXiv251229] Generative Actor Critic [link](https://arxiv.org/pdf/2512.21527)
- [arXiv251229] A Survey of Freshness-Aware Wireless Networking with Reinforcement Learning [link](https://arxiv.org/pdf/2512.21412)
- [arXiv251229] Leash: Adaptive Length Penalty and Reward Shaping for Efficient Large Reasoning Model [link](https://arxiv.org/pdf/2512.21540)
- [arXiv251229] dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning [link](https://arxiv.org/pdf/2512.21446)
- [arXiv251229] A Comedy of Estimators: On KL Regularization in RL Training of LLMs [link](https://arxiv.org/pdf/2512.21852)
- [arXiv251229] Meta-Learning-Based Handover Management in NextG O-RAN [link](https://arxiv.org/pdf/2512.22022)

**cs.AI/cs.LG contains "accelerate" total: 12**
- [arXiv251229] Variance-Aware Prior-Based Tree Policies for Monte Carlo Tree Search [link](https://arxiv.org/pdf/2512.21648)
- [arXiv251229] How Do Agents Perform Code Optimization? An Empirical Study [link](https://arxiv.org/pdf/2512.21757)
- [arXiv251229] BertsWin: Resolving Topological Sparsity in 3D Masked Autoencoders via Component-Balanced Structural Optimization [link](https://arxiv.org/pdf/2512.21769)
- [arXiv251229] Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism [link](https://arxiv.org/pdf/2512.21452)
- [arXiv251229] AVP-Fusion: Adaptive Multi-Modal Fusion and Contrastive Learning for Two-Stage Antiviral Peptide Identification [link](https://arxiv.org/pdf/2512.21544)
- [arXiv251229] dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning [link](https://arxiv.org/pdf/2512.21446)
- [arXiv251229] Discovering Sparse Recovery Algorithms Using Neural Architecture Search [link](https://arxiv.org/pdf/2512.21563)
- [arXiv251229] Hybrid Combinatorial Multi-armed Bandits with Probabilistically Triggered Arms [link](https://arxiv.org/pdf/2512.21925)
- [arXiv251229] DuaDeep-SeqAffinity: Dual-Stream Deep Learning Framework for Sequence-Only Antigen-Antibody Affinity Prediction [link](https://arxiv.org/pdf/2512.22007)
- [arXiv251229] StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars [link](https://arxiv.org/pdf/2512.22065)
- [arXiv251229] Prefill vs. Decode Bottlenecks: SRAM-Frequency Tradeoffs and the Memory-Bandwidth Ceiling [link](https://arxiv.org/pdf/2512.22066)
- [arXiv251229] Enabling Ultra-Fast Cardiovascular Imaging Across Heterogeneous Clinical Environments with a Generalist Foundation Model and Multimodal Database [link](https://arxiv.org/pdf/2512.21652)
