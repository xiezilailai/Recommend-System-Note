# 20260202-20260208

## 2026-02-02

**cs.DC total: 12**

- **[arXiv260202] Coordinating Power Grid Frequency Regulation Service with Data Center Load Flexibility**
  - **tags:** [mlsys], [cluster infrastructure], [frequency regulation, Exogenous Carbon, EcoCenter, GPU data centers, load flexibility]
  - **authors:** Ali Jahanshahi, Sara Rashidi Golrouye, Osten Anderson, Nanpeng Yu, Daniel Wong
  - **institution:** University of California, Riverside
  - **link:** https://arxiv.org/pdf/2601.22487
  - **Simple LLM Summary:** The paper introduces a framework called EcoCenter and a metric called Exogenous Carbon to enable GPU data centers to provide frequency regulation services to the power grid. It concludes that by coordinating with the grid, data centers can reduce the need for fossil-fueled reserves, and the resulting carbon savings often exceed the data centers' own operational emissions.

- **[arXiv260202] AsyncMesh: Fully Asynchronous Optimization for Data and Pipeline Parallelism**
  - **tags:** [mlsys], [llm training], [asynchronous optimization, sparse averaging, weight look-ahead, data parallelism, pipeline parallelism]
  - **authors:** Thalaiyasingam Ajanthan, Sameera Ramasinghe, Gil Avraham, Hadi Mohaghegh Dolatabadi, Chamin P Hewa Koneputugodage, Violetta Shevchenko, Yan Zuo, Alexander Long
  - **institution:** Pluralis Research
  - **link:** https://arxiv.org/pdf/2601.22442
  - **Simple LLM Summary:** This paper introduces AsyncMesh, a fully asynchronous optimization method for data and pipeline parallelism that eliminates synchronization barriers to reduce communication overhead. It mitigates staleness using weight look-ahead for pipeline parallelism and asynchronous sparse averaging with an exponential moving average correction for data parallelism. Experiments on large language models up to 1B parameters show it matches synchronous baseline performance while significantly reducing communication costs.

- **[arXiv260202] Towards Resiliency in Large Language Model Serving with KevlarFlow**
  - **tags:** [mlsys], [fault-tolerance], [decoupled model parallelism initialization, dynamic traffic rerouting, background KV cache replication, model parallelism, tensor parallelism, pipeline parallelism]
  - **authors:** Shangshu Qian, Kipling Liu, P. C. Sruthi, Lin Tan, Yongle Zhang
  - **institution:** Purdue University
  - **link:** https://arxiv.org/pdf/2601.22438
  - **Simple LLM Summary:** The paper introduces KevlarFlow, a fault-tolerant serving architecture for LLMs that uses decoupled model parallelism initialization, dynamic traffic rerouting, and background KV cache replication to handle hardware failures. It significantly reduces recovery time and improves latency and time-to-first-token metrics during failures compared to existing systems.

- **[arXiv260202] FedAdaVR: Adaptive Variance Reduction for Robust Federated Learning under Limited Client Participation**
  - **tags:** [mlsys], [others], [federated learning, variance reduction, adaptive optimizer, quantization, client drift, partial client participation]
  - **authors:** S M Ruhul Kabir Howlader, Xiao Chen, Yifei Xie, Lu Liu
  - **institution:** University of Leicester, University of Edinburgh, University of Exeter
  - **link:** https://arxiv.org/pdf/2601.22204
  - **Simple LLM Summary:** The paper proposes FedAdaVR, a federated learning algorithm that combines an adaptive optimizer with variance reduction to address errors from sporadic client participation by reusing stored client updates. It also introduces a quantized version, FedAdaVR-Quant, to reduce memory overhead. The method is proven to eliminate partial participation error and is shown to outperform state-of-the-art baselines in experiments.

- **[arXiv260202] Learning Provably Correct Distributed Protocols Without Human Knowledge**
  - **tags:** [mlsys], [fault-tolerance], [Monte Carlo Tree Search, transformer-based action encoder, depth-first search, model checking, Satisfiability Modulo Theories (SMT)]
  - **authors:** Yujie Hui, Xiaoyi Lu, Andrew Perrault, Yang Wang
  - **institution:** The Ohio State University, University of Florida
  - **link:** https://arxiv.org/pdf/2601.22369
  - **Simple LLM Summary:** The paper proposes GGMS, a learning framework that combines a specialized Monte Carlo Tree Search with a transformer-based action encoder, global depth-first search, and model checker feedback to automatically design provably correct distributed protocols. It proves the search is complete under mild assumptions and demonstrates that GGMS can learn correct protocols for larger settings than existing methods.

- **[arXiv260202] CONCUR: High-Throughput Agentic Batch Inference of LLM via Congestion-Based Concurrency Control**
  - **tags:** [mlsys], [llm inference], [KV cache management, admission control, congestion control, batch inference, agentic workloads]
  - **authors:** Qiaoling Chen, Zhisheng Ye, Tian Tang, Peng Sun, Boyu Tian, Guoteng Wang, Shenggui Li, Yonggang Wen, Zhenhua Han, Tianwei Zhang
  - **institution:** Nanyang Technological University, Shanghai Qiji Zhifeng Co., Ltd.
  - **link:** https://arxiv.org/pdf/2601.22705
  - **Simple LLM Summary:** The paper introduces CONCUR, a proactive agent-level admission control system that prevents KV cache thrashing in LLM batch inference by dynamically regulating the number of active agents based on runtime cache pressure. It improves throughput significantly, up to 4.09x on Qwen3-32B, and remains compatible with existing serving systems.

- **[arXiv260202] ZK-HybridFL: Zero-Knowledge Proof-Enhanced Hybrid Ledger for Federated Learning**
  - **tags:** [mlsys], [others], [zero-knowledge proofs, directed acyclic graph, sidechains, federated learning, smart contracts, blockchain]
  - **authors:** Amirhossein Taherpour, Xiaodong Wang
  - **institution:** Columbia University
  - **link:** https://arxiv.org/pdf/2601.22302
  - **Simple LLM Summary:** This paper proposes ZK-HybridFL, a decentralized federated learning framework that uses a DAG-based hybrid ledger with sidechains and zero-knowledge proofs to privately and securely validate model updates. The method integrates event-driven smart contracts and a challenge mechanism to detect adversarial behavior. The authors conclude that ZK-HybridFL offers a scalable and secure solution, demonstrating faster convergence, higher accuracy, and robustness against attacks in experiments.

- **[arXiv260202] HetCCL: Accelerating LLM Training with Heterogeneous GPUs**
  - **tags:** [mlsys], [cluster infrastructure], [collective communication library, RDMA, NCCL, RCCL, heterogeneous GPUs, All-Reduce]
  - **authors:** Heehoon Kim, Jaehwan Lee, Taejeoung Kim, Jongwon Park, Jinpyo Kim, Pyongwon Suh, Ryan H. Choi, Sangwoo Lee, Jaejin Lee
  - **institution:** Seoul National University, Samsung Research, Moreh Inc.
  - **link:** https://arxiv.org/pdf/2601.22585
  - **Simple LLM Summary:** The paper introduces HetCCL, a collective communication library that enables RDMA-based communication across heterogeneous GPUs from different vendors (e.g., NVIDIA and AMD) without requiring driver modifications. It unifies vendor-specific backends like NCCL and RCCL to facilitate cross-vendor communication. Evaluations show that HetCCL matches homogeneous performance and uniquely scales in heterogeneous environments, enabling high-performance LLM training without changes to existing applications.

- **[arXiv260202] SAIR: Cost-Efficient Multi-Stage ML Pipeline Autoscaling via In-Context Reinforcement Learning**
  - **tags:** [mlsys], [llm inference], [in-context reinforcement learning, Pareto-dominance reward shaping, surprisal-guided experience retrieval, GPU rate control, CUDA interception, regret analysis]
  - **authors:** Jianchang Su, Yifan Zhang, Shengkai Lin, Shizhen Zhao, Yusheng Zheng, Yiwei Yang, Wei Zhang
  - **institution:** University of Connecticut, Shanghai Jiao Tong University, University of California, Santa Cruz
  - **link:** https://arxiv.org/pdf/2601.22397
  - **Simple LLM Summary:** This paper presents SAIR, an autoscaling framework for multi-stage ML inference pipelines that uses an LLM as an in-context reinforcement learning controller to learn scaling policies online without gradient updates. It combines reward shaping, efficient experience retrieval, and fine-grained GPU control to dynamically manage resources. The method achieves significant improvements in latency and cost reduction compared to existing baselines.

- **[arXiv260202] SQUAD: Scalable Quorum Adaptive Decisions via ensemble of early exit neural networks**
  - **tags:** [mlsys], [llm inference], [early-exit neural networks, ensemble learning, quorum-based stopping, neural architecture search, distributed inference]
  - **authors:** Matteo Gambella, Fabrizio Pittorino, Giuliano Casale, Manuel Roveri
  - **institution:** Politecnico di Milano, Imperial College London
  - **link:** https://arxiv.org/pdf/2601.22711
  - **Simple LLM Summary:** This paper introduces SQUAD, an inference scheme that combines early-exit neural networks with distributed ensemble learning, using a quorum-based voting mechanism to decide when to stop computation. It also proposes QUEST, a neural architecture search method to optimize the diversity of the ensemble learners. The method improves test accuracy by up to 5.95% compared to dynamic baselines and reduces inference latency by up to 70.60% compared to static ensembles.

- **[arXiv260202] ERA: Epoch-Resolved Arbitration for Duelling Admins in Group Management CRDTs**
  - **tags:** [sys], [distributed systems], [CRDTs, Byzantine fault tolerance, epoch events, finality, arbitration]
  - **authors:** Kegan Dougal
  - **institution:** Element Creations Ltd
  - **link:** https://arxiv.org/pdf/2601.22963
  - **Simple LLM Summary:** The paper proposes ERA, a method using epoch events and external arbitration to resolve the "Duelling Admins" problem in group management CRDTs. It introduces a bounded total order within epochs to prevent Byzantine admins from exploiting concurrency, thereby improving consistency and providing finality.

- **[arXiv260202] AscendCraft: Automatic Ascend NPU Kernel Generation via DSL-Guided Transcompilation**
  - **tags:** [mlsys], [GPU kernels], [DSL-guided transcompilation, large language models, AscendC kernel generation, MultiKernelBench]
  - **authors:** Zhongzhen Wen, Shudi Shao, Zhong Li, Yu Ge, Tongtong Xu, Yuanyi Lin, Tian Zhang
  - **institution:** Nanjing University, Huawei
  - **link:** https://arxiv.org/pdf/2601.22760
  - **Simple LLM Summary:** AscendCraft introduces a lightweight domain-specific language (DSL) to abstract complexity and model Ascend NPU execution semantics, enabling LLMs to generate kernels via structured transcompilation. It achieves high compilation success and functional correctness, with many kernels matching or surpassing PyTorch eager performance, demonstrating effective NPU kernel generation.


**cs.AI/cs.LG contains "reinforcement learning" total: 39**
- [arXiv260202] RulePlanner: All-in-One Reinforcement Learner for Unifying Design Rules in 3D Floorplanning [link](https://arxiv.org/pdf/2601.22476)
- [arXiv260202] Adapting Reinforcement Learning for Path Planning in Constrained Parking Scenarios [link](https://arxiv.org/pdf/2601.22545)
- [arXiv260202] ShellForge: Adversarial Co-Evolution of Webshell Generation and Multi-View Detection for Robust Webshell Defense [link](https://arxiv.org/pdf/2601.22182)
- [arXiv260202] Real-Time Aligned Reward Model beyond Semantics [link](https://arxiv.org/pdf/2601.22664)
- [arXiv260202] Models Under SCOPE: Scalable and Controllable Routing via Pre-hoc Reasoning [link](https://arxiv.org/pdf/2601.22323)
- [arXiv260202] Unrewarded Exploration in Large Language Models Reveals Latent Learning from Psychology [link](https://arxiv.org/pdf/2601.22474)
- [arXiv260202] Learn More with Less: Uncertainty Consistency Guided Query Selection for RLVR [link](https://arxiv.org/pdf/2601.22595)
- [arXiv260202] Action-Sufficient Goal Representations [link](https://arxiv.org/pdf/2601.22496)
- [arXiv260202] Aligning Microscopic Vehicle and Macroscopic Traffic Statistics: Reconstructing Driving Behavior from Partial Data [link](https://arxiv.org/pdf/2601.22242)
- [arXiv260202] Exo-Plore: Exploring Exoskeleton Control Space through Human-aligned Simulation [link](https://arxiv.org/pdf/2601.22550)
- [arXiv260202] Continual Policy Distillation from Distributed Reinforcement Learning Teachers [link](https://arxiv.org/pdf/2601.22475)
- [arXiv260202] Latent Spherical Flow Policy for Reinforcement Learning with Combinatorial Actions [link](https://arxiv.org/pdf/2601.22211)
- [arXiv260202] Quantum-Inspired Reinforcement Learning for Secure and Sustainable AIoT-Driven Supply Chain Systems [link](https://arxiv.org/pdf/2601.22339)
- [arXiv260202] From Self-Evolving Synthetic Data to Verifiable-Reward RL: Post-Training Multi-turn Interactive Tool-Using Agents [link](https://arxiv.org/pdf/2601.22607)
- [arXiv260202] Detect and Act: Automated Dynamic Optimizer through Meta-Black-Box Optimization [link](https://arxiv.org/pdf/2601.22542)
- [arXiv260202] Learning Reward Functions for Cooperative Resilience in Multi-Agent Systems [link](https://arxiv.org/pdf/2601.22292)
- [arXiv260202] A Step Back: Prefix Importance Ratio Stabilizes Policy Optimization [link](https://arxiv.org/pdf/2601.22718)
- [arXiv260202] TSPO: Breaking the Double Homogenization Dilemma in Multi-turn Search Policy Optimization [link](https://arxiv.org/pdf/2601.22776)
- [arXiv260202] Clipping-Free Policy Optimization for Large Language Models [link](https://arxiv.org/pdf/2601.22801)
- [arXiv260202] CVeDRL: An Efficient Code Verifier via Difficulty-aware Reinforcement Learning [link](https://arxiv.org/pdf/2601.22803)
- [arXiv260202] Offline Reinforcement Learning of High-Quality Behaviors Under Robust Style Alignment [link](https://arxiv.org/pdf/2601.22823)
- [arXiv260202] Degradation-Aware Frequency Regulation of a Heterogeneous Battery Fleet via Reinforcement Learning [link](https://arxiv.org/pdf/2601.22865)
- [arXiv260202] Reinforcement Learning-Based Co-Design and Operation of Chiller and Thermal Energy Storage for Cost-Optimal HVAC Systems [link](https://arxiv.org/pdf/2601.22880)
- [arXiv260202] PlatoLTL: Learning to Generalize Across Symbols in LTL Instructions for Multi-Task RL [link](https://arxiv.org/pdf/2601.22891)
- [arXiv260202] MulFeRL: Enhancing Reinforcement Learning with Verbal Feedback in a Multi-turn Loop [link](https://arxiv.org/pdf/2601.22900)
- [arXiv260202] MTDrive: Multi-turn Interactive Reinforcement Learning for Autonomous Driving [link](https://arxiv.org/pdf/2601.22930)
- [arXiv260202] Golden Goose: A Simple Trick to Synthesize Unlimited RLVR Tasks from Unverifiable Internet Text [link](https://arxiv.org/pdf/2601.22975)
- [arXiv260202] Automatic Constraint Policy Optimization based on Continuous Constraint Interpolation Framework for Offline Reinforcement Learning [link](https://arxiv.org/pdf/2601.23010)
- [arXiv260202] Mem-T: Densifying Rewards for Long-Horizon Memory Agents [link](https://arxiv.org/pdf/2601.23014)
- [arXiv260202] Guided by Trajectories: Repairing and Rewarding Tool-Use Trajectories for Tool-Integrated Reasoning [link](https://arxiv.org/pdf/2601.23032)
- [arXiv260202] From Absolute to Relative: Rethinking Reward Shaping in Group-Based Reinforcement Learning [link](https://arxiv.org/pdf/2601.23058)
- [arXiv260202] RN-D: Discretized Categorical Actors with Regularized Networks for On-Policy Reinforcement Learning [link](https://arxiv.org/pdf/2601.23075)
- [arXiv260202] Why GRPO Needs Normalization: A Local-Curvature Perspective on Adaptive Gradients [link](https://arxiv.org/pdf/2601.23135)
- [arXiv260202] THINKSAFE: Self-Generated Safety Alignment for Reasoning Models [link](https://arxiv.org/pdf/2601.23143)
- [arXiv260202] On Safer Reinforcement Learning Policies for Sedation and Analgesia in Intensive Care [link](https://arxiv.org/pdf/2601.23154)
- [arXiv260202] Unsupervised Hierarchical Skill Discovery [link](https://arxiv.org/pdf/2601.23156)
- [arXiv260202] Med-Scout: Curing MLLMs' Geometric Blindness in Medical Perception via Geometry-Aware RL Post-Training [link](https://arxiv.org/pdf/2601.23220)
- [arXiv260202] Agile Reinforcement Learning through Separable Neural Architecture [link](https://arxiv.org/pdf/2601.23225)
- [arXiv260202] IRL-DAL: Safe and Adaptive Trajectory Planning for Autonomous Driving via Energy-Guided Diffusion Models [link](https://arxiv.org/pdf/2601.23266)

**cs.AI/cs.LG contains "accelerate" total: 10**
- [arXiv260202] Predicting Intermittent Job Failure Categories for Diagnosis Using Few-Shot Fine-Tuned Language Models [link](https://arxiv.org/pdf/2601.22264)
- [arXiv260202] Scalable Topology-Preserving Graph Coarsening with Graph Collapse [link](https://arxiv.org/pdf/2601.22943)
- [arXiv260202] Why GRPO Needs Normalization: A Local-Curvature Perspective on Adaptive Gradients [link](https://arxiv.org/pdf/2601.23135)
- [arXiv260202] Unsupervised Hierarchical Skill Discovery [link](https://arxiv.org/pdf/2601.23156)
- [arXiv260202] TriSpec: Ternary Speculative Decoding via Lightweight Proxy Verification [link](https://arxiv.org/pdf/2601.23180)
- [arXiv260202] YuriiFormer: A Suite of Nesterov-Accelerated Transformers [link](https://arxiv.org/pdf/2601.23236)
- [arXiv260202] Spectral Gradient Descent Mitigates Anisotropy-Driven Misalignment: A Case Study in Phase Retrieval [link](https://arxiv.org/pdf/2601.22652)
- [arXiv260202] A Cross-Domain Graph Learning Protocol for Single-Step Molecular Geometry Refinement [link](https://arxiv.org/pdf/2601.22723)
- [arXiv260202] Disentangling multispecific antibody function with graph neural networks [link](https://arxiv.org/pdf/2601.23212)
- [arXiv260202] Nested Slice Sampling: Vectorized Nested Sampling for GPU-Accelerated Inference [link](https://arxiv.org/pdf/2601.23252)

## 2026-02-03

**cs.DC total: 31**

- **[arXiv260203] Stabilizing Decentralized Federated Fine-Tuning via Topology-Aware Alternating LoRA**
  - **tags:** [mlsys], [llm training], [decentralized federated learning, low-rank adaptation, alternating optimization, parameter-efficient fine-tuning, topology-aware]
  - **authors:** Xiaoyu Wang, Xiaotian Li, Zhixiang Zhou, Chen Li, Yong Liu
  - **institution:** New York University
  - **link:** https://arxiv.org/pdf/2602.00451
  - **Simple LLM Summary:** The paper proposes TAD-LoRA, a topology-aware decentralized framework that coordinates alternating updates and mixing of LoRA factors to stabilize federated fine-tuning under dynamic communication graphs. It theoretically proves convergence and shows the method achieves robust performance, particularly delivering clear gains under moderately and weakly connected topologies.

- **[arXiv260203] Self-Attention at Constant Cost per Token via Symmetry-Aware Taylor Approximation**
  - **tags:** [mlsys], [llm inference], [self-attention, Taylor approximation, symmetric tensor products, polynomial-kernel feature basis, constant cost per token]
  - **authors:** Franz A. Heinsen, Leo Kozachkov
  - **institution:** GlassRoom Software LLC, Brown University
  - **link:** https://arxiv.org/pdf/2602.00294
  - **Simple LLM Summary:** This paper introduces a method to compute self-attention with constant cost per token by using a symmetry-aware Taylor approximation, decomposing the expansion into symmetric tensor product chains. This enables efficient, unbounded token generation with fixed memory and computational requirements. The approach significantly reduces the infrastructure and energy demands of large-scale Transformer models.

- **[arXiv260203] Standardized Methods and Recommendations for Green Federated Learning**
  - **tags:** [mlsys], [others], [federated learning, carbon accounting, energy measurement, NVFlare, CodeCarbon]
  - **authors:** Austin Tapp, Holger R. Roth, Ziyue Xu, Abhijeet Parida, Hareem Nisar, Marius George Linguraru
  - **institution:** Children’s National Hospital, NVIDIA
  - **link:** https://arxiv.org/pdf/2602.00343
  - **Simple LLM Summary:** The paper proposes a standardized carbon-accounting methodology for federated learning using NVIDIA NVFlare and CodeCarbon to track CO2e emissions across different training phases and client configurations. It demonstrates that system-level inefficiencies and hardware heterogeneity can significantly increase the carbon footprint, highlighting the need for consistent measurement and reporting. The main conclusion is that standardized methods are essential for reproducible and comparable "green" FL evaluations.

- **[arXiv260203] VoxServe: Streaming-Centric Serving System for Speech Language Models**
  - **tags:** [mlsys], [multi-modal inference], [streaming-aware scheduling, asynchronous inference pipeline, model-execution abstraction]
  - **authors:** Keisuke Kamahori, Wei-Tzu Lee, Atindra Jha, Rohan Kadekodi, Stephanie Wang, Arvind Krishnamurthy, Baris Kasikci
  - **institution:** University of Washington, Stanford University
  - **link:** https://arxiv.org/pdf/2602.00269
  - **Simple LLM Summary:** VoxServe is a unified serving system for Speech Language Models that introduces a model-execution abstraction to decouple architecture from system optimizations, enabling support for diverse models. It employs streaming-aware scheduling and an asynchronous inference pipeline to improve efficiency. The system achieves 10-20x higher throughput than existing implementations at comparable latency while maintaining high streaming viability.

- **[arXiv260203] FedMOA: Federated GRPO for Personalized Reasoning LLMs under Heterogeneous Rewards**
  - **tags:** [mlsys], [post-training], [federated learning, group relative policy optimization, multi-objective reinforcement learning, hypergradient descent, adaptive weighting, task-aware aggregation]
  - **authors:** Ziyao Wang, Daeun Jung, Yexiao He, Guoheng Sun, Zheyu Shen, Myungjin Lee, Ang Li
  - **institution:** University of Maryland, College Park, Cisco Research
  - **link:** https://arxiv.org/pdf/2602.00453
  - **Simple LLM Summary:** This paper proposes FedMOA, a federated learning framework that adapts Group Relative Policy Optimization (GRPO) for personalized reasoning LLMs under heterogeneous client rewards. It introduces an online adaptive weighting mechanism for local training and a task-aware aggregation strategy on the server. Experiments show FedMOA outperforms standard federated averaging, improving accuracy, personalization, and multi-objective balance.

- **[arXiv260203] A Fault-Tolerant Version of Safra's Termination Detection Algorithm**
  - **tags:** [sys], [distributed algorithms], [termination detection, fault tolerance, logical token ring, model checking, Safra's algorithm]
  - **authors:** Wan Fokkink, Georgios Karlos, Andy Tatman
  - **institution:** Vrije Universiteit Amsterdam, Paderborn University, University of Groningen
  - **link:** https://arxiv.org/pdf/2602.00272
  - **Simple LLM Summary:** This paper presents a fault-tolerant adaptation of Safra's classic distributed termination detection algorithm. The core method involves splitting the global token counter into per-node counters to isolate crashes, locally restoring the token ring, and propagating crash information via the token itself. The main conclusion is that this modified algorithm tolerates any number of simultaneous crashes in a decentralized manner without adding extra message overhead.

- **[arXiv260203] Asynchronous MultiAgent Reinforcement Learning for 5G Routing under Side Constraints**
  - **tags:** [mlsys], [others], [multi-agent reinforcement learning, asynchronous learning, PPO, routing, O-RAN]
  - **authors:** Sebastian Racedo, Brigitte Jaumard, Oscar Delgado, Meysam Masoudi
  - **institution:** Concordia University, Ecole de Technologie Supérieure (ETS), Ericsson
  - **link:** https://arxiv.org/pdf/2602.00035
  - **Simple LLM Summary:** This paper proposes an asynchronous multi-agent reinforcement learning (AMARL) framework where independent PPO agents, each specialized for a different service, plan routes in parallel and coordinate via a shared global resource environment. The method is evaluated on an O-RAN-like network simulation using real-time traffic data and achieves similar performance to a single-agent baseline but with reduced training time and improved robustness to demand shifts, demonstrating a scalable approach to distributed routing.

- **[arXiv260203] What Artificial Intelligence can do for High-Performance Computing systems?**
  - **tags:** [mlsys], [cluster infrastructure], [scheduling, performance estimation, graph neural networks, time-series models, language models, reinforcement learning, surrogate modeling, fault detection]
  - **authors:** Pierrick Pochelu, Hyacinthe Cartiaux, Julien Schleich
  - **institution:** LuxProvide S.A., University of Luxembourg
  - **link:** https://arxiv.org/pdf/2602.00014
  - **Simple LLM Summary:** This paper reviews how artificial intelligence (AI) and machine learning (ML) techniques can improve the operational efficiency of High-Performance Computing (HPC) systems. It synthesizes findings from 74 publications, identifying key application areas like scheduling, performance optimization, and fault detection. The main conclusion is that AI integration, particularly through hybrid schedulers and domain-specialized language models, offers significant opportunities for HPC efficiency but requires advances in MLOps and standardization.

- **[arXiv260203] Training LLMs with Fault Tolerant HSDP on 100,000 GPUs**
  - **tags:** [mlsys], [fault-tolerance], [FT-HSDP, Fault Tolerant All Reduce (FTAR), non-blocking catch-up protocol, Hybrid-Shared Data Parallelism (HSDP), data parallel replicas]
  - **authors:** Omkar Salpekar, Rohan Varma, Kenny Yu, Vladimir Ivanov, Yang Wang, Ahmed Sharif, Min Si, Shawn Xu, Feng Tian, Shengbao Zheng, Tristan Rice, Ankush Garg, Shangfu Peng, Shreyas Siravara, Wenyin Fu, Rodrigo de Castro, Adithya Gangidi, Andrey Obraztsov, Sharan Narang, Sergey Edunov, Maxim Naumov, Chunqiang Tang, Mathew Oldham
  - **institution:** Meta Platforms, Thinking Machines Labs, The Ohio State University, Genesis Molecular AI
  - **link:** https://arxiv.org/pdf/2602.00277
  - **Simple LLM Summary:** The paper proposes Fault Tolerant Hybrid-Shared Data Parallelism (FT-HSDP), a novel training paradigm that isolates failures to individual data-parallel replicas and uses a Fault Tolerant All Reduce protocol and non-blocking catch-up to minimize recovery stalls. This method reduces failure recovery stall time from 10 minutes to 3 minutes at O(100K) GPU scale, increasing effective training time from 44% to 80% without degrading model accuracy.

- **[arXiv260203] PROBE: Co-Balancing Computation and Communication in MoE Inference via Real-Time Predictive Prefetching**
  - **tags:** [mlsys], [llm inference], [mixture-of-experts, expert parallelism, continuous lookahead pipelining, predictive prefetching, gate-initialized lookahead predictor, hardware-aware balance planning, phase-locked co-scheduling]
  - **authors:** Qianchao Zhu, Xucheng Ye, Yuliang Liu, Haodong Ouyang, Chengru Song
  - **institution:** Kling Infra, Kuaishou Technology
  - **link:** https://arxiv.org/pdf/2602.00509
  - **Simple LLM Summary:** The paper proposes PROBE, an inference system for Mixture-of-Experts models that uses real-time predictive prefetching to balance computation and communication. Its core method involves predicting expert activations for upcoming layers and co-scheduling data transfers to hide network latency behind computation. The main conclusion is that PROBE reduces prefill latency and improves decoding throughput compared to state-of-the-art baselines, especially under volatile workloads.

- **[arXiv260203] Forecasting Energy Availability in Local Energy Communities via LSTM Federated Learning**
  - **tags:** [mlsys], [others], [federated learning, LSTM, forecasting, privacy, local energy communities]
  - **authors:** Fabio Turazza, Marcello Pietri, Natalia Selini Hadjidimitriou, Marco Mamei
  - **institution:** University of Modena and Reggio Emilia
  - **link:** https://arxiv.org/pdf/2602.00694
  - **Simple LLM Summary:** This paper proposes using Federated Learning combined with Long Short-Term Memory (LSTM) networks to forecast energy availability in Local Energy Communities while preserving user data privacy. The method allows for collaborative model training without sharing sensitive consumption data directly. The study demonstrates this approach as a viable solution, highlighting the trade-off between data sharing and forecasting accuracy.

- **[arXiv260203] Low-latency Federated LLM Fine-tuning Over Wireless Networks**
  - **tags:** [mlsys], [llm training], [federated learning, pruning, bandwidth allocation, block coordinate descent, parameter-efficient fine-tuning]
  - **authors:** Zhiwen Pang, Kang Wei, Long Shi, Zhe Wang, Jun Li, Feng Shu
  - **institution:** Nanjing University of Science and Technology, Southeast University, Hainan University
  - **link:** https://arxiv.org/pdf/2602.01024
  - **Simple LLM Summary:** This paper proposes a joint client-specific pruning and bandwidth allocation (JCPBA) framework to minimize the latency of federated fine-tuning for large language models over wireless networks. The method formulates and solves an optimization problem to jointly determine pruning rates and bandwidth allocations for heterogeneous clients. Experiments show the framework significantly reduces fine-tuning time while achieving comparable or better model performance with lower overhead.

- **[arXiv260203] MedBeads: An Agent-Native, Immutable Data Substrate for Trustworthy Medical AI**
  - **tags:** [mlsys], [llm inference], [Merkle DAG, Immutable Data, Breadth-First Search (BFS), FHIR-to-DAG conversion, Cryptographic Chain, Tamper-evidence]
  - **authors:** Takahito Nakajima
  - **institution:** University of Tsukuba
  - **link:** https://arxiv.org/pdf/2602.01086
  - **Simple LLM Summary:** The paper proposes MedBeads, an agent-native data infrastructure that represents clinical events as immutable nodes in a Merkle DAG to provide deterministic, tamper-evident context for AI agents. It concludes that this approach addresses the "Context Mismatch" in medical AI by shifting from probabilistic data retrieval to deterministic graph traversal, enabling more trustworthy clinical decision support.

- **[arXiv260203] Privocracy: Online Democracy through Private Voting**
  - **tags:** [sys], [access control], [private voting, verifiable secret sharing, k-anonymity, asynchronous multiparty-computation]
  - **authors:** Pedro Camponês, Hugo Pereira, Adrian Persaud, Kevin Gallagher, Santiago Torres-Arias
  - **institution:** NOVA LINCS & NOVA School of Science and Technology, Purdue University
  - **link:** https://arxiv.org/pdf/2602.01341
  - **Simple LLM Summary:** This paper introduces Privocracy, an access control mechanism that replaces unilateral administrative privileges with a secure e-voting procedure to authorize sensitive commands. The system achieves everlasting vote privacy and incorporates features like delegation and rapid voting rounds. The experimental results show that Privocracy is efficient and can be deployed on commodity hardware.

- **[arXiv260203] FUPareto: Bridging the Forgetting-Utility Gap in Federated Unlearning via Pareto Augmented Optimization**
  - **tags:** TBD
  - **authors:** Zeyan Wang, Zhengmao Liu, Yongxin Cai, Chi Li, Xiaoying Tang, Jingchao Chen, Zibin Pan, Jing Qiu
  - **institution:** 
  - **link:** https://arxiv.org/pdf/2602.01852

- **[arXiv260203] HyperOffload: Graph-Driven Hierarchical Memory Management for Large Language Models on SuperNode Architectures**
  - **tags:** [mlsys], [llm inference], [graph-driven memory management, compiler-assisted offloading, static scheduling, remote memory access, hierarchical supernode architecture]
  - **authors:** Fangxin Liu, Qinghua Zhang, Hanjing Shen, Qinghua Zhang, Zhibo Liang, Li Jiang, Haibing Guan, Chong Bao, Xuefeng Jin
  - **institution:** Shanghai Jiao Tong University, Huawei Technologies Co., Ltd.
  - **link:** https://arxiv.org/pdf/2602.00748
  - **Simple LLM Summary:** The paper proposes HyperOffload, a compiler-assisted framework that integrates remote memory access as explicit operations in the computation graph to manage memory for large language models on supernode architectures. It uses static scheduling to hide data transfer latency behind compute, reducing peak device memory usage by up to 26% for inference while maintaining performance. The work demonstrates that integrating memory-augmented hardware into compiler optimization is key for scaling next-generation AI workloads.

- **[arXiv260203] Fast Sparse Matrix Permutation for Mesh-Based Direct Solvers**
  - **tags:** [sys], [sparse linear algebra], [nested-dissection, sparse matrix permutation, sparse Cholesky factorization, elimination-tree, quotient-graph ordering]
  - **authors:** Behrooz Zarebavami, Ahmed H. Mahmoud, Ana Dodik, Changcheng Yuan, Serban D. Porumbescu, John D. Owens, Maryam Mehri Dehnavi, Justin Solomon
  - **institution:** University of Toronto, Massachusetts Institute of Technology, Texas A&M University, University of California, Davis, NVIDIA Research
  - **link:** https://arxiv.org/pdf/2602.00898
  - **Simple LLM Summary:** The paper presents a fast sparse matrix permutation algorithm designed for linear systems from triangle meshes. It relaxes strict balance and separator optimality to favor fast partitioning and efficient elimination-tree construction, decomposing permutation into patch-level orderings and a compact separator ordering. The method reduces permutation time and improves sparse Cholesky solve performance by up to 6.27x when integrated into vendor-maintained solvers on CPUs and GPUs.

- **[arXiv260203] FedBGS: A Blockchain Approach to Segment Gossip Learning in Decentralized Systems**
  - **tags:** [mlsys], [fault-tolerance], [federated learning, blockchain, gossip learning, differential privacy, homomorphic encryption, non-iid data, decentralized aggregation]
  - **authors:** Fabio Turazza, Marcello Pietri, Marco Picone, Marco Mamei
  - **institution:** University of Modena and Reggio Emilia
  - **link:** https://arxiv.org/pdf/2602.01185
  - **Simple LLM Summary:** This paper proposes FedBGS, a fully decentralized federated learning framework that integrates blockchain and segmented gossip learning to eliminate the central server as a single point of failure. It aims to enhance security, scalability, and privacy while handling non-IID data distributions. The main conclusion is that this approach provides comprehensive protection against various attacks and optimizes blockchain usage in federated environments.

- **[arXiv260203] Mean field optimal Core Allocation across Malleable jobs**
  - **tags:** [sys], [scheduling theory], [mean field analysis, Whittle policy, concave speedup functions, core allocation, malleable jobs]
  - **authors:** Zhouzi Li, Mor Harchol-Balter, Benjamin Berg
  - **institution:** Carnegie Mellon University, UNC Chapel Hill
  - **link:** https://arxiv.org/pdf/2602.01411
  - **Simple LLM Summary:** This paper solves the Core Allocation to Malleable jobs (CAM) problem by analyzing it in the mean field asymptotic regime, deriving two optimal policies: FW-CAM and WHAM. The main conclusion is that WHAM is asymptotically optimal and serves as a good heuristic, and that job sizes are irrelevant for optimality in the mean field regime when jobs have different concave speedup functions.

- **[arXiv260203] System-Level Performance Modeling of Photonic In-Memory Computing**
  - **tags:** [sys], [photonic computing], [photonic in-memory computing, system-level performance modeling, algorithm-to-hardware mapping, photonic SRAM, opto-electronic conversion]
  - **authors:** Jebacyril Arockiaraj, Sasindu Wijeratne, Sugeet Sunder, Md Abdullah-Al Kaiser, Akhilesh Jaiswal, Ajey P. Jacob, Viktor Prasanna
  - **institution:** University of Southern California, University of Wisconsin-Madison
  - **link:** https://arxiv.org/pdf/2602.00892
  - **Simple LLM Summary:** This paper develops a comprehensive system-level performance model for photonic in-memory computing, accounting for key latencies like external memory access and opto-electronic conversion. By mapping diverse high-performance computing workloads to a photonic SRAM array architecture, the model demonstrates that the system can sustain high throughput (up to 1.5 TOPS) with an average energy efficiency of 2.5 TOPS/W, even when considering system overheads.

- **[arXiv260203] BOA Constrictor: Squeezing Performance out of GPUs in the Cloud via Budget-Optimal Allocation**
  - **tags:** [mlsys], [cluster infrastructure], [budget-optimal allocation, scheduling, job completion time, cost-performance tradeoff, GPU allocation, stochastic model]
  - **authors:** Zhouzi Li, Cindy Zhu, Arpan Mukhopadhyay, Mor Harchol-Balter, Benjamin Berg
  - **institution:** Carnegie Mellon University, University of Warwick, UNC Chapel Hill
  - **link:** https://arxiv.org/pdf/2602.01404
  - **Simple LLM Summary:** The paper introduces BOA Constrictor, a scheduler that uses a Budget-Optimal Allocation (BOA) policy to minimize the average job completion time for ML training jobs under a fixed monetary budget constraint in the cloud. It formulates the problem as a budget-constrained scheduling problem and derives an efficient optimal policy. The results show that BOA Constrictor can reduce average job completion time by up to 2x compared to state-of-the-art heuristic schedulers for a given budget.

- **[arXiv260203] Developing a Portable Solution for Post-Event Analysis Pipelines**
  - **tags:** [mlsys], [cluster infrastructure], [Science Gateways, Apache Airflow, Photogrammetry, Machine Learning, Data Visualization, Directed Acyclic Graphs (DAGs)]
  - **authors:** Leonardo Pelonero, Fabio Vitello, Eva Sciacca, Mauro Imbrosciano, Salvatore Scavo, Ugo Becciani
  - **institution:** INAF Astrophysical Observatory of Catania
  - **link:** https://arxiv.org/pdf/2602.01798
  - **Simple LLM Summary:** This paper presents a Science Gateway framework that integrates photogrammetry, data visualization, and AI into an automated, portable pipeline for post-event natural hazard analysis using aerial images. The core method involves orchestrating these workflows via Apache Airflow within a Science Gateway platform to enhance risk assessment and mitigation. The main conclusion is that this integrated, automated approach supports timely decision-making for disaster response and asset impact evaluation.

- **[arXiv260203] Grappa: Gradient-Only Communication for Scalable Graph Neural Network Training**
  - **tags:** [mlsys], [others], [gradient-only communication, coverage-corrected gradient aggregation, periodic repartitioning]
  - **authors:** Chongyang Xu, Christoph Siebenbrunner, Laurent Bindschaedler
  - **institution:** Max Planck Institute for Software Systems (MPI-SWS), Vienna University of Economics and Business (WU)
  - **link:** https://arxiv.org/pdf/2602.01872
  - **Simple LLM Summary:** Grappa is a distributed GNN training framework that eliminates per-iteration cross-partition communication by having partitions train in isolation and only exchange gradients. It recovers accuracy through periodic graph repartitioning and a corrected gradient aggregation technique. The method trains GNNs significantly faster than state-of-the-art systems while achieving better accuracy, especially for deeper models, and scales to trillion-edge graphs on commodity hardware.

- **[arXiv260203] Hierarchical Federated Learning with SignSGD: A Highly Communication-Efficient Approach**
  - **tags:** [mlsys], [others], [hierarchical federated learning, SignSGD, majority-vote aggregation, gradient compression, downlink quantization]
  - **authors:** Amirreza Kazemi, Seyed Mohammad Azimi-Abarghouyi, Gabor Fodor, Carlo Fischione
  - **institution:** KTH Royal Institute of Technology
  - **link:** https://arxiv.org/pdf/2602.02355
  - **Simple LLM Summary:** This paper proposes HierSignSGD, a hierarchical federated learning framework that uses one-bit gradient compression (SignSGD) and majority-vote aggregation at edge servers to drastically reduce communication overhead. The method is shown to achieve accuracy comparable to full-precision SGD while being robust to aggressive downlink sparsification and data heterogeneity.

- **[arXiv260203] LCLs Beyond Bounded Degrees**
  - **tags:** [sys], [distributed computing], [Locally Checkable Labelings, Locally Finite Labelings, LOCAL model, gap results, polynomial regime, trees, unbounded degree]
  - **authors:** Gustav Schmid
  - **institution:** University of Freiburg
  - **link:** https://arxiv.org/pdf/2602.02340
  - **Simple LLM Summary:** The paper investigates the complexity of Locally Checkable Labelings (LCLs) on trees with arbitrarily large degrees. It shows that polynomial gap results disappear if LCLs are defined with infinitely many configurations, but introduces Locally Finite Labelings (LFLs) to restore these gaps, proving their deterministic LOCAL complexity is either Θ(n^\{1/k\}) or O(log n).

- **[arXiv260203] vLLM-Omni: Fully Disaggregated Serving for Any-to-Any Multimodal Models**
  - **tags:** [mlsys], [multi-modal inference], [stage abstraction, disaggregated serving, request batching, GPU allocation, inter-stage connectors]
  - **authors:** Peiqi Yin, Jiangyun Zhu, Han Gao, Chenguang Zheng, Yongxiang Huang, Taichang Zhou, Ruirui Yang, Weizhi Liu, Weiqing Chen, Canlin Guo, Didan Deng, Zifeng Mo, Cong Wang, James Cheng, Roger Wang, Hongsheng Liu
  - **institution:** Huawei, The Chinese University of Hong Kong, Institute of Software Chinese Academy of Sciences, Sun Yat-sen University
  - **link:** https://arxiv.org/pdf/2602.02204
  - **Simple LLM Summary:** This paper introduces vLLM-Omni, a serving system that decomposes complex any-to-any multimodal models into a graph of interconnected stages, each independently executed with optimized resource management. It demonstrates that this fully disaggregated approach significantly improves serving efficiency, reducing job completion time by up to 91.4% compared to baseline methods.

- **[arXiv260203] ECHO-2: A Large Scale Distributed Rollout Framework for Cost-efficient Reinforcement Learning**
  - **tags:** [mlsys], [post-training], [distributed reinforcement learning, policy staleness, peer-assisted pipelined broadcast, cost-aware worker activation, overlap-based capacity model]
  - **authors:** Jie Xiao, Meng Chen, Qingnan Ren, Song Jingwei, Jiaqi Huang, Yangshen Deng, Chris Tong, Wanyi Chen, Suli Wang, Ziqian Bi, Shuo Lu, Yiqun Duan, Lynn Ai, Eric Yang, Bill Shi
  - **institution:** Gradient, Fudan University, The University of Hong Kong, University of Edinburgh, Technical University of Darmstadt
  - **link:** https://arxiv.org/pdf/2602.02192
  - **Simple LLM Summary:** This paper introduces ECHO-2, a distributed framework for cost-efficient reinforcement learning post-training of large language models. It enables overlapping rollout generation, policy dissemination, and centralized training by treating policy staleness as a controllable parameter and uses techniques like peer-assisted broadcast to mitigate wide-area latency. Experiments show that ECHO-2 significantly improves cost efficiency while maintaining reward performance comparable to strong baselines.

- **[arXiv260203] sVIRGO: A Scalable Virtual Tree Hierarchical Framework for Distributed Systems**
  - **tags:** [sys], [distributed systems], [virtual hierarchical tree, multi-region coordination, layer-scoped command execution, dynamic role mapping, fault tolerance]
  - **authors:** Lican Huang
  - **institution:** Hangzhou Domain Zones Technology Co., Ltd.
  - **link:** https://arxiv.org/pdf/2602.02438
  - **Simple LLM Summary:** The paper proposes sVIRGO, a scalable framework that constructs virtual hierarchical trees directly on physical nodes for large-scale distributed systems. It uses dynamic role mapping and multi-coordinator regions to achieve fault tolerance and efficient coordination with minimal recovery latency and bounded communication overhead.

- **[arXiv260203] Building a Correct-by-Design Lakehouse. Data Contracts, Versioning, and Transactional Pipelines for Humans and Agents**
  - **tags:** [mlsys], [cluster infrastructure], [data contracts, Git-like versioning, transactional pipelines, verification, lakehouse]
  - **authors:** Weiming Sheng, Jinlang Wang, Manuel Barros, Aldrin Montana, Jacopo Tagliabue, Luca Bigon
  - **institution:** Columbia University, University of Wisconsin-Madison, Carnegie Mellon University, Bauplan Labs
  - **link:** https://arxiv.org/pdf/2602.02335
  - **Simple LLM Summary:** The paper introduces Bauplan, a code-first lakehouse designed to ensure correctness by integrating typed data contracts for static checking, Git-like versioning for collaboration, and transactional pipeline execution for atomicity. Its core method combines these software engineering-inspired abstractions to prevent runtime errors and data leaks in multi-actor environments. The main conclusion is that this integrated approach makes data pipelines statically checkable, safe to run, and easy to review, addressing key safety concerns in analytics and AI workloads.

- **[arXiv260203] Enabling AI Deep Potentials for Ab Initio-quality Molecular Dynamics Simulations in GROMACS**
  - **tags:** [mlsys], [GPU kernels], [deep potentials, DeePMD-kit, GROMACS, molecular dynamics, attention mechanism, graph neural network, CUDA]
  - **authors:** Andong Hu, Luca Pennati, Stefano Markidis, Ivy Peng
  - **institution:** KTH Royal Institute of Technology
  - **link:** https://arxiv.org/pdf/2602.02234
  - **Simple LLM Summary:** This paper integrates AI deep potentials into the GROMACS molecular dynamics simulation code by coupling it with the DeePMD-kit backend. It evaluates two neural network architectures (DPA2 and DPA3) on protein benchmarks and finds that the attention-based DPA2 model achieves significantly higher throughput than the GNN-based DPA3 on GPUs. The main conclusion identifies kernel-launch overhead and domain-decomposed inference as key optimization targets for production MD simulations.

- **[arXiv260203] TriCloudEdge: A multi-layer Cloud Continuum**
  - **tags:** [mlsys], [others], [cloud continuum, edge computing, far-edge AI, multi-protocol communication, WebSocket, MQTT, Zenoh, federated learning, model adaptation]
  - **authors:** George Violettas, Lefteris Mamatas
  - **institution:** University of Macedonia (†)
  - **link:** https://arxiv.org/pdf/2602.02121
  - **Simple LLM Summary:** This paper proposes TriCloudEdge, a three-tier cloud continuum architecture that integrates far-edge devices, intermediate edge nodes, and central cloud services to distribute AI workloads. It compares a multi-protocol approach (WebSocket, MQTT, HTTP) with a versatile protocol (Zenoh) for data transfer, evaluating trade-offs in resource use and communication efficiency. The results demonstrate that the architecture can effectively address latency and privacy concerns by distributing computational challenges across the continuum.


**cs.AI/cs.LG contains "reinforcement learning" total: 101**
- [arXiv260203] Distributional Reinforcement Learning for Condition-Based Maintenance of Multi-Pump Equipment [link](https://arxiv.org/pdf/2602.00051)
- [arXiv260203] From Gameplay Traces to Game Mechanics: Causal Induction with Large Language Models [link](https://arxiv.org/pdf/2602.00190)
- [arXiv260203] AREAL-DTA: Dynamic Tree Attention for Efficient Reinforcement Learning of Large Language Models [link](https://arxiv.org/pdf/2602.00482)
- [arXiv260203] Variational Approach for Job Shop Scheduling [link](https://arxiv.org/pdf/2602.00408)
- [arXiv260203] Open Materials Generation with Inference-Time Reinforcement Learning [link](https://arxiv.org/pdf/2602.00424)
- [arXiv260203] LLMs as High-Dimensional Nonlinear Autoregressive Models with Attention: Training, Alignment and Inference [link](https://arxiv.org/pdf/2602.00426)
- [arXiv260203] DROGO: Default Representation Objective via Graph Optimization in Reinforcement Learning [link](https://arxiv.org/pdf/2602.00403)
- [arXiv260203] AdaFuse: Adaptive Multimodal Fusion for Lung Cancer Risk Prediction via Reinforcement Learning [link](https://arxiv.org/pdf/2602.00347)
- [arXiv260203] KEPO: Knowledge-Enhanced Preference Optimization for Reinforcement Learning with Reasoning [link](https://arxiv.org/pdf/2602.00400)
- [arXiv260203] Representation Learning Enhanced Deep Reinforcement Learning for Optimal Operation of Hydrogen-based Multi-Energy Systems [link](https://arxiv.org/pdf/2602.00027)
- [arXiv260203] ZEST: Zero-shot Embodied Skill Transfer for Athletic Robot Control [link](https://arxiv.org/pdf/2602.00401)
- [arXiv260203] Search Inspired Exploration in Reinforcement Learning [link](https://arxiv.org/pdf/2602.00460)
- [arXiv260203] Sample Complexity Analysis for Constrained Bilevel Reinforcement Learning [link](https://arxiv.org/pdf/2602.00282)
- [arXiv260203] Learning Robust Reasoning through Guided Adversarial Self-Play [link](https://arxiv.org/pdf/2602.00173)
- [arXiv260203] CamReasoner: Reinforcing Camera Movement Understanding via Structured Spatial Reasoning [link](https://arxiv.org/pdf/2602.00181)
- [arXiv260203] Joint Continual Learning of Local Language Models and Cloud Offloading Decisions with Budget Constraints [link](https://arxiv.org/pdf/2602.00166)
- [arXiv260203] Minerva: Reinforcement Learning with Verifiable Rewards for Cyber Threat Intelligence LLMs [link](https://arxiv.org/pdf/2602.00513)
- [arXiv260203] How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic Reasoning with Agentic Tool Use [link](https://arxiv.org/pdf/2602.00528)
- [arXiv260203] Reinforcement Learning-assisted Constraint Relaxation for Constrained Expensive Optimization [link](https://arxiv.org/pdf/2602.00532)
- [arXiv260203] Surrogate Ensemble in Expensive Multi-Objective Optimization via Deep Q-Learning [link](https://arxiv.org/pdf/2602.00540)
- [arXiv260203] Learning to Decode Against Compositional Hallucination in Video Multimodal Large Language Models [link](https://arxiv.org/pdf/2602.00559)
- [arXiv260203] Learning Modal-Mixed Chain-of-Thought Reasoning with Latent Embeddings [link](https://arxiv.org/pdf/2602.00574)
- [arXiv260203] Safe Langevin Soft Actor Critic [link](https://arxiv.org/pdf/2602.00587)
- [arXiv260203] Equilibrium of Feasible Zone and Uncertain Model in Safe Exploration [link](https://arxiv.org/pdf/2602.00636)
- [arXiv260203] SA-VLA: Spatially-Aware Flow-Matching for Vision-Language-Action Reinforcement Learning [link](https://arxiv.org/pdf/2602.00743)
- [arXiv260203] Adaptive Ability Decomposing for Unlocking Large Reasoning Model Effective Reinforcement Learning [link](https://arxiv.org/pdf/2602.00759)
- [arXiv260203] Communications-Incentivized Collaborative Reasoning in NetGPT through Agentic Reinforcement Learning [link](https://arxiv.org/pdf/2602.00766)
- [arXiv260203] Fast Non-Episodic Finite-Horizon RL with K-Step Lookahead Thresholding [link](https://arxiv.org/pdf/2602.00781)
- [arXiv260203] World Models as an Intermediary between Agents and the Real World [link](https://arxiv.org/pdf/2602.00785)
- [arXiv260203] Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement [link](https://arxiv.org/pdf/2602.00815)
- [arXiv260203] Learning Abstractions for Hierarchical Planning in Program-Synthesis Agents [link](https://arxiv.org/pdf/2602.00929)
- [arXiv260203] DISPO: Enhancing Training Efficiency and Stability in Reinforcement Learning for Large Language Model Mathematical Reasoning [link](https://arxiv.org/pdf/2602.00983)
- [arXiv260203] Reasoning and Tool-use Compete in Agentic RL:From Quantifying Interference to Disentangled Tuning [link](https://arxiv.org/pdf/2602.00994)
- [arXiv260203] ESSAM: A Novel Competitive Evolution Strategies Approach to Reinforcement Learning for Memory Efficient LLMs Fine-Tuning [link](https://arxiv.org/pdf/2602.01003)
- [arXiv260203] Discovering Process-Outcome Credit in Multi-Step LLM Reasoning [link](https://arxiv.org/pdf/2602.01034)
- [arXiv260203] Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning [link](https://arxiv.org/pdf/2602.01058)
- [arXiv260203] SetPO: Set-Level Policy Optimization for Diversity-Preserving LLM Reasoning [link](https://arxiv.org/pdf/2602.01062)
- [arXiv260203] Probing RLVR training instability through the lens of objective-level hacking [link](https://arxiv.org/pdf/2602.01103)
- [arXiv260203] Lyapunov Stability-Aware Stackelberg Game for Low-Altitude Economy: A Control-Oriented Pruning-Based DRL Approach [link](https://arxiv.org/pdf/2602.01131)
- [arXiv260203] Self-Generative Adversarial Fine-Tuning for Large Language Models [link](https://arxiv.org/pdf/2602.01137)
- [arXiv260203] PolicyFlow: Policy Optimization with Continuous Normalizing Flow in Reinforcement Learning [link](https://arxiv.org/pdf/2602.01156)
- [arXiv260203] Sample Efficient Active Algorithms for Offline Reinforcement Learning [link](https://arxiv.org/pdf/2602.01260)
- [arXiv260203] Mixture-of-World Models: Scaling Multi-Task Reinforcement Learning with Modular Latent Dynamics [link](https://arxiv.org/pdf/2602.01270)
- [arXiv260203] From Intents to Actions: Agentic AI in Autonomous Networks [link](https://arxiv.org/pdf/2602.01271)
- [arXiv260203] Adaptive Quantum-Safe Cryptography for 6G Vehicular Networks via Context-Aware Optimization [link](https://arxiv.org/pdf/2602.01342)
- [arXiv260203] CRAFT: Calibrated Reasoning with Answer-Faithful Traces via Reinforcement Learning for Multi-Hop Question Answering [link](https://arxiv.org/pdf/2602.01348)
- [arXiv260203] PromptRL: Prompt Matters in RL for Flow-Based Image Generation [link](https://arxiv.org/pdf/2602.01382)
- [arXiv260203] The Enhanced Physics-Informed Kolmogorov-Arnold Networks: Applications of Newton's Laws in Financial Deep Reinforcement Learning (RL) Algorithms [link](https://arxiv.org/pdf/2602.01388)
- [arXiv260203] TQL: Scaling Q-Functions with Transformers by Preventing Attention Collapse [link](https://arxiv.org/pdf/2602.01439)
- [arXiv260203] Provable Cooperative Multi-Agent Exploration for Reward-Free MDPs [link](https://arxiv.org/pdf/2602.01453)
- [arXiv260203] Alternating Reinforcement Learning for Rubric-Based Reward Modeling in Non-Verifiable LLM Post-Training [link](https://arxiv.org/pdf/2602.01511)
- [arXiv260203] A Relative-Budget Theory for Reinforcement Learning with Verifiable Rewards in Large Language Model Reasoning [link](https://arxiv.org/pdf/2602.01523)
- [arXiv260203] Making Bias Non-Predictive: Training Robust LLM Judges via Reinforcement Learning [link](https://arxiv.org/pdf/2602.01528)
- [arXiv260203] MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety [link](https://arxiv.org/pdf/2602.01539)
- [arXiv260203] Toward Cognitive Supersensing in Multimodal Large Language Model [link](https://arxiv.org/pdf/2602.01541)
- [arXiv260203] The Multiple Ticket Hypothesis: Random Sparse Subnetworks Suffice for RLVR [link](https://arxiv.org/pdf/2602.01599)
- [arXiv260203] Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards [link](https://arxiv.org/pdf/2602.01601)
- [arXiv260203] Boosting Maximum Entropy Reinforcement Learning via One-Step Flow Matching [link](https://arxiv.org/pdf/2602.01606)
- [arXiv260203] SUSD: Structured Unsupervised Skill Discovery through State Factorization [link](https://arxiv.org/pdf/2602.01619)
- [arXiv260203] Contribution-aware Token Compression for Efficient Video Understanding via Reinforcement Learning [link](https://arxiv.org/pdf/2602.01649)
- [arXiv260203] FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning [link](https://arxiv.org/pdf/2602.01664)
- [arXiv260203] TABX: A High-Throughput Sandbox Battle Simulator for Multi-Agent Reinforcement Learning [link](https://arxiv.org/pdf/2602.01665)
- [arXiv260203] TRIP-Bench: A Benchmark for Long-Horizon Interactive Agents in Real-World Scenarios [link](https://arxiv.org/pdf/2602.01675)
- [arXiv260203] Semantic-aware Wasserstein Policy Regularization for Large Language Model Alignment [link](https://arxiv.org/pdf/2602.01685)
- [arXiv260203] Restoring Exploration after Post-Training: Latent Exploration Decoding for Large Reasoning Models [link](https://arxiv.org/pdf/2602.01698)
- [arXiv260203] Mitigating loss of control in advanced AI systems through instrumental goal trajectories [link](https://arxiv.org/pdf/2602.01699)
- [arXiv260203] Beyond Mode Elicitation: Diversity-Preserving Reinforcement Learning via Latent Diffusion Reasoner [link](https://arxiv.org/pdf/2602.01705)
- [arXiv260203] Adversarial Reward Auditing for Active Detection and Mitigation of Reward Hacking [link](https://arxiv.org/pdf/2602.01750)
- [arXiv260203] Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting [link](https://arxiv.org/pdf/2602.01776)
- [arXiv260203] Grad2Reward: From Sparse Judgment to Dense Rewards for Improving Open-Ended LLM Reasoning [link](https://arxiv.org/pdf/2602.01791)
- [arXiv260203] Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It [link](https://arxiv.org/pdf/2602.01826)
- [arXiv260203] Designing Time Series Experiments in A/B Testing with Transformer Reinforcement Learning [link](https://arxiv.org/pdf/2602.01853)
- [arXiv260203] VLM-Guided Experience Replay [link](https://arxiv.org/pdf/2602.01915)
- [arXiv260203] Zero-Shot Off-Policy Learning [link](https://arxiv.org/pdf/2602.01962)
- [arXiv260203] Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models [link](https://arxiv.org/pdf/2602.01970)
- [arXiv260203] Bandwidth-Efficient Multi-Agent Communication through Information Bottleneck and Vector Quantization [link](https://arxiv.org/pdf/2602.02035)
- [arXiv260203] FORLER: Federated Offline Reinforcement Learning with Q-Ensemble and Actor Rectification [link](https://arxiv.org/pdf/2602.02055)
- [arXiv260203] Probabilistic Performance Guarantees for Multi-Task Reinforcement Learning [link](https://arxiv.org/pdf/2602.02098)
- [arXiv260203] Think Dense, Not Long: Dynamic Decoupled Conditional Advantage for Efficient Reasoning [link](https://arxiv.org/pdf/2602.02099)
- [arXiv260203] DCoPilot: Generative AI-Empowered Policy Adaptation for Dynamic Data Center Operations [link](https://arxiv.org/pdf/2602.02137)
- [arXiv260203] Learning Generative Selection for Best-of-N [link](https://arxiv.org/pdf/2602.02143)
- [arXiv260203] ECHO: Entropy-Confidence Hybrid Optimization for Test-Time Reinforcement Learning [link](https://arxiv.org/pdf/2602.02150)
- [arXiv260203] Online Fine-Tuning of Pretrained Controllers for Autonomous Driving via Real-Time Recurrent RL [link](https://arxiv.org/pdf/2602.02236)
- [arXiv260203] Learning While Staying Curious: Entropy-Preserving Supervised Fine-Tuning via Adaptive Self-Distillation for Large Reasoning Models [link](https://arxiv.org/pdf/2602.02244)
- [arXiv260203] Segment to Focus: Guiding Latent Action Models in the Presence of Distractors [link](https://arxiv.org/pdf/2602.02259)
- [arXiv260203] Learning Markov Decision Processes under Fully Bandit Feedback [link](https://arxiv.org/pdf/2602.02260)
- [arXiv260203] Choice-Model-Assisted Q-learning for Delayed-Feedback Revenue Management [link](https://arxiv.org/pdf/2602.02283)
- [arXiv260203] Advancing General-Purpose Reasoning Models with Modular Gradient Surgery [link](https://arxiv.org/pdf/2602.02301)
- [arXiv260203] Position: Explaining Behavioral Shifts in Large Language Models Requires a Comparative Approach [link](https://arxiv.org/pdf/2602.02304)
- [arXiv260203] SWE-Universe: Scale Real-World Verifiable Environments to Millions [link](https://arxiv.org/pdf/2602.02361)
- [arXiv260203] SLIME: Stabilized Likelihood Implicit Margin Enforcement for Preference Optimization [link](https://arxiv.org/pdf/2602.02383)
- [arXiv260203] David vs. Goliath: Verifiable Agent-to-Agent Jailbreaking via Reinforcement Learning [link](https://arxiv.org/pdf/2602.02395)
- [arXiv260203] World-Gymnast: Training Robots with Reinforcement Learning in a World Model [link](https://arxiv.org/pdf/2602.02454)
- [arXiv260203] Conflict-Aware Client Selection for Multi-Server Federated Learning [link](https://arxiv.org/pdf/2602.02458)
- [arXiv260203] RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System [link](https://arxiv.org/pdf/2602.02488)
- [arXiv260203] Design and Empirical Study of a Large Language Model-Based Multi-Agent Investment System for Chinese Public REITs [link](https://arxiv.org/pdf/2602.00082)
- [arXiv260203] Reinforcement Learning for Control Systems with Time Delays: A Comprehensive Survey [link](https://arxiv.org/pdf/2602.00399)
- [arXiv260203] Stabilizing Fixed-Point Iteration for Markov Chain Poisson Equations [link](https://arxiv.org/pdf/2602.00474)
- [arXiv260203] FinEvo: From Isolated Backtests to Ecological Market Games for Multi-Agent Financial Strategy Evolution [link](https://arxiv.org/pdf/2602.00948)
- [arXiv260203] Non-Uniform Noise-to-Signal Ratio in the REINFORCE Policy-Gradient Estimator [link](https://arxiv.org/pdf/2602.01460)
- [arXiv260203] Well-Posed KL-Regularized Control via Wasserstein and Kalman-Wasserstein KL Divergences [link](https://arxiv.org/pdf/2602.02250)

**cs.AI/cs.LG contains "accelerate" total: 39**
- [arXiv260203] Fast Forward: Accelerating LLM Prefill with Predictive FFN Sparsity [link](https://arxiv.org/pdf/2602.00397)
- [arXiv260203] 3DGS$^2$-TR: Scalable Second-Order Trust-Region Method for 3D Gaussian Splatting [link](https://arxiv.org/pdf/2602.00395)
- [arXiv260203] Representation Learning Enhanced Deep Reinforcement Learning for Optimal Operation of Hydrogen-based Multi-Energy Systems [link](https://arxiv.org/pdf/2602.00027)
- [arXiv260203] Harvest: Opportunistic Peer-to-Peer GPU Caching for LLM Inference [link](https://arxiv.org/pdf/2602.00328)
- [arXiv260203] Generation Order and Parallel Decoding in Masked Diffusion Models: An Information-Theoretic Perspective [link](https://arxiv.org/pdf/2602.00286)
- [arXiv260203] Autonomous Data Processing using Meta-Agents [link](https://arxiv.org/pdf/2602.00307)
- [arXiv260203] Agentic Framework for Epidemiological Modeling [link](https://arxiv.org/pdf/2602.00299)
- [arXiv260203] Scalable Generative Game Engine: Breaking the Resolution Wall via Hardware-Algorithm Co-Design [link](https://arxiv.org/pdf/2602.00608)
- [arXiv260203] Environment-Aware Adaptive Pruning with Interleaved Inference Orchestration for Vision-Language-Action Models [link](https://arxiv.org/pdf/2602.00780)
- [arXiv260203] Multi-Objective Multi-Fidelity Bayesian Optimization with Causal Priors [link](https://arxiv.org/pdf/2602.00788)
- [arXiv260203] Exploration of Unary Arithmetic-Based Matrix Multiply Units for Low Precision DL Accelerators [link](https://arxiv.org/pdf/2602.00838)
- [arXiv260203] LASS-ODE: Scaling ODE Computations to Connect Foundation Models with Dynamical Physical Systems [link](https://arxiv.org/pdf/2602.01009)
- [arXiv260203] Superposition unifies power-law training dynamics [link](https://arxiv.org/pdf/2602.01045)
- [arXiv260203] Not All Preferences Are Created Equal: Stability-Aware and Gradient-Efficient Alignment for Reasoning Models [link](https://arxiv.org/pdf/2602.01207)
- [arXiv260203] Sample Efficient Active Algorithms for Offline Reinforcement Learning [link](https://arxiv.org/pdf/2602.01260)
- [arXiv260203] PACER: Blockwise Pre-verification for Speculative Decoding with Adaptive Length [link](https://arxiv.org/pdf/2602.01274)
- [arXiv260203] LLM-Driven Ontology Construction for Enterprise Knowledge Graphs [link](https://arxiv.org/pdf/2602.01276)
- [arXiv260203] Gradient-Aligned Calibration for Post-Training Quantization of Diffusion Models [link](https://arxiv.org/pdf/2602.01289)
- [arXiv260203] Improve the Trade-off Between Watermark Strength and Speculative Sampling Efficiency for Language Models [link](https://arxiv.org/pdf/2602.01428)
- [arXiv260203] Contribution-aware Token Compression for Efficient Video Understanding via Reinforcement Learning [link](https://arxiv.org/pdf/2602.01649)
- [arXiv260203] TABX: A High-Throughput Sandbox Battle Simulator for Multi-Agent Reinforcement Learning [link](https://arxiv.org/pdf/2602.01665)
- [arXiv260203] Real-Time Loop Closure Detection in Visual SLAM via NetVLAD and Faiss [link](https://arxiv.org/pdf/2602.01673)
- [arXiv260203] Physics Informed Generative AI Enabling Labour Free Segmentation For Microscopy Analysis [link](https://arxiv.org/pdf/2602.01710)
- [arXiv260203] PRISM: Parametrically Refactoring Inference for Speculative Sampling Draft Models [link](https://arxiv.org/pdf/2602.01762)
- [arXiv260203] Fast Autoregressive Video Diffusion and World Models with Temporal Cache Compression and Sparse Attention [link](https://arxiv.org/pdf/2602.01801)
- [arXiv260203] IntraSlice: Towards High-Performance Structural Pruning with Block-Intra PCA for LLMs [link](https://arxiv.org/pdf/2602.01975)
- [arXiv260203] Position: The Need for Ultrafast Training [link](https://arxiv.org/pdf/2602.02005)
- [arXiv260203] BAPS: A Fine-Grained Low-Precision Scheme for Softmax in Attention via Block-Aware Precision reScaling [link](https://arxiv.org/pdf/2602.02071)
- [arXiv260203] Toxicity Assessment in Preclinical Histopathology via Class-Aware Mahalanobis Distance for Known and Novel Anomalies [link](https://arxiv.org/pdf/2602.02124)
- [arXiv260203] Scalable Spatio-Temporal SE(3) Diffusion for Long-Horizon Protein Dynamics [link](https://arxiv.org/pdf/2602.02128)
- [arXiv260203] Generalized Optimal Classification Trees: A Mixed-Integer Programming Approach [link](https://arxiv.org/pdf/2602.02173)
- [arXiv260203] Hierarchical Adaptive Eviction for KV Cache Management in Multimodal Language Models [link](https://arxiv.org/pdf/2602.02197)
- [arXiv260203] Spark: Modular Spiking Neural Networks [link](https://arxiv.org/pdf/2602.02306)
- [arXiv260203] Towards Agentic Intelligence for Materials Science [link](https://arxiv.org/pdf/2602.00169)
- [arXiv260203] Action-Free Offline-to-Online RL via Discretised State Policies [link](https://arxiv.org/pdf/2602.00629)
- [arXiv260203] A New Workflow for Materials Discovery Bridging the Gap Between Experimental Databases and Graph Neural Networks [link](https://arxiv.org/pdf/2602.00756)
- [arXiv260203] WAKESET: A Large-Scale, High-Reynolds Number Flow Dataset for Machine Learning of Turbulent Wake Dynamics [link](https://arxiv.org/pdf/2602.01379)
- [arXiv260203] FluxNet: Learning Capacity-Constrained Local Transport Operators for Conservative and Bounded PDE Surrogates [link](https://arxiv.org/pdf/2602.01941)
- [arXiv260203] Training-free score-based diffusion for parameter-dependent stochastic dynamical systems [link](https://arxiv.org/pdf/2602.02113)

## 2026-02-04

**cs.DC total: 15**

- **[arXiv260204] Studying the Effect of Schedule Preemption on Dynamic Task Graph Scheduling**
  - **tags:** [sys], [task scheduling], [Last-K Preemption, dynamic scheduling, task graphs, makespan, fairness, utilization]
  - **authors:** Mohammadali Khodabandehlou, Jared Coleman, Niranjan Suri, Bhaskar Krishnamachari
  - **institution:** University of Southern California, Loyola Marymount University, US Army Research Laboratory
  - **link:** https://arxiv.org/pdf/2602.03081
  - **Simple LLM Summary:** This paper proposes the Last-K Preemption model for dynamic task graph scheduling, which selectively reschedules only the most recent K task graphs to balance adaptability and stability. The results show that moderate preemption can achieve most of the makespan and utilization benefits of full preemption while maintaining fairness and low overhead.

- **[arXiv260204] Joint Network-and-Server Congestion in Multi-Source Traffic Allocation: A Convex Formulation and Price-Based Decentralization**
  - **tags:** [sys], [network optimization], [convex optimization, marginal cost pricing, Wardrop condition, distributed algorithm, queueing delay]
  - **authors:** Tamoghna Sarkar, Bhaskar Krishnamachari
  - **institution:** University of Southern California
  - **link:** https://arxiv.org/pdf/2602.03246
  - **Simple LLM Summary:** This paper formulates a multi-source traffic allocation problem with joint network and server congestion as a convex program and develops a decentralized pricing-based algorithm. The solution is characterized by KKT conditions that equalize total marginal costs across routes, akin to a Wardrop equilibrium. Numerical results show the distributed algorithm converges to the centralized optimum, effectively balancing access and service delays.

- **[arXiv260204] StreamShield: A Production-Proven Resiliency Solution for Apache Flink at ByteDance**
  - **tags:** [mlsys], [fault-tolerance], [runtime optimization, fine-grained fault-tolerance, hybrid replication strategy, high availability, robust testing and deployment pipeline]
  - **authors:** Yong Fang, Yuxing Han, Meng Wang, Yifan Zhang, Yue Ma, Chi Zhang
  - **institution:** ByteDance Inc.
  - **link:** https://arxiv.org/pdf/2602.03189
  - **Simple LLM Summary:** The paper presents StreamShield, a resiliency solution for Apache Flink clusters at ByteDance, which introduces techniques like runtime optimization, fine-grained fault-tolerance, and a hybrid replication strategy. Extensive evaluations on a production cluster demonstrate that StreamShield effectively enhances system resiliency and operational stability to meet strict SLOs.

- **[arXiv260204] Exploiting Multi-Core Parallelism in Blockchain Validation and Construction**
  - **tags:** [sys], [blockchain], [multi-core parallelism, Mixed-Integer Linear Programming (MILP), conflict-aware scheduling, deterministic heuristics, block construction, block execution]
  - **authors:** Arivarasan Karmegam, Lucianna Kiffer, Antonio Fernández Anta
  - **institution:** IMDEA Networks Institute, Universidad Carlos III de Madrid, IMDEA Software Institute
  - **link:** https://arxiv.org/pdf/2602.03444
  - **Simple LLM Summary:** This paper proposes methods to exploit multi-core parallelism for blockchain validation and block construction using MILP formulations and fast deterministic heuristics. The heuristics achieve near-optimal performance, significantly reducing block processing time while maintaining deterministic execution. The results show substantial speedups, demonstrating that lightweight scheduling can unlock parallelism in blockchain systems.

- **[arXiv260204] Controlled disagreement improves generalization in decentralized training**
  - **tags:** [mlsys], [others], [decentralized SGD, adaptive consensus, Hessian subspace, structured perturbations, flatter minima, implicit regularization]
  - **authors:** Zesen Wang, Mikael Johansson
  - **institution:** KTH Royal Institute of Technology
  - **link:** https://arxiv.org/pdf/2602.02899
  - **Simple LLM Summary:** This paper introduces Decentralized SGD with Adaptive Consensus (DSGD-AC), a method that intentionally preserves controlled disagreement between workers to act as a structured regularizer. The authors prove these consensus errors align with the dominant Hessian subspace, guiding optimization toward flatter minima. The method consistently outperforms both standard decentralized and centralized SGD in generalization across benchmarks, re-framing consensus errors as a beneficial tool rather than a drawback.

- **[arXiv260204] Error Analysis of Matrix Multiplication Emulation Using Ozaki-II Scheme**
  - **tags:** [mlsys], [GPU kernels], [Ozaki-II scheme, Chinese Remainder Theorem, error analysis, matrix multiplication, emulation, low-precision arithmetic, INT8, INT32]
  - **authors:** Yuki Uchino, Katsuhisa Ozaki, Toshiyuki Imamura
  - **institution:** RIKEN Center for Computational Science, Shibaura Institute of Technology
  - **link:** https://arxiv.org/pdf/2602.02549
  - **Simple LLM Summary:** This paper presents a rigorous deterministic error analysis of the Ozaki-II scheme, a method that emulates high-precision matrix multiplication using a sequence of low-precision operations via the Chinese Remainder Theorem. The analysis clarifies the scheme's accuracy behavior and enables estimation of the number of low-precision multiplications needed to achieve a desired numerical accuracy.

- **[arXiv260204] Dynamic Topology Optimization for Non-IID Data in Decentralized Learning**
  - **tags:** [mlsys], [others], [decentralized learning, topology optimization, gossip protocol, non-IID data, peer-to-peer communication, dynamic graph]
  - **authors:** Bart Cox, Antreas Ioannou, Jérémie Decouchant
  - **institution:** Delft University of Technology
  - **link:** https://arxiv.org/pdf/2602.03383
  - **Simple LLM Summary:** The paper proposes Morph, a dynamic topology optimization algorithm for decentralized learning where nodes adaptively select peers for model exchange based on maximum model dissimilarity to handle non-IID data. It uses gossip-based peer discovery and diversity-driven neighbor selection to reshape the communication graph. Experiments show Morph achieves higher accuracy, faster convergence, and lower variance than static baselines, closely matching the performance of a fully connected network.

- **[arXiv260204] Prefix Consensus For Censorship Resistant BFT**
  - **tags:** [sys], [distributed consensus], [prefix consensus, strong prefix consensus, leaderless BFT, censorship resistance, state machine replication, graded consensus, binary consensus]
  - **authors:** Zhuolun Xiang, Andrei Tonkikh, Alexander Spiegelman
  - **institution:** Aptos Labs
  - **link:** https://arxiv.org/pdf/2602.02892
  - **Simple LLM Summary:** This paper introduces a new consensus primitive called Prefix Consensus and its stronger variant to build a leaderless, censorship-resistant BFT protocol. The core method uses a multi-proposer approach with deterministic ranking and demotion rules to guarantee inclusion of honest proposals within four rounds. The main conclusion is that this protocol ensures progress and limits censorship to at most f slots after GST, even under an adversary that can suspend one party per round.

- **[arXiv260204] Kino-PAX$^+$: Near-Optimal Massively Parallel Kinodynamic Sampling-based Motion Planner**
  - **tags:** [ai], [motion planning], [sampling-based motion planning, kinodynamic constraints, parallelization, GPU, asymptotic near-optimality, δ-robust completeness]
  - **authors:** Nicolas Perrault, Qi Heng Ho, Morteza Lahijanian
  - **institution:** University of Colorado Boulder, Virginia Tech
  - **link:** https://arxiv.org/pdf/2602.02846
  - **Simple LLM Summary:** The paper introduces Kino-PAX+, a massively parallel kinodynamic sampling-based motion planner that decomposes serial operations into three parallel subroutines to build a sparse tree of trajectories. It focuses computation on promising nodes for rapid cost improvement, providing asymptotic near-optimal guarantees. The results show it finds solutions up to three orders of magnitude faster than serial methods and achieves lower costs than a state-of-the-art GPU-based planner.

- **[arXiv260204] Large-Scale LLM Inference with Heterogeneous Workloads: Prefill-Decode Contention and Asymptotically Optimal Control**
  - **tags:** [mlsys], [llm inference], [stochastic control, queueing network, fluid approximation, linear programming, gate-and-route policy, prefill-decode contention]
  - **authors:** Ruihan Lin, Zezhen Ding, Zean Han, Jiheng Zhang
  - **institution:** The Hong Kong University of Science and Technology
  - **link:** https://arxiv.org/pdf/2602.02987
  - **Simple LLM Summary:** This paper develops a stochastic control and queueing network framework to schedule heterogeneous LLM inference workloads on GPU clusters, addressing the contention between prefill and decode phases. It designs and proves the asymptotic optimality of gate-and-route admission and routing policies. Numerical experiments show these policies outperform standard serving heuristics.

- **[arXiv260204] Recursive Energy Efficient Agreement**
  - **tags:** [sys], [distributed computing], [agreement, energy efficiency, crash faults, recursive algorithm, active rounds]
  - **authors:** Shachar Meir, David Peleg
  - **institution:** Weizmann Institute of Science
  - **link:** https://arxiv.org/pdf/2602.03474
  - **Simple LLM Summary:** The paper proposes a recursive algorithm for solving the Agreement problem in distributed systems with crash faults. Its core method focuses on minimizing the number of rounds each participant must be active. The main conclusion is that the algorithm achieves O(log f) active rounds per participant, thereby improving energy efficiency.

- **[arXiv260204] Do We Need Asynchronous SGD? On the Near-Optimality of Synchronous Methods**
  - **tags:** [mlsys], [cluster infrastructure], [synchronous sgd, asynchronous sgd, m-synchronous sgd, distributed optimization, heterogeneous computation]
  - **authors:** Grigory Begunov, Alexander Tyurin
  - **institution:** AXXX, Lomonosov Moscow State University
  - **link:** https://arxiv.org/pdf/2602.03802
  - **Simple LLM Summary:** This paper revisits Synchronous SGD and its robust variant m-Synchronous SGD, analyzing them under random computation times and adversarial partial worker participation. It theoretically shows that these synchronous methods achieve near-optimal time complexity in many heterogeneous computing scenarios, suggesting they are often sufficient despite the prevalence of asynchronous optimization approaches.

- **[arXiv260204] Mitigating Staleness in Asynchronous Pipeline Parallelism via Basis Rotation**
  - **tags:** [mlsys], [llm training], [asynchronous pipeline parallelism, gradient staleness, basis rotation, Adam, Hessian eigenbasis, coordinate-wise adaptivity]
  - **authors:** Hyunji Jung, Sungbin Shin, Namhoon Lee
  - **institution:** POSTECH
  - **link:** https://arxiv.org/pdf/2602.03515
  - **Simple LLM Summary:** This paper proposes using basis rotation to mitigate the gradient staleness problem in asynchronous pipeline parallelism for large language model training. The method addresses the misalignment between the Hessian eigenbasis and the coordinate basis, which hampers optimizers like Adam. The authors demonstrate that this approach significantly accelerates convergence, achieving the same training loss in 76.8% fewer iterations for a 1B-parameter model compared to baseline methods.

- **[arXiv260204] Improved Analysis of the Accelerated Noisy Power Method with Applications to Decentralized PCA**
  - **tags:** [ai], [optimization algorithms], [accelerated noisy power method, decentralized PCA, inexact matrix-vector products, perturbation analysis, convergence rate]
  - **authors:** Pierre Aguié, Mathieu Even, Laurent Massoulié
  - **institution:** Inria, DI-ENS, PSL Research University, Université de Montpellier, INSERM
  - **link:** https://arxiv.org/pdf/2602.03682
  - **Simple LLM Summary:** The paper presents an improved analysis of the Accelerated Noisy Power Method for PCA, showing it achieves faster convergence under milder noise conditions than prior work. The analysis is proven optimal and is applied to develop the first provably accelerated decentralized PCA algorithm with low communication costs.

- **[arXiv260204] DALI: A Workload-Aware Offloading Framework for Efficient MoE Inference on Local PCs**
  - **tags:** [mlsys], [llm inference], [mixture of experts, offloading, workload-aware, greedy assignment, residual-based prefetching, cache replacement]
  - **authors:** Zeyu Zhu, Gang Li, Peisong Wang, Zitao Mo, Minnan Pei, Zhuoran Song, Xiaoyao Liang, Jian Cheng
  - **institution:** Institute of Automation, Chinese Academy of Sciences; Shanghai Jiao Tong University
  - **link:** https://arxiv.org/pdf/2602.03495
  - **Simple LLM Summary:** The paper proposes DALI, a workload-aware offloading framework for efficient Mixture of Experts (MoE) inference on local PCs. It dynamically assigns experts to CPU/GPU, uses residual-based prefetching, and employs a workload-aware cache policy to address load imbalance and improve resource utilization. The evaluation shows DALI achieves significant speedups over state-of-the-art offloading frameworks.


**cs.AI/cs.LG contains "reinforcement learning" total: 60**
- [arXiv260204] Embodiment-Aware Generalist Specialist Distillation for Unified Humanoid Whole-Body Control [link](https://arxiv.org/pdf/2602.02960)
- [arXiv260204] Accordion-Thinking: Self-Regulated Step Summaries for Efficient and Readable LLM Reasoning [link](https://arxiv.org/pdf/2602.03249)
- [arXiv260204] CoBA-RL: Capability-Oriented Budget Allocation for Reinforcement Learning in LLMs [link](https://arxiv.org/pdf/2602.03048)
- [arXiv260204] Learning to Explore with Parameter-Space Noise: A Deep Dive into Parameter-Space Noise for Reinforcement Learning with Verifiable Rewards [link](https://arxiv.org/pdf/2602.02555)
- [arXiv260204] IMAGINE: Intelligent Multi-Agent Godot-based Indoor Networked Exploration [link](https://arxiv.org/pdf/2602.02858)
- [arXiv260204] Human-Centric Traffic Signal Control for Equity: A Multi-Agent Action Branching Deep Reinforcement Learning Approach [link](https://arxiv.org/pdf/2602.02959)
- [arXiv260204] Learning Fast Monomial Orders for Gröbner Basis Computations [link](https://arxiv.org/pdf/2602.02972)
- [arXiv260204] MentalSeek-Dx: Towards Progressive Hypothetico-Deductive Reasoning for Real-world Psychiatric Diagnosis [link](https://arxiv.org/pdf/2602.03340)
- [arXiv260204] Maximum Likelihood Reinforcement Learning [link](https://arxiv.org/pdf/2602.02710)
- [arXiv260204] BinaryPPO: Efficient Policy Optimization for Binary Classification [link](https://arxiv.org/pdf/2602.02708)
- [arXiv260204] Beyond Alignment: Expanding Reasoning Capacity via Manifold-Reshaping Policy Optimization [link](https://arxiv.org/pdf/2602.02545)
- [arXiv260204] How Does the Lagrangian Guide Safe Reinforcement Learning through Diffusion Models? [link](https://arxiv.org/pdf/2602.02924)
- [arXiv260204] Quantized Evolution Strategies: High-precision Fine-tuning of Quantized LLMs at Low-precision Cost [link](https://arxiv.org/pdf/2602.03120)
- [arXiv260204] Structuring Value Representations via Geometric Coherence in Markov Decision Processes [link](https://arxiv.org/pdf/2602.02978)
- [arXiv260204] Hierarchical Entity-centric Reinforcement Learning with Factored Subgoal Diffusion [link](https://arxiv.org/pdf/2602.02722)
- [arXiv260204] ForesightKV: Optimizing KV Cache Eviction for Reasoning Models by Learning Long-Term Contribution [link](https://arxiv.org/pdf/2602.03203)
- [arXiv260204] MedSAM-Agent: Empowering Interactive Medical Image Segmentation with Multi-turn Agentic Reinforcement Learning [link](https://arxiv.org/pdf/2602.03320)
- [arXiv260204] Co2PO: Coordinated Constrained Policy Optimization for Multi-Agent RL [link](https://arxiv.org/pdf/2602.02970)
- [arXiv260204] BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation [link](https://arxiv.org/pdf/2602.02554)
- [arXiv260204] From Scalar Rewards to Potential Trends: Shaping Potential Landscapes for Model-Based Reinforcement Learning [link](https://arxiv.org/pdf/2602.03201)
- [arXiv260204] TMS: Trajectory-Mixed Supervision for Reward-Free, On-Policy SFT [link](https://arxiv.org/pdf/2602.03073)
- [arXiv260204] Entropy-Gated Selective Policy Optimization:Token-Level Gradient Allocation for Hybrid Training of Large Language Models [link](https://arxiv.org/pdf/2602.03309)
- [arXiv260204] CADENT: Gated Hybrid Distillation for Sample-Efficient Transfer in Reinforcement Learning [link](https://arxiv.org/pdf/2602.02532)
- [arXiv260204] Manifold-Constrained Energy-Based Transition Models for Offline Reinforcement Learning [link](https://arxiv.org/pdf/2602.02900)
- [arXiv260204] Hypersonic Flow Control: Generalized Deep Reinforcement Learning for Hypersonic Intake Unstart Control under Uncertainty [link](https://arxiv.org/pdf/2602.02531)
- [arXiv260204] Reinforcement Learning with Promising Tokens for Large Language Models [link](https://arxiv.org/pdf/2602.03195)
- [arXiv260204] Spatiotemporal Decision Transformer for Traffic Coordination [link](https://arxiv.org/pdf/2602.02903)
- [arXiv260204] medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions [link](https://arxiv.org/pdf/2602.03305)
- [arXiv260204] Intelligent Front-End Personalization: AI-Driven UI Adaptation [link](https://arxiv.org/pdf/2602.03154)
- [arXiv260204] Neural Predictor-Corrector: Solving Homotopy Problems with Reinforcement Learning [link](https://arxiv.org/pdf/2602.03086)
- [arXiv260204] Self-Hinting Language Models Enhance Reinforcement Learning [link](https://arxiv.org/pdf/2602.03143)
- [arXiv260204] Prompt Augmentation Scales up GRPO Training on Mathematical Reasoning [link](https://arxiv.org/pdf/2602.03190)
- [arXiv260204] Chain-of-Goals Hierarchical Policy for Long-Horizon Offline Goal-Conditioned RL [link](https://arxiv.org/pdf/2602.03389)
- [arXiv260204] Periodic Regularized Q-Learning [link](https://arxiv.org/pdf/2602.03301)
- [arXiv260204] Training and Simulation of Quadrupedal Robot in Adaptive Stair Climbing for Indoor Firefighting: An End-to-End Reinforcement Learning Approach [link](https://arxiv.org/pdf/2602.03087)
- [arXiv260204] Notes on the Reward Representation of Posterior Updates [link](https://arxiv.org/pdf/2602.02912)
- [arXiv260204] An Approximate Ascent Approach To Prove Convergence of PPO [link](https://arxiv.org/pdf/2602.03386)
- [arXiv260204] From Tokens to Numbers: Continuous Number Modeling for SVG Generation [link](https://arxiv.org/pdf/2602.02820)
- [arXiv260204] ContextEvolve: Multi-Agent Context Compression for Systems Code Optimization [link](https://arxiv.org/pdf/2602.02597)
- [arXiv260204] Formulating Reinforcement Learning for Human-Robot Collaboration through Off-Policy Evaluation [link](https://arxiv.org/pdf/2602.02530)
- [arXiv260204] QuantLRM: Quantization of Large Reasoning Models via Fine-Tuning Signals [link](https://arxiv.org/pdf/2602.02581)
- [arXiv260204] StepScorer: Accelerating Reinforcement Learning with Step-wise Scoring and Psychological Regret Modeling [link](https://arxiv.org/pdf/2602.03171)
- [arXiv260204] Causal Flow Q-Learning for Robust Offline Reinforcement Learning [link](https://arxiv.org/pdf/2602.02847)
- [arXiv260204] GraphDancer: Training LLMs to Explore and Reason over Graphs via Curriculum Reinforcement Learning [link](https://arxiv.org/pdf/2602.02518)
- [arXiv260204] CRL-VLA: Continual Vision-Language-Action Learning [link](https://arxiv.org/pdf/2602.03445)
- [arXiv260204] Beyond Variance: Prompt-Efficient RLVR via Rare-Event Amplification and Bidirectional Pairing [link](https://arxiv.org/pdf/2602.03452)
- [arXiv260204] IntentRL: Training Proactive User-intent Agents for Open-ended Deep Research via Reinforcement Learning [link](https://arxiv.org/pdf/2602.03468)
- [arXiv260204] Reparameterization Flow Policy Optimization [link](https://arxiv.org/pdf/2602.03501)
- [arXiv260204] CMR: Contractive Mapping Embeddings for Robust Humanoid Locomotion on Unstructured Terrains [link](https://arxiv.org/pdf/2602.03511)
- [arXiv260204] Not All Negative Samples Are Equal: LLMs Learn Better from Plausible Reasoning [link](https://arxiv.org/pdf/2602.03516)
- [arXiv260204] TRE: Encouraging Exploration in the Trust Region [link](https://arxiv.org/pdf/2602.03635)
- [arXiv260204] Reinforcement Fine-Tuning for History-Aware Dense Retriever in RAG [link](https://arxiv.org/pdf/2602.03645)
- [arXiv260204] Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration [link](https://arxiv.org/pdf/2602.03647)
- [arXiv260204] Rethinking the Reranker: Boundary-Aware Evidence Selection for Robust Retrieval-Augmented Generation [link](https://arxiv.org/pdf/2602.03689)
- [arXiv260204] Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL [link](https://arxiv.org/pdf/2602.03773)
- [arXiv260204] Efficient Estimation of Kernel Surrogate Models for Task Attribution [link](https://arxiv.org/pdf/2602.03783)
- [arXiv260204] Bridging Online and Offline RL: Contextual Bandit Learning for Multi-Turn Code Generation [link](https://arxiv.org/pdf/2602.03806)
- [arXiv260204] SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving [link](https://arxiv.org/pdf/2602.03816)
- [arXiv260204] Understanding and Exploiting Weight Update Sparsity for Communication-Efficient Distributed RL [link](https://arxiv.org/pdf/2602.03839)
- [arXiv260204] Relaxed Triangle Inequality for Kullback-Leibler Divergence Between Multivariate Gaussian Distributions [link](https://arxiv.org/pdf/2602.02577)

**cs.AI/cs.LG contains "accelerate" total: 25**
- [arXiv260204] Consistency Deep Equilibrium Models [link](https://arxiv.org/pdf/2602.03024)
- [arXiv260204] Rethinking Benign Relearning: Syntax as the Hidden Driver of Unlearning Failures [link](https://arxiv.org/pdf/2602.03379)
- [arXiv260204] Learning-Infused Formal Reasoning: From Contract Synthesis to Artifact Reuse and Formal Semantics [link](https://arxiv.org/pdf/2602.02881)
- [arXiv260204] Digital Lifelong Learning in the Age of AI: Trends and Insights [link](https://arxiv.org/pdf/2602.03114)
- [arXiv260204] Nüwa: Mending the Spatial Integrity Torn by VLM Token Pruning [link](https://arxiv.org/pdf/2602.02951)
- [arXiv260204] Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection [link](https://arxiv.org/pdf/2602.03216)
- [arXiv260204] From Inexact Gradients to Byzantine Robustness: Acceleration and Optimization under Similarity [link](https://arxiv.org/pdf/2602.03329)
- [arXiv260204] Every Bit Counts: A Theoretical Study of Precision-Expressivity Tradeoffs in Quantized Transformers [link](https://arxiv.org/pdf/2602.02707)
- [arXiv260204] Understanding Multi-Agent LLM Frameworks: A Unified Benchmark and Experimental Analysis [link](https://arxiv.org/pdf/2602.03128)
- [arXiv260204] Aligning Language Model Benchmarks with Pairwise Preferences [link](https://arxiv.org/pdf/2602.02898)
- [arXiv260204] Probe-then-Commit Multi-Objective Bandits: Theoretical Benefits of Limited Multi-Arm Feedback [link](https://arxiv.org/pdf/2602.03175)
- [arXiv260204] A Semi-Supervised Pipeline for Generalized Behavior Discovery from Animal-Borne Motion Time Series [link](https://arxiv.org/pdf/2602.02618)
- [arXiv260204] FIRE-Bench: Evaluating Agents on the Rediscovery of Scientific Insights [link](https://arxiv.org/pdf/2602.02905)
- [arXiv260204] Koopman Autoencoders with Continuous-Time Latent Dynamics for Fluid Dynamics Forecasting [link](https://arxiv.org/pdf/2602.02832)
- [arXiv260204] GraDE: A Graph Diffusion Estimator for Frequent Subgraph Discovery in Neural Architectures [link](https://arxiv.org/pdf/2602.03257)
- [arXiv260204] NLI:Non-uniform Linear Interpolation Approximation of Nonlinear Operations for Efficient LLMs Inference [link](https://arxiv.org/pdf/2602.02988)
- [arXiv260204] StepScorer: Accelerating Reinforcement Learning with Step-wise Scoring and Psychological Regret Modeling [link](https://arxiv.org/pdf/2602.03171)
- [arXiv260204] Privasis: Synthesizing the Largest "Public" Private Dataset from Scratch [link](https://arxiv.org/pdf/2602.03183)
- [arXiv260204] Fast-MWEM: Private Data Release in Sublinear Time [link](https://arxiv.org/pdf/2602.03732)
- [arXiv260204] Zero-shot large vision-language model prompting for automated bone identification in paleoradiology x-ray archives [link](https://arxiv.org/pdf/2602.03750)
- [arXiv260204] Fast-Slow Efficient Training for Multimodal Large Language Models via Visual Token Pruning [link](https://arxiv.org/pdf/2602.03815)
- [arXiv260204] Accelerating Scientific Research with Gemini: Case Studies and Common Techniques [link](https://arxiv.org/pdf/2602.03837)
- [arXiv260204] PrevizWhiz: Combining Rough 3D Scenes and 2D Video to Guide Generative Video Previsualization [link](https://arxiv.org/pdf/2602.03838)
- [arXiv260204] CryoLVM: Self-supervised Learning from Cryo-EM Density Maps with Large Vision Models [link](https://arxiv.org/pdf/2602.02620)
- [arXiv260204] Multiparameter Uncertainty Mapping in Quantitative Molecular MRI using a Physics-Structured Variational Autoencoder (PS-VAE) [link](https://arxiv.org/pdf/2602.03317)

## 2026-02-05

**cs.DC total: 9**

- **[arXiv260205] Scalable Explainability-as-a-Service (XaaS) for Edge AI Systems**
  - **tags:** [mlsys], [others], [distributed explanation cache, semantic similarity retrieval, lightweight verification protocol, adaptive explanation engine]
  - **authors:** Samaresh Kumar Singh, Joyjit Roy
  - **institution:** IEEE
  - **link:** https://arxiv.org/pdf/2602.04120
  - **Simple LLM Summary:** The paper proposes Explainability-as-a-Service (XaaS), a distributed architecture that decouples model inference from explanation generation to improve efficiency in edge AI systems. It introduces a caching mechanism, a verification protocol, and an adaptive engine to reduce latency and computational redundancy. Experimental results show that XaaS reduces latency by 38% while maintaining high explanation quality across real-world deployments.

- **[arXiv260205] SPEAR: An Engineering Case Study of Multi-Agent Coordination for Smart Contract Auditing**
  - **tags:** [ai], [multi-agent systems], [multi-agent coordination, Contract Net protocol, AGM belief revision, programmatic-first repair, risk-aware heuristics]
  - **authors:** Arnab Mallick, Indraveni Chebolu, Harmesh Rana
  - **institution:** Center for Development of Advanced Computing, Hyderabad, India
  - **link:** https://arxiv.org/pdf/2602.04418
  - **Simple LLM Summary:** The paper presents SPEAR, a multi-agent system framework for smart contract auditing that uses specialized agents (Planning, Execution, Repair) coordinating via established protocols like Contract Net. An empirical study shows this multi-agent design offers advantages in coordination, recovery, and resource use compared to centralized and pipeline-based alternatives.

- **[arXiv260205] Entanglement improves coordination in distributed systems**
  - **tags:** [sys], [quantum distributed systems], [quantum entanglement, non-local game, queueing-theoretic analysis, distributed scheduling, coordination]
  - **authors:** Francisco Ferreira da Silva, Stephanie Wehner
  - **institution:** Delft University of Technology, QuTech, Delft Networks
  - **link:** https://arxiv.org/pdf/2602.04588
  - **Simple LLM Summary:** This paper proposes using shared quantum entanglement to coordinate two servers in a distributed system, eliminating the latency of classical communication. Through analytical modeling and queueing analysis, it demonstrates that entanglement-assisted strategies achieve superior performance trade-offs in processing tasks compared to optimal classical strategies without communication. The work identifies distributed scheduling as a novel application for near-term quantum networks.

- **[arXiv260205] A TEE-based Approach for Preserving Data Secrecy in Process Mining with Decentralized Sources**
  - **tags:** [sys], [confidential computing], [Trusted Execution Environments (TEEs), segmentation-based strategy, formal verification, security analysis]
  - **authors:** Davide Basile, Valerio Goretti, Luca Barbaro, Hajo A. Reijers, Claudio Di Ciccio
  - **institution:** Sapienza University of Rome, Utrecht University
  - **link:** https://arxiv.org/pdf/2602.04697
  - **Simple LLM Summary:** This paper proposes CONFINE, a method for secrecy-preserving inter-organizational process mining that uses Trusted Execution Environments (TEEs) and a four-stage protocol with a segmentation strategy to securely process decentralized event logs. The evaluation shows the approach can handle real-world workloads with logarithmic memory growth relative to log size and confirms its ability to preserve data confidentiality across organizational boundaries.

- **[arXiv260205] GPU Acceleration and Portability of the TRIMEG Code for Gyrokinetic Plasma Simulations using OpenMP**
  - **tags:** [sys], [high performance computing], [GPU offloading, OpenMP, particle-in-cell, finite element, MPI-OpenMP, scalability]
  - **authors:** Giorgio Daneri
  - **institution:** Politecnico di Milano
  - **link:** https://arxiv.org/pdf/2601.14301
  - **Simple LLM Summary:** This paper describes the GPU acceleration of the TRIMEG gyrokinetic plasma simulation code using OpenMP for offloading, focusing on particle pushing and grid-to-particle operations. The implementation was made portable to AMD and NVIDIA GPUs, overcoming compiler limitations and restructuring code. The main conclusion is that the GPU-accelerated version achieved significant speedup compared to the CPU version while correctly simulating the Ion Temperature Gradient mode.

- **[arXiv260205] Trust The Typical**
  - **tags:** [mlsys], [llm inference], [out-of-distribution detection, semantic space, guardrails, vLLM integration, GPU-optimization]
  - **authors:** Debargha Ganguly, Sreehari Sankar, Biyao Zhang, Vikash Singh, Kanan Gupta, Harshini Kavuru, Alan Luo, Weicong Chen, Warren Morningstar, Raghu Machiraju, Vipin Chaudhary
  - **institution:** Case Western Reserve University, University of Pittsburgh, The Ohio State University, Google Research
  - **link:** https://arxiv.org/pdf/2602.04581
  - **Simple LLM Summary:** The paper introduces Trust The Typical (T3), a framework that treats LLM safety as an out-of-distribution detection problem by learning the distribution of safe prompts in a semantic space and flagging deviations as threats. It achieves state-of-the-art performance across multiple safety benchmarks without training on harmful examples and integrates efficiently into production inference systems like vLLM with minimal overhead.

- **[arXiv260205] Pending Conflicts Make Progress Impossible**
  - **tags:** [sys], [distributed computing], [conflict-obstruction-freedom, linearizability, universal constructions, asynchronous read-write memory, commutativity]
  - **authors:** Petr Kuznetsov, Pierre Sutra, Guillermo Toyos-Marfurt
  - **institution:** Télécom Paris, Télécom SudParis, Institut Polytechnique de Paris
  - **link:** https://arxiv.org/pdf/2602.04013
  - **Simple LLM Summary:** This paper introduces conflict-obstruction-freedom, a progress condition for shared objects that allows operations to complete if they run without step contention from non-commuting operations. It proves that implementing conflict-obstruction-free universal constructions is impossible in asynchronous read-write shared memory. The conclusion reveals a fundamental limitation: the invocation of conflicting operations inherently imposes a synchronization cost, and progress requires resolving pending conflicts.

- **[arXiv260205] Horizon-LM: A RAM-Centric Architecture for LLM Training**
  - **tags:** [mlsys], [llm training], [CPU-master GPU-template, explicit recomputation, manual gradient propagation, pipelined double-buffered execution, host-memory parameter store]
  - **authors:** Zhengqing Yuan, Lichao Sun, Yanfang
  - **institution:** University of Notre Dame, Lehigh University
  - **link:** https://arxiv.org/pdf/2602.04816
  - **Simple LLM Summary:** Horizon-LM is a RAM-centric training system that treats host memory as the primary parameter store and uses GPUs only as transient compute engines, eliminating persistent GPU-resident modules. This approach decouples model scale from GPU count and bounds memory usage to the theoretical parameter footprint. The system enables training of up to 120B parameter models on a single GPU and demonstrates significantly higher throughput compared to existing offloading methods.

- **[arXiv260205] Six Times to Spare: LDPC Acceleration on DGX Spark for AI-Native Open RAN**
  - **tags:** [mlsys], [GPU kernels], [LDPC decoding, GPU acceleration, Sionna, Blackwell GB10, DGX Spark, 5G NR, belief-propagation]
  - **authors:** Ryan Barker, Fatemeh Afghah
  - **institution:** Clemson University
  - **link:** https://arxiv.org/pdf/2602.04652
  - **Simple LLM Summary:** This paper evaluates offloading 5G LDPC decoding from a Grace CPU to an integrated Blackwell GB10 GPU on an NVIDIA DGX Spark platform using the high-level Sionna framework. It finds that GPU-based decoding achieves an average 6x throughput speedup and stays well within the 0.5 ms slot time, while CPU decoding exceeds it, demonstrating that GPU acceleration can meet real-time deadlines and free CPU resources for higher-layer tasks.


**cs.AI/cs.LG contains "reinforcement learning" total: 31**
- [arXiv260205] Beyond Rewards in Reinforcement Learning for Cyber Defence [link](https://arxiv.org/pdf/2602.04809)
- [arXiv260205] The Missing Half: Unveiling Training-time Implicit Safety Risks Beyond Deployment [link](https://arxiv.org/pdf/2602.04196)
- [arXiv260205] When Silence Is Golden: Can LLMs Learn to Abstain in Temporal QA and Beyond? [link](https://arxiv.org/pdf/2602.04755)
- [arXiv260205] Steering LLMs via Scalable Interactive Oversight [link](https://arxiv.org/pdf/2602.04210)
- [arXiv260205] CRoSS: A Continual Robotic Simulation Suite for Scalable Reinforcement Learning with High Task Diversity and Realistic Physics Simulation [link](https://arxiv.org/pdf/2602.04868)
- [arXiv260205] Decoupling Time and Risk: Risk-Sensitive Reinforcement Learning with General Discounting [link](https://arxiv.org/pdf/2602.04131)
- [arXiv260205] Lyapunov Constrained Soft Actor-Critic (LC-SAC) using Koopman Operator Theory for Quadrotor Trajectory Tracking [link](https://arxiv.org/pdf/2602.04132)
- [arXiv260205] Topology-Aware Revival for Efficient Sparse Training [link](https://arxiv.org/pdf/2602.04166)
- [arXiv260205] Skin Tokens: A Learned Compact Representation for Unified Autoregressive Rigging [link](https://arxiv.org/pdf/2602.04805)
- [arXiv260205] Evolving Afferent Architectures: Biologically-inspired Models for Damage-Avoidance Learning [link](https://arxiv.org/pdf/2602.04807)
- [arXiv260205] Piece of CAKE: Adaptive Execution Engines via Microsecond-Scale Learning [link](https://arxiv.org/pdf/2602.04181)
- [arXiv260205] WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning [link](https://arxiv.org/pdf/2602.04634)
- [arXiv260205] Monitorability as a Free Gift: How RLVR Spontaneously Aligns Reasoning [link](https://arxiv.org/pdf/2602.03978)
- [arXiv260205] Dual Mind World Model Inspired Network Digital Twin for Access Scheduling [link](https://arxiv.org/pdf/2602.04566)
- [arXiv260205] Learning the Value Systems of Agents with Preference-based and Inverse Reinforcement Learning [link](https://arxiv.org/pdf/2602.04518)
- [arXiv260205] Scaling In-Context Online Learning Capability of LLMs via Cross-Episode Meta-RL [link](https://arxiv.org/pdf/2602.04089)
- [arXiv260205] Thickening-to-Thinning: Reward Shaping via Human-Inspired Learning Dynamics for LLM Reasoning [link](https://arxiv.org/pdf/2602.04265)
- [arXiv260205] Rethinking the Trust Region in LLM Reinforcement Learning [link](https://arxiv.org/pdf/2602.04879)
- [arXiv260205] Agent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning [link](https://arxiv.org/pdf/2602.04284)
- [arXiv260205] From Ambiguity to Action: A POMDP Perspective on Partial Multi-Label Ambiguity and Its Horizon-One Resolution [link](https://arxiv.org/pdf/2602.04255)
- [arXiv260205] Stochastic Decision Horizons for Constrained Reinforcement Learning [link](https://arxiv.org/pdf/2602.04599)
- [arXiv260205] Rethinking the Design Space of Reinforcement Learning for Diffusion Models: On the Importance of Likelihood Estimation Beyond Loss Design [link](https://arxiv.org/pdf/2602.04663)
- [arXiv260205] Autonomous AI Agents for Real-Time Affordable Housing Site Selection: Multi-Objective Reinforcement Learning Under Regulatory Constraints [link](https://arxiv.org/pdf/2602.03940)
- [arXiv260205] GOPO: Policy Optimization using Ranked Rewards [link](https://arxiv.org/pdf/2602.03876)
- [arXiv260205] Mixture of Masters: Sparse Chess Language Models with Player Routing [link](https://arxiv.org/pdf/2602.04447)
- [arXiv260205] Reinforced Attention Learning [link](https://arxiv.org/pdf/2602.04884)
- [arXiv260205] HoRD: Robust Humanoid Control via History-Conditioned Reinforcement Learning and Online Distillation [link](https://arxiv.org/pdf/2602.04412)
- [arXiv260205] Learning to Reason in 13 Parameters [link](https://arxiv.org/pdf/2602.04118)
- [arXiv260205] QUATRO: Query-Adaptive Trust Region Policy Optimization for LLM Fine-tuning [link](https://arxiv.org/pdf/2602.04620)
- [arXiv260205] EMA Policy Gradient: Taming Reinforcement Learning for LLMs with EMA Anchor and Top-k KL [link](https://arxiv.org/pdf/2602.04417)
- [arXiv260205] Rationality Measurement and Theory for Reinforcement Learning Agents [link](https://arxiv.org/pdf/2602.04737)

**cs.AI/cs.LG contains "accelerate" total: 5**
- [arXiv260205] HybridQuestion: Human-AI Collaboration for Identifying High-Impact Research Questions [link](https://arxiv.org/pdf/2602.03849)
- [arXiv260205] SparVAR: Exploring Sparsity in Visual AutoRegressive Modeling for Training-Free Acceleration [link](https://arxiv.org/pdf/2602.04361)
- [arXiv260205] Multi-Head LatentMoE and Head Parallel: Communication-Efficient and Deterministic MoE Parallelism [link](https://arxiv.org/pdf/2602.04870)
- [arXiv260205] Digital Twins & ZeroConf AI: Structuring Automated Intelligent Pipelines for Industrial Applications [link](https://arxiv.org/pdf/2602.04385)
- [arXiv260205] Machine Learning-Driven Crystal System Prediction for Perovskites Using Augmented X-ray Diffraction Data [link](https://arxiv.org/pdf/2602.04435)
