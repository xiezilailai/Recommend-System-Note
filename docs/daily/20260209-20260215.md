# 20260209-20260215

## 2026-02-09

**cs.DC total: 21**

- **[arXiv260209] Computationally Efficient Laplacian CL-colME**
  - **tags:** [mlsys], [cluster infrastructure], [decentralized learning, collaborative mean estimation, laplacian consensus, graph-based consensus, doubly stochastic matrices]
  - **authors:** Nikola Stankovic
  - **institution:** IEEE
  - **link:** https://arxiv.org/pdf/2602.06070
  - **Simple LLM Summary:** This paper proposes CL-colME, a Laplacian-based consensus variant of decentralized collaborative mean estimation that avoids the computationally expensive normalization of doubly stochastic matrices. Simulation results show that CL-colME maintains the convergence and accuracy of the prior C-colME method while improving computational efficiency.

- **[arXiv260209] Experimental Analysis of Server-Side Caching for Web Performance**
  - **tags:** [sys], [web performance optimization], [server-side caching, in-memory cache, response time optimization, time-to-live (TTL), performance evaluation]
  - **authors:** Mohammad Umar, Bharat Tripathi
  - **institution:** Allenhouse Business School
  - **link:** https://arxiv.org/pdf/2602.06074
  - **Simple LLM Summary:** This paper experimentally compares the performance of a web server with and without simple in-memory caching using a fixed time-to-live. The results demonstrate that server-side caching significantly reduces response times, making it an effective and simple technique for improving performance in small-scale or educational web applications.

- **[arXiv260209] iScheduler: Reinforcement Learning-Driven Continual Optimization for Large-Scale Resource Investment Problems**
  - **tags:** [mlsys], [cluster infrastructure], [reinforcement learning, iterative scheduling, Markov decision process, resource investment problem, L-RIPLIB benchmark]
  - **authors:** Yi-Xiang Hu, Yuke Wang, Feng Wu, Zirui Huang, Shuli Zeng, Xiang-Yang Li
  - **institution:** University of Science and Technology of China
  - **link:** https://arxiv.org/pdf/2602.06064
  - **Simple LLM Summary:** The paper presents iScheduler, a framework that uses reinforcement learning to formulate and solve large-scale resource investment problems as a Markov decision process over decomposed subproblems. It achieves competitive resource costs while reducing the time to find a feasible schedule by up to 43 times compared to commercial solvers, and supports efficient schedule reconfiguration by reusing unaffected parts.

- **[arXiv260209] HQP: Sensitivity-Aware Hybrid Quantization and Pruning for Ultra-Low-Latency Edge AI Inference**
  - **tags:** [mlsys], [post-training], [hybrid quantization, structural pruning, Fisher Information Matrix, 8-bit post-training quantization, sensitivity-aware pruning, model compression]
  - **authors:** Dinesh Gopalan, Ratul Ali
  - **institution:** AMD, Jahangirnagar University
  - **link:** https://arxiv.org/pdf/2602.06069
  - **Simple LLM Summary:** This paper introduces the HQP framework, which combines sensitivity-aware structural pruning using a Fisher Information Matrix approximation and conditional 8-bit post-training quantization to optimize models for edge inference. The method achieves up to 3.12× speedup and 55% size reduction while limiting accuracy drop to under 1.5%, outperforming single-objective compression techniques.

- **[arXiv260209] FlashSketch: Sketch-Kernel Co-Design for Fast Sparse Sketching on GPUs**
  - **tags:** [mlsys], [GPU kernels], [sparse sketching, BlockPerm-SJLT, sketch-kernel co-design, oblivious subspace embedding, CUDA]
  - **authors:** Rajat Vadiraj Dwaraknath, Sungyoon Kim, Mert Pilanci
  - **institution:** Stanford University
  - **link:** https://arxiv.org/pdf/2602.06071
  - **Simple LLM Summary:** The paper introduces FlashSketch, a method that co-designs a new sparse sketch family called BlockPerm-SJLT with an optimized CUDA kernel to improve GPU efficiency. This approach introduces a tunable parameter to trade off between GPU performance and sketching robustness. The method achieves a geomean speedup of 1.7x over prior state-of-the-art GPU sketches, advancing the Pareto frontier of sketching quality versus speed.

- **[arXiv260209] PackInfer: Compute- and I/O-Efficient Attention for Batched LLM Inference**
  - **tags:** [mlsys], [llm inference], [kernel-level attention, load-balanced execution groups, I/O-aware grouping, KV cache reorganization, FlashAttention]
  - **authors:** Rui Ning, Wei Zhang, Fan Lai
  - **institution:** Nanjing University, University of Illinois Urbana-Champaign
  - **link:** https://arxiv.org/pdf/2602.06072
  - **Simple LLM Summary:** PackInfer is a kernel-level attention framework that improves batched LLM inference by packing heterogeneous requests into load-balanced execution groups and reorganizing KV caches with I/O-aware grouping. This approach reduces computation and I/O imbalance, leading to better GPU utilization. Evaluations show it reduces latency by 13.0-20.1% and improves throughput by 20% compared to FlashAttention.

- **[arXiv260209] Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers**
  - **tags:** [mlsys], [llm training], [matrix-based optimizers, data parallelism, tensor parallelism, asynchronous compute, load balancing, Shampoo, SOAP, Muon]
  - **authors:** Liangyu Wang, Siqi Zhang, Junjie Wang, Yiming Dong, Bo Zheng, Zihan Qiu, Shengkun Tang, Di Wang, Rui Men, Dayiheng Liu
  - **institution:** KAUST, Alibaba Group, Peking University, MBZUAI
  - **link:** https://arxiv.org/pdf/2602.06079
  - **Simple LLM Summary:** The paper proposes Canzona, a framework that decouples logical optimizer assignment from physical parameter distribution to efficiently run matrix-based optimizers in distributed LLM training. It introduces strategies like α-Balanced Static Partitioning for Data Parallelism and an Asynchronous Compute pipeline for Tensor Parallelism. Evaluations show it achieves significant speedups in iteration time and reduces optimizer step latency compared to baselines.

- **[arXiv260209] MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments**
  - **tags:** [mlsys], [others], [memory-centric benchmark, mobile GUI agents, LLM-as-judge evaluation, pass@k, cross-session learning, cross-temporal retention, cross-spatial retention]
  - **authors:** Guangyi Liu, Pengxiang Zhao, Yaozhen Liang, Qinyi Luo, Shunye Tang, Yuxiang Chai, Weifeng Lin, Han Xiao, WenHao Wang, Siheng Chen, Zhengxi Lu, Gao Wu, Hao Wang, Liang Liu, Yong Liu
  - **institution:** Zhejiang University, Nankai University, The Chinese University of Hong Kong, Shanghai Jiao Tong University, vivo AI Lab
  - **link:** https://arxiv.org/pdf/2602.06075
  - **Simple LLM Summary:** This paper introduces MemGUI-Bench, a comprehensive benchmark designed to evaluate the memory capabilities of mobile GUI agents, featuring an automated evaluation pipeline with staged LLM-as-judge and pass@k metrics. The study reveals significant memory deficits across all 11 evaluated state-of-the-art agents and identifies distinct failure modes, concluding with actionable design implications for improving agent memory.

- **[arXiv260209] Quantifying Energy-Efficient Edge Intelligence: Inference-time Scaling Laws for Heterogeneous Computing**
  - **tags:** [mlsys], [llm inference], [scaling laws, heterogeneous orchestration, hardware-aware routing, progressive sample multiplexing, cost models, Intelligence Per Watt, Energy-Coverage Efficiency, Price-Power-Performance]
  - **authors:** Satyam Kumar, Saurabh Jha
  - **institution:** Not explicitly provided in the given text. The author names are listed without affiliations or email domains.
  - **link:** https://arxiv.org/pdf/2602.06057
  - **Simple LLM Summary:** This paper introduces QEIL, a framework that uses inference-time scaling laws and heterogeneous hardware orchestration (across CPU, GPU, NPU) to optimize LLM inference on edge devices. It demonstrates that this approach achieves superlinear efficiency gains, significantly improves coverage and energy efficiency, and reduces latency without accuracy loss, establishing it as an optimal strategy for energy-constrained systems.

- **[arXiv260209] Mapping Gemma3 onto an Edge Dataflow Architecture**
  - **tags:** [mlsys], [llm inference], [dequantization engine, tiled matrix multiplication, FlowQKV, FusedDQP, FlowKV, Q4NX quantization, dataflow architecture]
  - **authors:** Shouyu Du, Miaoxiang Yu, Zhiheng Ni, Jillian Cai, Qing Yang, Tao Wei, Zhenyu Xu
  - **institution:** Clemson University, University of Rhode Island
  - **link:** https://arxiv.org/pdf/2602.06063
  - **Simple LLM Summary:** This paper presents the first end-to-end deployment of the Gemma3 family of models on an AMD Ryzen AI NPU using hardware-aware techniques like optimized dequantization, fused kernels, and novel attention mechanisms (FlowQKV, FlowKV). The proposed methods, combined with a 4-bit quantization format, achieve significant speedups and power efficiency improvements over iGPU and CPU baselines. The work demonstrates that modern NPUs can enable practical, low-power LLM and VLM inference at the edge and provides a blueprint for mapping transformers onto tiled dataflow accelerators.

- **[arXiv260209] LAAFD: LLM-based Agents for Accelerated FPGA Design**
  - **tags:** [mlsys], [llm inference], [high-level synthesis, agentic workflow, pipelining, vectorization, dataflow partitioning, co-simulation]
  - **authors:** Maxim Moraru, Kamalavasan Kamalakkannan, Jered Dominguez-Trujillo, Patrick Diehl, Atanu Barai, Julien Loiseau, Zachary Kent Baker, Howard Pritchard, Galen M Shipman
  - **institution:** Los Alamos National Laboratory
  - **link:** https://arxiv.org/pdf/2602.06085
  - **Simple LLM Summary:** This paper introduces LAAFD, an agentic workflow that uses large language models to automatically translate general-purpose C++ code into optimized FPGA kernels for Vitis HLS. The system automates hardware optimizations like pipelining and uses feedback from HLS reports and co-simulation to iteratively improve performance. The results show that LAAFD achieves near hand-tuned performance, significantly lowering the expertise barrier for FPGA acceleration.

- **[arXiv260209] AdFL: In-Browser Federated Learning for Online Advertisement**
  - **tags:** [mlsys], [others], [federated learning, differential privacy, in-browser training, ad viewability prediction]
  - **authors:** Ahmad Alemari, Pritam Sen, Cristian Borcea
  - **institution:** New Jersey Institute of Technology, Jazan University
  - **link:** https://arxiv.org/pdf/2602.06336
  - **Simple LLM Summary:** This paper presents AdFL, a federated learning framework that operates within web browsers to learn user ad preferences without sharing raw data, balancing targeted advertising with privacy. The system aggregates local updates into a global model and was tested with a proof-of-concept model for ad viewability prediction. Experiments show the framework is feasible, achieves high prediction accuracy (up to 92.59% AUC), and maintains adequate performance when differential privacy is applied to protect local model parameters.

- **[arXiv260209] BouquetFL: Emulating diverse participant hardware in Federated Learning**
  - **tags:** [mlsys], [cluster infrastructure], [federated learning, hardware emulation, resource restriction, GPU constraints, CPU throttling, memory constraints]
  - **authors:** Arno Geimer
  - **institution:** Unknown (Author email/affiliation not provided in excerpt)
  - **link:** https://arxiv.org/pdf/2602.06498
  - **Simple LLM Summary:** The paper presents BouquetFL, a framework that simulates heterogeneous client hardware (CPU, RAM, GPU) on a single machine through programmatic resource restriction for Federated Learning experiments. It concludes that this approach addresses a methodological gap by enabling controlled, reproducible study of system heterogeneity without requiring multiple physical devices, bringing FL research closer to practical deployment conditions.

- **[arXiv260209] Degradation of Feature Space in Continual Learning**
  - **tags:** [ai], [continual learning], [contrastive learning, isotropic regularization, feature space anisotropy, catastrophic forgetting, stability-plasticity dilemma]
  - **authors:** Chiara Lanza, Roberto Pereira, Marco Miozzo, Eduard Angelats, Paolo Dini
  - **institution:** CTTC (Centre Tecnològic de Telecomunicacions de Catalunya)
  - **link:** https://arxiv.org/pdf/2602.06586
  - **Simple LLM Summary:** This paper investigates whether enforcing isotropy in the feature space can improve representation quality in continual learning. Through experiments using contrastive continual learning techniques on CIFAR datasets, the study finds that isotropic regularization fails to improve and can even degrade model accuracy. The results suggest that isotropy, beneficial in centralized training, may not be a suitable inductive bias for non-stationary continual learning scenarios.

- **[arXiv260209] FCDP: Fully Cached Data Parallel for Communication-Avoiding Large-Scale Training**
  - **tags:** [mlsys], [llm training], [ZeRO-3, host memory caching, parameter-efficient fine-tuning (PEFT), all-gather, communication optimization]
  - **authors:** Gyeongseo Park, Eungyeong Lee, Song-woo Sok, Myung-Hoon Cha, Kwangwon Koh, Baik-Song An, Hongyeon Kim, Ki-Dong Kang
  - **institution:** Electronics and Telecommunications Research Institute (ETRI)
  - **link:** https://arxiv.org/pdf/2602.06499
  - **Simple LLM Summary:** The paper proposes FCDP, a method that caches forward-pass parameters in host memory and reuses them during the backward pass via fast intra-node all-gather to eliminate redundant inter-node communication. This approach preserves ZeRO-3's minimal GPU memory footprint while significantly boosting throughput on bandwidth-limited commodity clusters. The results show FCDP achieves up to 100x higher throughput than ZeRO-3 while maintaining the same maximum batch size.

- **[arXiv260209] Reinforcement Learning-Based Dynamic Management of Structured Parallel Farm Skeletons on Serverless Platforms**
  - **tags:** [mlsys], [cluster infrastructure], [reinforcement learning, autoscaling, serverless, OpenFaaS, farm skeleton, QoS, Gymnasium]
  - **authors:** Lanpei Li, Massimo Coppola, Malio Li, Valerio Besozzi, Jack Bell, Vincenzo Lomonaco
  - **institution:** National Research Council of Italy (CNR), University of Pisa, LUISS University
  - **link:** https://arxiv.org/pdf/2602.06555
  - **Simple LLM Summary:** This paper proposes a reinforcement learning-based framework for dynamically managing parallel task farm skeletons on serverless platforms like OpenFaaS. The method uses RL agents to learn autoscaling policies for worker pools, aiming to meet QoS targets. The results show that this AI-driven approach outperforms reactive, model-based baselines by better handling platform-specific limitations, improving QoS while maintaining efficient resource usage.

- **[arXiv260209] Same Engine, Multiple Gears: Parallelizing Fixpoint Iteration at Different Granularities (Extended Version)**
  - **tags:** [sys], [static analysis], [fixpoint iteration, parallelization, top-down solver, task granularity, immediate approach, independent approach]
  - **authors:** Ali Rasim Kocal, Michael Schwarz, Simmo Saan, Helmut Seidl
  - **institution:** Technical University of Munich, National University of Singapore, University of Tartu
  - **link:** https://arxiv.org/pdf/2602.06680
  - **Simple LLM Summary:** The paper proposes a parallelization method for a generic fixpoint engine used in static analysis, which is parametric in task granularity and implements two competing philosophies: an immediate approach with a shared thread-safe state and an independent approach with per-task states. The method is integrated into the Goblint static analysis framework and evaluated on large real-world programs. The results demonstrate that this flexible parallelization can significantly reduce analysis times.

- **[arXiv260209] Wonderboom -- Efficient, and Censorship-Resilient Signature Aggregation for Million Scale Consensus**
  - **tags:** [sys], [blockchain consensus], [signature aggregation, censorship-resilience, Byzantine Fault Tolerance, validator set, quorum attestation]
  - **authors:** Zeta Avarikioti, Ray Neiheiser, Krzysztof Pietrzak, Michelle X. Yeo
  - **institution:** TU Wien, Institute of Science and Technology Austria, Nanyang Technological University, Aarhus University, Common Prefix
  - **link:** https://arxiv.org/pdf/2602.06655
  - **Simple LLM Summary:** This paper introduces Wonderboom, a new signature aggregation protocol designed to speed up finality in large-scale blockchain consensus like Ethereum. It claims to aggregate millions of validator signatures 32 times faster than the current state-of-the-art while offering improved security against censorship and stake-shifting attacks. The authors also implement a simulation tool to demonstrate that Wonderboom can handle over 2 million signatures within a single Ethereum slot.

- **[arXiv260209] DualMap: Enabling Both Cache Affinity and Load Balancing for Distributed LLM Serving**
  - **tags:** [mlsys], [llm inference], [cache-affinity scheduling, load balancing, dual-mapping, SLO-aware routing, hotspot-aware rebalancing, dual-hash-ring scaling]
  - **authors:** Ying Yuan, Pengfei Zuo, Bo Wang, Zhangyu Chen, Zhipeng Tan, Zhou Yu
  - **institution:** Huazhong University of Science and Technology, Huawei
  - **link:** https://arxiv.org/pdf/2602.06502
  - **Simple LLM Summary:** The paper proposes DualMap, a dual-mapping scheduling strategy for distributed LLM serving that uses two hash functions to map each request to two candidate instances, intelligently selecting the better one to achieve both cache affinity and load balancing. It incorporates SLO-aware routing, hotspot-aware rebalancing, and lightweight scaling techniques to handle dynamic workloads. Experiments show DualMap improves effective request capacity by up to 2.25x under the same TTFT SLO constraints compared to state-of-the-art methods.

- **[arXiv260209] Implementing Grassroots Logic Programs with Multiagent Transition Systems and AI**
  - **tags:** [sys], [concurrent logic programming], [multiagent transition systems, operational semantics, deterministic semantics, grassroots platforms, peer-to-peer]
  - **authors:** Ehud Shapiro
  - **institution:** London School of Economics, Weizmann Institute of Science
  - **link:** https://arxiv.org/pdf/2602.06934
  - **Simple LLM Summary:** This paper presents deterministic operational semantics (dGLP and madGLP) for implementing Grassroots Logic Programs, a concurrent logic programming language for peer-to-peer systems. The authors prove the correctness of these semantics and describe their use as formal specifications for AI-assisted implementation in Dart for workstation and smartphone platforms. The main conclusion is the development of a mathematically verified, implementation-ready semantic framework for building distributed, grassroots applications.

- **[arXiv260209] Distributed Knowledge in Simplicial Models**
  - **tags:** [sys], [distributed computing], [simplicial complexes, epistemic logic, distributed knowledge, majority consensus, Kripke models]
  - **authors:** Éric Goubault, Jérémy Ledent, Sergio Rajsbaum
  - **institution:** LIX, CNRS, École Polytechnique, Institut Polytechnique de Paris; Université Paris Cité, CNRS, IRIF; Instituto de Matemáticas, UNAM
  - **link:** https://arxiv.org/pdf/2602.06945
  - **Simple LLM Summary:** This paper introduces simplicial complexes as models for multi-agent epistemic logic, shifting from world-based Kripke models to agent-view-based models. It connects distributed computing, logic, and topology, showing how distributed knowledge relates to solving the majority consensus task in different communication models and identifying logical obstructions when it is unsolvable.


**cs.AI/cs.LG contains "reinforcement learning" total: 30**
- [arXiv260209] Transformer-Based Reinforcement Learning for Autonomous Orbital Collision Avoidance in Partially Observable Environments [link](https://arxiv.org/pdf/2602.06088)
- [arXiv260209] Jackpot: Optimal Budgeted Rejection Sampling for Extreme Actor-Policy Mismatch Reinforcement Learning [link](https://arxiv.org/pdf/2602.06107)
- [arXiv260209] Self-Improving World Modelling with Latent Actions [link](https://arxiv.org/pdf/2602.06130)
- [arXiv260209] Flow Matching for Offline Reinforcement Learning with Discrete Actions [link](https://arxiv.org/pdf/2602.06138)
- [arXiv260209] Learning Rate Scaling across LoRA Ranks and Transfer to Full Finetuning [link](https://arxiv.org/pdf/2602.06204)
- [arXiv260209] Online Adaptive Reinforcement Learning with Echo State Networks for Non-Stationary Dynamics [link](https://arxiv.org/pdf/2602.06326)
- [arXiv260209] Training Data Selection with Gradient Orthogonality for Efficient Domain Adaptation [link](https://arxiv.org/pdf/2602.06359)
- [arXiv260209] Unlocking Noisy Real-World Corpora for Foundation Model Pre-Training via Quality-Aware Tokenization [link](https://arxiv.org/pdf/2602.06394)
- [arXiv260209] TrailBlazer: History-Guided Reinforcement Learning for Black-Box LLM Jailbreaking [link](https://arxiv.org/pdf/2602.06440)
- [arXiv260209] Prism: Spectral Parameter Sharing for Multi-Agent Reinforcement Learning [link](https://arxiv.org/pdf/2602.06476)
- [arXiv260209] AgentCPM-Explore: Realizing Long-Horizon Deep Exploration for Edge-Scale Agents [link](https://arxiv.org/pdf/2602.06485)
- [arXiv260209] Adaptive Uncertainty-Aware Tree Search for Robust Reasoning [link](https://arxiv.org/pdf/2602.06493)
- [arXiv260209] Progress Constraints for Reinforcement Learning in Behavior Trees [link](https://arxiv.org/pdf/2602.06525)
- [arXiv260209] Dynamics-Aligned Shared Hypernetworks for Zero-Shot Actuator Inversion [link](https://arxiv.org/pdf/2602.06550)
- [arXiv260209] SeeUPO: Sequence-Level Agentic-RL with Convergence Guarantees [link](https://arxiv.org/pdf/2602.06554)
- [arXiv260209] SPARC: Separating Perception And Reasoning Circuits for Test-time Scaling of VLMs [link](https://arxiv.org/pdf/2602.06566)
- [arXiv260209] Sample-Efficient Policy Space Response Oracles with Joint Experience Best Response [link](https://arxiv.org/pdf/2602.06599)
- [arXiv260209] The hidden risks of temporal resampling in clinical reinforcement learning [link](https://arxiv.org/pdf/2602.06603)
- [arXiv260209] Humanoid Manipulation Interface: Humanoid Whole-Body Manipulation from Robot-Free Demonstrations [link](https://arxiv.org/pdf/2602.06643)
- [arXiv260209] compar:IA: The French Government's LLM arena to collect French-language human prompts and preference data [link](https://arxiv.org/pdf/2602.06669)
- [arXiv260209] F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare [link](https://arxiv.org/pdf/2602.06717)
- [arXiv260209] Semantically Labelled Automata for Multi-Task Reinforcement Learning with LTL Instructions [link](https://arxiv.org/pdf/2602.06746)
- [arXiv260209] Soft Forward-Backward Representations for Zero-shot Reinforcement Learning with General Utilities [link](https://arxiv.org/pdf/2602.06769)
- [arXiv260209] Generating Data-Driven Reasoning Rubrics for Domain-Adaptive Reward Modeling [link](https://arxiv.org/pdf/2602.06795)
- [arXiv260209] AEGPO: Adaptive Entropy-Guided Policy Optimization for Diffusion Models [link](https://arxiv.org/pdf/2602.06825)
- [arXiv260209] A first realization of reinforcement learning-based closed-loop EEG-TMS [link](https://arxiv.org/pdf/2602.06907)
- [arXiv260209] Continuous-time reinforcement learning: ellipticity enables model-free value function approximation [link](https://arxiv.org/pdf/2602.06930)
- [arXiv260209] Cochain Perspectives on Temporal-Difference Signals for Learning Beyond Markov Dynamics [link](https://arxiv.org/pdf/2602.06939)
- [arXiv260209] Optimal Derivative Feedback Control for an Active Magnetic Levitation System: An Experimental Study on Data-Driven Approaches [link](https://arxiv.org/pdf/2602.06944)
- [arXiv260209] InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning [link](https://arxiv.org/pdf/2602.06960)

**cs.AI/cs.LG contains "accelerate" total: 16**
- [arXiv260209] Analyzing Diffusion and Autoregressive Vision Language Models in Multimodal Embedding Space [link](https://arxiv.org/pdf/2602.06056)
- [arXiv260209] Compressing LLMs with MoP: Mixture of Pruners [link](https://arxiv.org/pdf/2602.06127)
- [arXiv260209] Stop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding [link](https://arxiv.org/pdf/2602.06161)
- [arXiv260209] To 2:4 Sparsity and Beyond: Neuron-level Activation Function to Accelerate LLM Pre-Training [link](https://arxiv.org/pdf/2602.06183)
- [arXiv260209] Accelerating Vision Transformers on Brain Processing Unit [link](https://arxiv.org/pdf/2602.06300)
- [arXiv260209] Beyond Code Contributions: How Network Position, Temporal Bursts, and Code Review Activities Shape Contributor Influence in Large-Scale Open Source Ecosystems [link](https://arxiv.org/pdf/2602.06426)
- [arXiv260209] Principle-Evolvable Scientific Discovery via Uncertainty Minimization [link](https://arxiv.org/pdf/2602.06448)
- [arXiv260209] SaDiT: Efficient Protein Backbone Design via Latent Structural Tokenization and Diffusion Transformers [link](https://arxiv.org/pdf/2602.06706)
- [arXiv260209] GhostCite: A Large-Scale Analysis of Citation Validity in the Age of Large Language Models [link](https://arxiv.org/pdf/2602.06718)
- [arXiv260209] AEGPO: Adaptive Entropy-Guided Policy Optimization for Diffusion Models [link](https://arxiv.org/pdf/2602.06825)
- [arXiv260209] Are Deep Learning Based Hybrid PDE Solvers Reliable? Why Training Paradigms and Update Strategies Matter [link](https://arxiv.org/pdf/2602.06842)
- [arXiv260209] Rethinking Multi-Condition DiTs: Eliminating Redundant Attention via Position-Alignment and Keyword-Scoping [link](https://arxiv.org/pdf/2602.06850)
- [arXiv260209] AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents [link](https://arxiv.org/pdf/2602.06855)
- [arXiv260209] When RL Meets Adaptive Speculative Training: A Unified Training-Serving System [link](https://arxiv.org/pdf/2602.06932)
- [arXiv260209] DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos [link](https://arxiv.org/pdf/2602.06949)
- [arXiv260209] InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning [link](https://arxiv.org/pdf/2602.06960)

## 2026-02-10

**cs.DC total: 18**

- **[arXiv260210] Parallel Track Transformers: Enabling Fast GPU Inference with Reduced Synchronization**
  - **tags:** [mlsys], [llm inference], [tensor parallelism, parallel track transformer, multi-GPU synchronization, model architecture, serving efficiency]
  - **authors:** Chong Wang, Nan Du, Tom Gunter, Tao Lei, Kulin Seth, Senyu Tong, Jianyu Wang, Guoli Yin, Xiyou Zhou, Kelvin Zou, Ruoming Pang
  - **institution:** Apple
  - **link:** https://arxiv.org/pdf/2602.07306
  - **Simple LLM Summary:** The paper proposes the Parallel Track (PT) Transformer, a novel model architecture that uses multiple independent transformer "tracks" to minimize inter-GPU synchronization during inference. This approach significantly reduces synchronization operations compared to standard tensor parallelism. The method, integrated into popular serving stacks, demonstrates improved latency and throughput while maintaining model quality.

- **[arXiv260210] Privacy-Preserving Coding Schemes for Multi-Access Distributed Computing Models**
  - **tags:** [sys], [distributed computing], [multi-access distributed computing (MADC), placement delivery arrays (PDAs), privacy-preserving coding, coded MapReduce]
  - **authors:** Shanuja Sasi
  - **institution:** Indian Institute of Technology Kanpur
  - **link:** https://arxiv.org/pdf/2602.07850
  - **Simple LLM Summary:** This paper introduces privacy constraints into the Multi-Access Distributed Computing (MADC) model and develops private coded schemes for specific connectivity models. The core method involves constructing new families of extended placement delivery arrays to derive coding schemes that guarantee the privacy of each reducer's assigned function.

- **[arXiv260210] Knowledge Graphs-Driven Intelligence for Distributed Decision Systems**
  - **tags:** [mlsys], [others], [Knowledge Graphs, Graph Embeddings, GraphSAGE, Knowledge Map, distributed resource orchestration]
  - **authors:** Rosario Napoli, Gabriele Morabito, Antonio Celesti, Massimo Villari, Maria Fazio
  - **institution:** University of Messina
  - **link:** https://arxiv.org/pdf/2602.07614
  - **Simple LLM Summary:** This paper proposes a distributed knowledge-sharing architecture where nodes use Knowledge Graphs and Graph Embeddings, aggregated via GraphSAGE, to build a global Knowledge Map for decentralized coordination. The method was validated through simulations of distributed resource orchestration, showing it effectively maintains semantic coherence and adaptability in dynamic environments like Edge Computing and IoT.

- **[arXiv260210] Don't Always Pick the Highest-Performing Model: An Information Theoretic View of LLM Ensemble Selection**
  - **tags:** [mlsys], [llm inference], [ensemble selection, mutual information, greedy algorithm, Gaussian-copula, error correlation, budgeted selection]
  - **authors:** Yigit Turkmen, Baturalp Buyukates, Melih Bastopcu
  - **institution:** Bilkent University, University of Birmingham
  - **link:** https://arxiv.org/pdf/2602.08003
  - **Simple LLM Summary:** The paper proposes a greedy mutual-information selection algorithm for forming LLM ensembles under a query budget, aiming to maximize the information about the true label rather than just picking the highest-performing models. It shows that due to correlated model errors, simply selecting the most accurate models is suboptimal, and its method consistently outperforms baselines across multiple datasets.

- **[arXiv260210] Wireless Streamlet: A Spectrum-Aware and Cognitive Consensus Protocol for Edge IoT**
  - **tags:** [sys], [blockchain consensus], [Channel-Aware Leader Election (CALE), coded dual-chain architecture, TDMA voting schedule, erasure coding, spectrum-aware consensus]
  - **authors:** Taotao Wang, Long Shi, Fang Liu, Qing Yang, Shengli Zhang
  - **institution:** Shenzhen University, Nanjing University of Science and Technology
  - **link:** https://arxiv.org/pdf/2602.07630
  - **Simple LLM Summary:** This paper proposes Wireless Streamlet, a consensus protocol for edge IoT that introduces a Channel-Aware Leader Election mechanism and a coded dual-chain architecture to improve reliability and efficiency in lossy wireless environments. It achieves deterministic spectral access and reduces storage overhead. Experiments show it achieves higher throughput and lower latency than baselines in such conditions.

- **[arXiv260210] Multi-Agentic AI for Fairness-Aware and Accelerated Multi-modal Large Model Inference in Real-world Mobile Edge Networks**
  - **tags:** [mlsys], [llm inference], [multi-agentic ai, mobile edge networks, containerized deployment, natural language reasoning, prompt scheduling, fairness-aware optimization]
  - **authors:** Haiyuan Li, Hari Madhukumar, Shuangyi Yan, Yulei Wu, Dimitra Simeonidou
  - **institution:** University of Bristol
  - **link:** https://arxiv.org/pdf/2602.07215
  - **Simple LLM Summary:** The paper proposes a multi-agentic AI framework, powered by foundation language models, to optimize latency and fairness for multi-modal large model inference in mobile edge networks. The agents cooperate to manage prompt routing and model deployment using natural language reasoning over runtime data. Experiments on a city-wide testbed show the solution reduces average latency by over 80% and improves fairness without requiring fine-tuning.

- **[arXiv260210] Fork, Explore, Commit: OS Primitives for Agentic Exploration**
  - **tags:** [mlsys], [others], [branch context, copy-on-write, FUSE, atomic commit, process isolation, first-commit-wins]
  - **authors:** Cong Wang, Yusheng Zheng
  - **institution:** Multikernel Technologies, Inc., University of California, Santa Cruz
  - **link:** https://arxiv.org/pdf/2602.08199
  - **Simple LLM Summary:** The paper introduces a new OS abstraction called the branch context, realized through BranchFS (a FUSE-based filesystem) and a proposed branch() syscall, to provide isolated, copy-on-write environments for AI agents performing parallel exploration. It enables atomic commit and rollback of both filesystem and process state with a first-commit-wins resolution. Preliminary evaluation shows BranchFS achieves fast branch creation and low commit overhead.

- **[arXiv260210] The CAPSARII Approach to Cyber-Secure Wearable, Ultra-Low-Power Networked Sensors for Soldier Health Monitoring**
  - **tags:** [mlsys], [others], [Internet of Battlefield Things (IoBT), edge AI, cloud-based analytics, smart textile integration, ultra-low-power optimization, encryption, authentication]
  - **authors:** Luciano Bozzi, Christian Celidonio, Umberto Nuzzi, Massimo Biagini, Stefano Cherubin, Asbjørn Djupdal, Tor Andre Haugdahl, Andrea Aliverti, Alessandra Angelucci, Giovanni Agosta, Gerardo Pelosi, Paolo Belluco, Samuele Polistina, Riccardo Volpi, Luigi Malagò, Michael Schneider, Florian Wieczorek, Xabier Eguiluz
  - **institution:** Sea Sky Technologies, NTNU, Politecnico di Milano, LWT3, QUAESTA AI, BORN GmbH, IKERLAN
  - **link:** https://arxiv.org/pdf/2602.08080
  - **Simple LLM Summary:** The CAPSARII project proposes a wearable sensor system and IoBT framework to monitor soldier health using edge AI for real-time decision support and cloud analytics. It focuses on ultra-low-power design, smart textile integration, and cybersecurity to enhance situational awareness and operational effectiveness. The approach aims to transform military operations through a robust, data-driven decision support tool.

- **[arXiv260210] ZipFlow: a Compiler-based Framework to Unleash Compressed Data Movement for Modern GPUs**
  - **tags:** [mlsys], [GPU kernels], [data compression, GPU decompression, compiler-based framework, parallel patterns, PCIe bandwidth optimization]
  - **authors:** Gwangoo Yeo, Zhiyang Shen, Wei Cui, Matteo Interlandi, Rathijit Sen, Bailu Ding, Qi Chen, Minsoo Rhu
  - **institution:** KAIST, Tsinghua University, Microsoft Research Asia, Microsoft Gray Systems Lab, Microsoft Research
  - **link:** https://arxiv.org/pdf/2602.08190
  - **Simple LLM Summary:** ZipFlow is a compiler-based framework that optimizes compressed data transfer for GPU-accelerated data analytics by classifying compression algorithms into parallel patterns and applying generalized GPU scheduling strategies. It achieves significant speedups, with an average 2.08× improvement over state-of-the-art GPU compression libraries and 3.14× faster than CPU-based query engines on TPC-H benchmarks.

- **[arXiv260210] Accuracy-Delay Trade-Off in LLM Offloading via Token-Level Uncertainty**
  - **tags:** [mlsys], [llm inference], [mobile edge computing, offloading strategy, token-level uncertainty, greedy offloading algorithm, accuracy-delay trade-off]
  - **authors:** Yumin Kim, Hyeonsu Lyu, Minjae Lee, Hyun Jong Yang
  - **institution:** Seoul National University, POSTECH
  - **link:** https://arxiv.org/pdf/2602.07958
  - **Simple LLM Summary:** The paper proposes an uncertainty-aware offloading framework for LLM inference in mobile edge computing, using a margin-based token-level uncertainty metric to decide whether to compute locally or offload to an edge server. It introduces a greedy offloading algorithm (GOA) that prioritizes offloading high-uncertainty queries to minimize delay while maintaining accuracy. Experiments show GOA achieves a favorable accuracy-delay trade-off, outperforming baseline strategies across varying user densities.

- **[arXiv260210] Modalities, a PyTorch-native Framework For Large-scale LLM Training and Research**
  - **tags:** [mlsys], [llm training], [PyTorch-native, distributed pretraining, modular design, declarative configuration, parallelization strategies]
  - **authors:** Max Lübbering, Timm Ruland, Richard Rutmann, Felix Stollenwerk, David Fitzek, Michael Fromm, Alexander Weber, Rafet Sifa, Nicolas Flores-Herr, Joachim Köhler, Mehdi Ali
  - **institution:** Fraunhofer IAIS, AI Sweden, University of Bonn, Lamarr Institute
  - **link:** https://arxiv.org/pdf/2602.08387
  - **Simple LLM Summary:** The paper introduces Modalities, a PyTorch-native framework designed for large-scale LLM training and research. It integrates advanced parallelization for efficient pretraining and systematic ablations at scale, and uses a modular, declarative configuration to improve reproducibility and extensibility compared to existing frameworks.

- **[arXiv260210] Towards CXL Resilience to CPU Failures**
  - **tags:** [sys], [fault-tolerance], [CXL 3.0, hardware cache coherence, logging unit, data replication, recovery protocol]
  - **authors:** Antonis Psistakis, Burak Ocalan, Chloe Alverti, Fabien Chaix, Ramnatthan Alagappan, Josep Torrellas
  - **institution:** University of Illinois Urbana-Champaign, National Technical University of Athens, Foundation for Research and Technology-Hellas
  - **link:** https://arxiv.org/pdf/2602.08271
  - **Simple LLM Summary:** This paper proposes ReCXL, a system that extends the CXL specification to be resilient to CPU failures by replicating cache line updates to other nodes and logging them in hardware units. The method enables fault-tolerant execution with only a 30% performance slowdown compared to a non-fault-tolerant platform.

- **[arXiv260210] HEAL: Online Incremental Recovery for Leaderless Distributed Systems Across Persistency Models**
  - **tags:** [sys], [distributed systems recovery], [leaderless systems, incremental recovery, linearizable consistency, memory persistency models, fault tolerance]
  - **authors:** Antonis Psistakis, Burak Ocalan, Fabien Chaix, Ramnatthan Alagappan, Josep Torrellas
  - **institution:** University of Illinois Urbana-Champaign, Foundation for Research and Technology-Hellas
  - **link:** https://arxiv.org/pdf/2602.08257
  - **Simple LLM Summary:** The paper proposes HEAL, a low-overhead online incremental recovery scheme for non-transactional leaderless distributed systems. It demonstrates that HEAL recovers clusters significantly faster (120ms vs 360s) and with less throughput degradation (8.7% vs 16.2%) compared to conventional recovery methods for leaderless systems.

- **[arXiv260210] PARD: Enhancing Goodput for Inference Pipeline via Proactive Request Dropping**
  - **tags:** [mlsys], [multi-modal inference], [proactive request dropping, adaptive request priority, runtime information, latency objective, goodput enhancement]
  - **authors:** Zhixin Zhao, Yitao Hu, Simin Chen, Mingfang Ji, Wei Yang, Yuhao Zhang, Laiping Zhao, Wenxin Li, Xiulong Liu, Wenyu Qu, Hao Wang
  - **institution:** Tianjin University, University of Texas at Dallas, Stevens Institute of Technology
  - **link:** https://arxiv.org/pdf/2602.08747
  - **Simple LLM Summary:** PARD introduces a proactive request dropping system for DNN inference pipelines, using runtime information to decide when to drop requests and an adaptive priority mechanism to select which ones to drop. It achieves higher goodput than reactive approaches by reducing wasted computations and drop rates, as validated on a 64-GPU cluster with real-world workloads.

- **[arXiv260210] Equilibria: Fair Multi-Tenant CXL Memory Tiering At Scale**
  - **tags:** [sys], [memory tiering], [CXL, multi-tenancy, fairness policies, promotion and demotion, thrashing suppression, TPP, Linux kernel]
  - **authors:** Kaiyang Zhao, Neha Gholkar, Hasan Maruf, Abhishek Dhanotia, Johannes Weiner, Gregory Price, Ning Sun, Bhavya Dwivedi, Stuart Clark, Dimitrios Skarlatos
  - **institution:** Carnegie Mellon University, Meta
  - **link:** https://arxiv.org/pdf/2602.08800
  - **Simple LLM Summary:** The paper presents Equilibria, an OS framework for fair, multi-tenant memory tiering using CXL-attached memory. It provides per-container controls, enforces fairness policies, and suppresses thrashing to mitigate interference. Evaluation in a hyperscaler fleet shows it improves performance over the state-of-the-art Linux solution by up to 52% for production workloads.

- **[arXiv260210] RIFLE: Robust Distillation-based FL for Deep Model Deployment on Resource-Constrained IoT Networks**
  - **tags:** [mlsys], [others], [federated learning, knowledge distillation, logit-based knowledge transfer, Kullback-Leibler divergence, robust aggregation, non-IID data, IoT]
  - **authors:** Pouria Arefijamal, Mahdi Ahmadlou, Bardia Safaei, Jörg Henkel
  - **institution:** Sharif University of Technology, Karlsruhe Institute of Technology (KIT)
  - **link:** https://arxiv.org/pdf/2602.08446
  - **Simple LLM Summary:** This paper introduces RIFLE, a robust federated learning framework that uses knowledge distillation and logit-based knowledge transfer instead of gradient sharing to train deep models on resource-constrained IoT devices. It employs a KL divergence-based validation mechanism to assess client update reliability without exposing raw data. The method significantly improves accuracy, reduces training time, and enhances robustness against poisoning attacks compared to conventional FL baselines.

- **[arXiv260210] Recursive QAOA for Interference-Aware Resource Allocation in Wireless Networks**
  - **tags:** [ai], [quantum optimization], [RQAOA, QAOA, QUBO, Ising model, variable elimination, channel assignment]
  - **authors:** Kuan-Cheng Chen, Hiromichi Matsuyama, Wei-hao Huang, Yu Yamashiro
  - **institution:** J-ij Europe Ltd, Jij Inc.
  - **link:** https://arxiv.org/pdf/2602.07483
  - **Simple LLM Summary:** This paper proposes using the Recursive Quantum Approximate Optimization Algorithm (RQAOA) to solve interference-aware wireless channel assignment problems formulated as QUBO/Ising models. The method recursively eliminates variables based on measured quantum correlations to reduce problem size and improve feasibility. The results show that RQAOA can find optimal solutions and mitigate scalability issues associated with standard QAOA for this application.

- **[arXiv260210] DynamiQ: Accelerating Gradient Synchronization using Compressed Multi-hop All-reduce**
  - **tags:** [mlsys], [llm training], [gradient quantization, multi-hop all-reduce, decompress-accumulate-recompress kernel, PyTorch DDP, NCCL P2P]
  - **authors:** Wenchen Han, Shay Vargaftik, Michael Mitzenmacher, Ran Ben Basat
  - **institution:** University College London, Broadcom, Harvard University
  - **link:** https://arxiv.org/pdf/2602.08923
  - **Simple LLM Summary:** DynamiQ is a quantization framework optimized for multi-hop all-reduce in distributed training, introducing techniques to better represent partial sums and a fused kernel for fast execution. It accelerates gradient synchronization while maintaining near-baseline accuracy, outperforming state-of-the-art methods by up to 34.2% across various LLM tasks and scales.


**cs.AI/cs.LG contains "reinforcement learning" total: 68**
- [arXiv260210] ToolSelf: Unifying Task Execution and Self-Reconfiguration via Tool-Driven Intrinsic Adaptation [link](https://arxiv.org/pdf/2602.07883)
- [arXiv260210] CoMI-IRL: Contrastive Multi-Intention Inverse Reinforcement Learning [link](https://arxiv.org/pdf/2602.07496)
- [arXiv260210] Secure Code Generation via Online Reinforcement Learning with Vulnerability Reward Model [link](https://arxiv.org/pdf/2602.07422)
- [arXiv260210] Cerebellar-Inspired Residual Control for Fault Recovery: From Inference-Time Adaptation to Structural Consolidation [link](https://arxiv.org/pdf/2602.07227)
- [arXiv260210] Efficient Anti-exploration via VQVAE and Fuzzy Clustering in Offline Reinforcement Learning [link](https://arxiv.org/pdf/2602.07889)
- [arXiv260210] Proximal Action Replacement for Behavior Cloning Actor-Critic in Offline Reinforcement Learning [link](https://arxiv.org/pdf/2602.07441)
- [arXiv260210] Risk-Sensitive Exponential Actor Critic [link](https://arxiv.org/pdf/2602.07202)
- [arXiv260210] Efficient Planning in Reinforcement Learning via Model Introspection [link](https://arxiv.org/pdf/2602.07719)
- [arXiv260210] Horizon Imagination: Efficient On-Policy Training in Diffusion World Models [link](https://arxiv.org/pdf/2602.08032)
- [arXiv260210] AceGRPO: Adaptive Curriculum Enhanced Group Relative Policy Optimization for Autonomous Machine Learning Engineering [link](https://arxiv.org/pdf/2602.07906)
- [arXiv260210] Time Series Reasoning via Process-Verifiable Thinking Data Synthesis and Scheduling for Tailored LLM Reasoning [link](https://arxiv.org/pdf/2602.07830)
- [arXiv260210] VideoTemp-o3: Harmonizing Temporal Grounding and Video Understanding in Agentic Thinking-with-Videos [link](https://arxiv.org/pdf/2602.07801)
- [arXiv260210] Do We Need Adam? Surprisingly Strong and Sparse Reinforcement Learning with SGD in LLMs [link](https://arxiv.org/pdf/2602.07729)
- [arXiv260210] rePIRL: Learn PRM with Inverse RL for LLM Reasoning [link](https://arxiv.org/pdf/2602.07832)
- [arXiv260210] The Laplacian Keyboard: Beyond the Linear Span [link](https://arxiv.org/pdf/2602.07730)
- [arXiv260210] FIRE: Frobenius-Isometry Reinitialization for Balancing the Stability-Plasticity Tradeoff [link](https://arxiv.org/pdf/2602.08040)
- [arXiv260210] Adaptive Scaffolding for Cognitive Engagement in an Intelligent Tutoring System [link](https://arxiv.org/pdf/2602.07308)
- [arXiv260210] The Optimal Token Baseline: Variance Reduction for Long-Horizon LLM-RL [link](https://arxiv.org/pdf/2602.07078)
- [arXiv260210] Graph-Enhanced Deep Reinforcement Learning for Multi-Objective Unrelated Parallel Machine Scheduling [link](https://arxiv.org/pdf/2602.08052)
- [arXiv260210] Generative Reasoning Re-ranker [link](https://arxiv.org/pdf/2602.07774)
- [arXiv260210] Joint Reward Modeling: Internalizing Chain-of-Thought for Efficient Visual Reward Models [link](https://arxiv.org/pdf/2602.07533)
- [arXiv260210] High Fidelity Textual User Representation over Heterogeneous Sources via Reinforcement Learning [link](https://arxiv.org/pdf/2602.07333)
- [arXiv260210] TodoEvolve: Learning to Architect Agent Planning Systems [link](https://arxiv.org/pdf/2602.07839)
- [arXiv260210] Preference Conditioned Multi-Objective Reinforcement Learning: Decomposed, Diversity-Driven Policy Optimization [link](https://arxiv.org/pdf/2602.07764)
- [arXiv260210] Scalable Dexterous Robot Learning with AR-based Remote Human-Robot Interactions [link](https://arxiv.org/pdf/2602.07341)
- [arXiv260210] Epigraph-Guided Flow Matching for Safe and Performant Offline Reinforcement Learning [link](https://arxiv.org/pdf/2602.08054)
- [arXiv260210] TeleBoost: A Systematic Alignment Framework for High-Fidelity, Controllable, and Robust Video Generation [link](https://arxiv.org/pdf/2602.07595)
- [arXiv260210] When Is Compositional Reasoning Learnable from Verifiable Rewards? [link](https://arxiv.org/pdf/2602.07992)
- [arXiv260210] Direct Soft-Policy Sampling via Langevin Dynamics [link](https://arxiv.org/pdf/2602.07873)
- [arXiv260210] Unified Biomolecular Trajectory Generation via Pretrained Variational Bridge [link](https://arxiv.org/pdf/2602.07588)
- [arXiv260210] Learning to Self-Verify Makes Language Models Better Reasoners [link](https://arxiv.org/pdf/2602.07594)
- [arXiv260210] Objective Decoupling in Social Reinforcement Learning: Recovering Ground Truth from Sycophantic Majorities [link](https://arxiv.org/pdf/2602.08092)
- [arXiv260210] MARTI-MARS$^2$: Scaling Multi-Agent Self-Search via Reinforcement Learning for Code Generation [link](https://arxiv.org/pdf/2602.07848)
- [arXiv260210] Regret Analysis of Unichain Average Reward Constrained MDPs with General Parameterization [link](https://arxiv.org/pdf/2602.08000)
- [arXiv260210] Interpretable Failure Analysis in Multi-Agent Reinforcement Learning Systems [link](https://arxiv.org/pdf/2602.08104)
- [arXiv260210] CADO: From Imitation to Cost Minimization for Heatmap-based Solvers in Combinatorial Optimization [link](https://arxiv.org/pdf/2602.08210)
- [arXiv260210] DrugR: Optimizing Molecular Drugs through LLM-based Explicit Reasoning [link](https://arxiv.org/pdf/2602.08213)
- [arXiv260210] SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning [link](https://arxiv.org/pdf/2602.08234)
- [arXiv260210] Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs [link](https://arxiv.org/pdf/2602.08241)
- [arXiv260210] Learning in Context, Guided by Choice: A Reward-Free Paradigm for Reinforcement Learning with Transformers [link](https://arxiv.org/pdf/2602.08244)
- [arXiv260210] When Do Multi-Agent Systems Outperform? Analysing the Learning Efficiency of Agentic Systems [link](https://arxiv.org/pdf/2602.08272)
- [arXiv260210] Towards Efficient Large Language Reasoning Models via Extreme-Ratio Chain-of-Thought Compression [link](https://arxiv.org/pdf/2602.08324)
- [arXiv260210] Who Deserves the Reward? SHARP: Shapley Credit-based Optimization for Multi-Agent System [link](https://arxiv.org/pdf/2602.08335)
- [arXiv260210] OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration [link](https://arxiv.org/pdf/2602.08344)
- [arXiv260210] Does Your Reasoning Model Implicitly Know When to Stop Thinking? [link](https://arxiv.org/pdf/2602.08354)
- [arXiv260210] Learning Human-Like Badminton Skills for Humanoid Robots [link](https://arxiv.org/pdf/2602.08370)
- [arXiv260210] Reinforcement Learning with Backtracking Feedback [link](https://arxiv.org/pdf/2602.08377)
- [arXiv260210] Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning [link](https://arxiv.org/pdf/2602.08382)
- [arXiv260210] Intelligent support for Human Oversight: Integrating Reinforcement Learning with Gaze Simulation to Personalize Highlighting [link](https://arxiv.org/pdf/2602.08403)
- [arXiv260210] Beyond Correctness: Learning Robust Reasoning via Transfer [link](https://arxiv.org/pdf/2602.08489)
- [arXiv260210] Contextual Rollout Bandits for Reinforcement Learning with Verifiable Rewards [link](https://arxiv.org/pdf/2602.08499)
- [arXiv260210] Learning Self-Correction in Vision-Language Models via Rollout Augmentation [link](https://arxiv.org/pdf/2602.08503)
- [arXiv260210] Dialogue Model Optimization via Agent Game and Adaptive Tree-based GRPO [link](https://arxiv.org/pdf/2602.08533)
- [arXiv260210] Conditional Sequence Modeling for Safe Reinforcement Learning [link](https://arxiv.org/pdf/2602.08584)
- [arXiv260210] Breaking the Grid: Distance-Guided Reinforcement Learning in Large Discrete and Hybrid Action Spaces [link](https://arxiv.org/pdf/2602.08616)
- [arXiv260210] From Robotics to Sepsis Treatment: Offline RL via Geometric Pessimism [link](https://arxiv.org/pdf/2602.08655)
- [arXiv260210] LLaDA2.1: Speeding Up Text Diffusion via Token Editing [link](https://arxiv.org/pdf/2602.08676)
- [arXiv260210] Learning To Sample From Diffusion Models Via Inverse Reinforcement Learning [link](https://arxiv.org/pdf/2602.08689)
- [arXiv260210] SoK: The Pitfalls of Deep Reinforcement Learning for Cybersecurity [link](https://arxiv.org/pdf/2602.08690)
- [arXiv260210] Finite-State Controllers for (Hidden-Model) POMDPs using Deep Reinforcement Learning [link](https://arxiv.org/pdf/2602.08734)
- [arXiv260210] Bayesian Preference Learning for Test-Time Steerable Reward Models [link](https://arxiv.org/pdf/2602.08819)
- [arXiv260210] Learning the Value Systems of Societies with Preference-based Multi-objective Reinforcement Learning [link](https://arxiv.org/pdf/2602.08835)
- [arXiv260210] Dr. MAS: Stable Reinforcement Learning for Multi-Agent LLM Systems [link](https://arxiv.org/pdf/2602.08847)
- [arXiv260210] AnomSeer: Reinforcing Multimodal LLMs to Reason for Time-Series Anomaly Detection [link](https://arxiv.org/pdf/2602.08868)
- [arXiv260210] Efficient and Stable Reinforcement Learning for Diffusion Language Models [link](https://arxiv.org/pdf/2602.08905)
- [arXiv260210] StealthRL: Reinforcement Learning Paraphrase Attacks for Multi-Detector Evasion of AI-Text Detectors [link](https://arxiv.org/pdf/2602.08934)
- [arXiv260210] Learning to Coordinate via Quantum Entanglement in Multi-Agent Reinforcement Learning [link](https://arxiv.org/pdf/2602.08965)
- [arXiv260210] iGRPO: Self-Feedback-Driven LLM Reasoning [link](https://arxiv.org/pdf/2602.09000)

**cs.AI/cs.LG contains "accelerate" total: 30**
- [arXiv260210] GRAFT: Decoupling Ranking and Calibration for Survival Analysis [link](https://arxiv.org/pdf/2602.07884)
- [arXiv260210] aerial-autonomy-stack -- a Faster-than-real-time, Autopilot-agnostic, ROS2 Framework to Simulate and Deploy Perception-based Drones [link](https://arxiv.org/pdf/2602.07264)
- [arXiv260210] Interpreting Physics in Video World Models [link](https://arxiv.org/pdf/2602.07050)
- [arXiv260210] FlashVID: Efficient Video Large Language Models via Training-free Tree-based Spatiotemporal Token Merging [link](https://arxiv.org/pdf/2602.08024)
- [arXiv260210] PipeMFL-240K: A Large-scale Dataset and Benchmark for Object Detection in Pipeline Magnetic Flux Leakage Imaging [link](https://arxiv.org/pdf/2602.07044)
- [arXiv260210] GraphAgents: Knowledge Graph-Guided Agentic AI for Cross-Domain Materials Design [link](https://arxiv.org/pdf/2602.07491)
- [arXiv260210] MaD-Mix: Multi-Modal Data Mixtures via Latent Space Coupling for Vision-Language Model Training [link](https://arxiv.org/pdf/2602.07790)
- [arXiv260210] When Excellence Stops Producing Knowledge: A Practitioner's Observation on Research Funding [link](https://arxiv.org/pdf/2602.07039)
- [arXiv260210] Accelerating Social Science Research via Agentic Hypothesization and Experimentation [link](https://arxiv.org/pdf/2602.07983)
- [arXiv260210] DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents [link](https://arxiv.org/pdf/2602.07035)
- [arXiv260210] Systematic Performance Assessment of Deep Material Networks for Multiscale Material Modeling [link](https://arxiv.org/pdf/2602.07192)
- [arXiv260210] Scalable Dexterous Robot Learning with AR-based Remote Human-Robot Interactions [link](https://arxiv.org/pdf/2602.07341)
- [arXiv260210] "Death" of a Chatbot: Investigating and Designing Toward Psychologically Safe Endings for Human-AI Relationships [link](https://arxiv.org/pdf/2602.07193)
- [arXiv260210] MSP-LLM: A Unified Large Language Model Framework for Complete Material Synthesis Planning [link](https://arxiv.org/pdf/2602.07543)
- [arXiv260210] Optimizing Few-Step Generation with Adaptive Matching Distillation [link](https://arxiv.org/pdf/2602.07345)
- [arXiv260210] STEP: Warm-Started Visuomotor Policies with Spatiotemporal Consistency Prediction [link](https://arxiv.org/pdf/2602.08245)
- [arXiv260210] Noise Stability of Transformer Models [link](https://arxiv.org/pdf/2602.08287)
- [arXiv260210] Fast Flow Matching based Conditional Independence Tests for Causal Discovery [link](https://arxiv.org/pdf/2602.08315)
- [arXiv260210] Prism: Spectral-Aware Block-Sparse Attention [link](https://arxiv.org/pdf/2602.08426)
- [arXiv260210] GISA: A Benchmark for General Information-Seeking Assistant [link](https://arxiv.org/pdf/2602.08543)
- [arXiv260210] Predicting Future Utility: Global Combinatorial Optimization for Task-Agnostic KV Cache Eviction [link](https://arxiv.org/pdf/2602.08585)
- [arXiv260210] QUOKA: Query-Oriented KV Selection For Efficient LLM Prefill [link](https://arxiv.org/pdf/2602.08722)
- [arXiv260210] Default Machine Learning Hyperparameters Do Not Provide Informative Initialization for Bayesian Optimization [link](https://arxiv.org/pdf/2602.08774)
- [arXiv260210] Multimodal Learning for Arcing Detection in Pantograph-Catenary Systems [link](https://arxiv.org/pdf/2602.08792)
- [arXiv260210] FlattenGPT: Depth Compression for Transformer with Layer Flattening [link](https://arxiv.org/pdf/2602.08858)
- [arXiv260210] ARO: A New Lens On Matrix Optimization For Large Models [link](https://arxiv.org/pdf/2602.09006)
- [arXiv260210] ANCRe: Adaptive Neural Connection Reassignment for Efficient Depth Scaling [link](https://arxiv.org/pdf/2602.09009)
- [arXiv260210] Electron-Informed Coarse-Graining Molecular Representation Learning for Real-World Molecular Physics [link](https://arxiv.org/pdf/2602.07087)
- [arXiv260210] Fast and Robust Likelihood-Guided Diffusion Posterior Sampling with Amortized Variational Inference [link](https://arxiv.org/pdf/2602.07102)
- [arXiv260210] Optimizing Spectral Prediction in MXene-Based Metasurfaces Through Multi-Channel Spectral Refinement and Savitzky-Golay Smoothing [link](https://arxiv.org/pdf/2602.08406)

## 2026-02-11

**cs.DC total: 11**

- **[arXiv260211] The Coordination Criterion**
  - **tags:** [sys], [distributed computing], [Lamport histories, happens-before, monotonicity, coordination-free implementation, asynchronous message-passing]
  - **authors:** Joseph M. Hellerstein
  - **institution:** UC Berkeley, Amazon Web Services
  - **link:** https://arxiv.org/pdf/2602.09435
  - **Simple LLM Summary:** The paper proposes a general Coordination Criterion, showing that a distributed specification can be implemented without coordination if and only if its observable outcomes are monotone with respect to the extension of Lamport histories. This criterion provides a unifying explanation for when coordination is intrinsically necessary across various distributed computing results.

- **[arXiv260211] Distributed Hybrid Parallelism for Large Language Models: Comparative Study and System Design Guide**
  - **tags:** [mlsys], [llm training], [distributed parallelism, hybrid parallelization, communication-computation overlap, collective operations, automated strategy search]
  - **authors:** Hossam Amer, Rezaul Karim, Ali Pourranjbar, Weiwei Zhang, Walid Ahmed, Boxing Chen
  - **institution:** Huawei Canada
  - **link:** https://arxiv.org/pdf/2602.09109
  - **Simple LLM Summary:** This paper provides a comprehensive review and comparative study of distributed hybrid parallelism strategies for large language models, analyzing collective operations and hybrid designs to optimize training and inference. It concludes by offering empirical insights and a system design guide to help select effective parallelism strategies, while also highlighting current limitations and future directions for large-scale model development.

- **[arXiv260211] Harvest: Adaptive Photonic Switching Schedules for Collective Communication in Scale-up Domains**
  - **tags:** [mlsys], [cluster infrastructure], [silicon photonics, circuit switching, topology reconfiguration, dynamic programming, collective communication, AllReduce, Recursive Doubling]
  - **authors:** Mahir Rahman, Samuel Joseph, Nihar Kodkani, Behnaz Arzani, Vamsi Addanki
  - **institution:** Purdue University, Microsoft Research
  - **link:** https://arxiv.org/pdf/2602.09188
  - **Simple LLM Summary:** Harvest is a method for synthesizing adaptive photonic switching schedules to optimize collective communication in scale-up domains. It uses a dynamic programming approach to balance reconfiguration delay against congestion and propagation delay. The synthesized schedules significantly reduce collective completion time compared to static or naive per-step reconfiguration baselines.

- **[arXiv260211] ALPHA-PIM: Analysis of Linear Algebraic Processing for High-Performance Graph Applications on a Real Processing-In-Memory System**
  - **tags:** [sys], [graph processing], [Processing-In-Memory, UPMEM, data partitioning, sparse matrix-vector multiplication, DMA engines, interconnection networks]
  - **authors:** Marzieh Barkhordar, Alireza Tabatabaeian, Mohammad Sadrosadati, Christina Giannoula, Juan Gomez Luna, Izzat El Hajj, Onur Mutlu, Alaa R. Alameldeen
  - **institution:** Simon Fraser University, ETH Zürich, University of Toronto, NVIDIA, American University of Beirut
  - **link:** https://arxiv.org/pdf/2602.09174
  - **Simple LLM Summary:** This paper implements and analyzes common graph algorithms on a real-world UPMEM Processing-In-Memory (PIM) system to accelerate data-intensive workloads. The study finds that optimal data partitioning across PIM cores is crucial for performance and identifies key hardware limitations, recommending future improvements in instruction-level parallelism, DMA engines, and core interconnects.

- **[arXiv260211] LLM-CoOpt: A Co-Design and Optimization Framework for Efficient LLM Inference on Heterogeneous Platforms**
  - **tags:** [mlsys], [llm inference], [key-value cache optimization, FP8 quantization, grouped-query attention, paged attention, algorithm-hardware co-design]
  - **authors:** Jie Kong, Wei Wang, Jiehan Zhou, Chen Yu
  - **institution:** Shandong University of Science and Technology, Huazhong University of Science and Technology
  - **link:** https://arxiv.org/pdf/2602.09323
  - **Simple LLM Summary:** The paper proposes LLM-CoOpt, a co-design framework that integrates KV cache optimization with FP8 quantization, grouped-query attention, and paged attention to improve LLM inference efficiency. Experiments show it increases throughput by up to 13.43% and reduces latency by up to 16.79% while maintaining accuracy, providing a practical optimization path for real-world deployment.

- **[arXiv260211] It's not a lie if you don't get caught: simplifying reconfiguration in SMR through dirty logs**
  - **tags:** [sys], [distributed systems, consensus protocols], [state-machine replication, reconfiguration, modularity, dirty logs, inner log, outer log, Gauss, Rialo blockchain]
  - **authors:** Allen Clement, Natacha Crooks, Neil Giridharan, Alex Shamis
  - **institution:** Subzero Labs, UC Berkeley
  - **link:** https://arxiv.org/pdf/2602.09441
  - **Simple LLM Summary:** The paper introduces Gauss, a reconfiguration engine that decouples membership changes from specific consensus protocols by separating a protocol's "inner log" from a sanitized "outer log." This modular approach allows independent upgrades of membership, failure thresholds, and the consensus protocol itself with minimal downtime. The initial evaluation on the Rialo blockchain demonstrates that this separation enables seamless evolution of the SMR stack across diverse protocol implementations.

- **[arXiv260211] Revealing the Challenges of Attention-FFN Disaggregation for Modern MoE Models and Hardware Systems**
  - **tags:** [mlsys], [llm inference], [Attention-FFN Disaggregation, Expert Parallelism, roofline model, Hardware FLOPS Utilization, MoE models]
  - **authors:** Guowei Liu, Hongming Li, Yaning Guo, Yongxi Lyu, Mo Zhou, Yi Liu, Zhaogeng Li, Yanpeng Wang
  - **institution:** Baidu Inc.
  - **link:** https://arxiv.org/pdf/2602.09721
  - **Simple LLM Summary:** This paper systematically analyzes the Attention-FFN Disaggregation (AFD) architecture for deploying large MoE models by extending the roofline model to the communication level. It finds that AFD has a performance "dead zone" on standard clusters due to bandwidth limitations and scaling imbalance, but can be beneficial on high-bandwidth Superpod-class hardware with specific model characteristics. The conclusion is that AFD is a promising but non-universal solution, effective only for certain hardware-model combinations.

- **[arXiv260211] Efficient Remote Prefix Fetching with GPU-native Media ASICs**
  - **tags:** [mlsys], [llm inference], [KV cache reuse, video codec compression, GPU-native decoding, pipelined fetching]
  - **authors:** Liang Mi, Weijun Wang, Jinghan Chen, Ting Cao, Haipeng Dai, Yunxin Liu
  - **institution:** Nanjing University, Institute for AI Industry Research (AIR), Tsinghua University
  - **link:** https://arxiv.org/pdf/2602.09725
  - **Simple LLM Summary:** The paper proposes KVFetcher, a system that accelerates remote KV cache reuse for LLM inference by compressing the cache into a video format and using GPU-native hardware codecs for efficient, pipelined decompression and transmission. This approach minimizes the time-to-first-token (TTFT) by avoiding resource contention and masking network latency. Experiments show it reduces TTFT by up to 3.51x compared to state-of-the-art methods while maintaining lossless accuracy.

- **[arXiv260211] High-performance Vector-length Agnostic Quantum Circuit Simulations on ARM Processors**
  - **tags:** [mlsys], [GPU kernels], [vector-length agnostic, SVE, quantum state-vector simulation, memory layout adjustment, load buffering, gate fusion]
  - **authors:** Ruimin Shi, Gabin Schieffer, Pei-Hung Lin, Maya Gokhale, Andreas Herten, Ivy Peng
  - **institution:** KTH Royal Institute of Technology, Lawrence Livermore National Laboratory, Jülich Supercomputing Centre
  - **link:** https://arxiv.org/pdf/2602.09604
  - **Simple LLM Summary:** The paper proposes a vector-length agnostic (VLA) design for quantum circuit simulations, employing optimization techniques like VLEN-adaptive memory layout and gate fusion. It implements this in Google's Qsim and evaluates it on ARM processors with SVE. The single-source VLA implementation achieves significant speedups, demonstrating high-performance portability across different ARM platforms.

- **[arXiv260211] Rashomon Sets and Model Multiplicity in Federated Learning**
  - **tags:** [mlsys], [others], [Rashomon set, model multiplicity, federated learning, global Rashomon set, t-agreement Rashomon set, individual Rashomon set, privacy constraints, fairness]
  - **authors:** Xenia Heilmann, Luca Corbucci, Mattia Cerrato
  - **institution:** Johannes Gutenberg University, University of Pisa
  - **link:** https://arxiv.org/pdf/2602.09520
  - **Simple LLM Summary:** This paper formalizes the concept of Rashomon sets for Federated Learning, proposing three new definitions: a global set, a t-agreement set, and individual client sets. It shows how to estimate model multiplicity metrics under FL's privacy constraints and introduces a multiplicity-aware FL pipeline. The empirical results demonstrate that these federated Rashomon sets provide valuable insights, enabling clients to choose models better aligned with their local data and fairness requirements.

- **[arXiv260211] Architectural Foundations for Checkpointing and Restoration in Quantum HPC Systems**
  - **tags:** [sys], [quantum computing systems], [checkpointing, restoration, dynamic circuits, mid-circuit measurements, classical feed forward, conditional execution, variational eigensolvers, quantum approximate optimization]
  - **authors:** Qiang Guan, Qinglei Cao, Xiaoyi Lu, Siyuan Niu
  - **institution:** Kent State University, Saint Louis University, University of Florida, University of Central Florida
  - **link:** https://arxiv.org/pdf/2602.09325
  - **Simple LLM Summary:** This paper proposes a checkpointing and restoration framework for quantum HPC systems that redefines the problem as capturing control flow and algorithmic state rather than quantum states. It leverages dynamic circuit capabilities like mid-circuit measurements and classical feedback to enable resilient, restartable quantum workflows. The approach is designed to support iterative quantum algorithms common in scientific computing.


**cs.AI/cs.LG contains "reinforcement learning" total: 31**
- [arXiv260211] CausalGDP: Causality-Guided Diffusion Policies for Reinforcement Learning [link](https://arxiv.org/pdf/2602.09207)
- [arXiv260211] Reward Modeling for Reinforcement Learning-Based LLM Reasoning: Design, Challenges, and Evaluation [link](https://arxiv.org/pdf/2602.09305)
- [arXiv260211] Training deep physical neural networks with local physical information bottleneck [link](https://arxiv.org/pdf/2602.09569)
- [arXiv260211] EExApp: GNN-Based Reinforcement Learning for Radio Unit Energy Optimization in 5G O-RAN [link](https://arxiv.org/pdf/2602.09206)
- [arXiv260211] Rollout-Training Co-Design for Efficient LLM-Based Multi-Agent Reinforcement Learning [link](https://arxiv.org/pdf/2602.09578)
- [arXiv260211] On the Optimal Reasoning Length for RL-Trained Language Models [link](https://arxiv.org/pdf/2602.09591)
- [arXiv260211] Squeezing More from the Stream : Learning Representation Online for Streaming Reinforcement Learning [link](https://arxiv.org/pdf/2602.09396)
- [arXiv260211] Risk-sensitive reinforcement learning using expectiles, shortfall risk and optimized certainty equivalent risk [link](https://arxiv.org/pdf/2602.09300)
- [arXiv260211] $n$-Musketeers: Reinforcement Learning Shapes Collaboration Among Language Models [link](https://arxiv.org/pdf/2602.09173)
- [arXiv260211] Online Learning in MDPs with Partially Adversarial Transitions and Losses [link](https://arxiv.org/pdf/2602.09474)
- [arXiv260211] UI-Venus-1.5 Technical Report [link](https://arxiv.org/pdf/2602.09082)
- [arXiv260211] Bridging Efficiency and Transparency: Explainable CoT Compression in Multimodal Large Reasoning Models [link](https://arxiv.org/pdf/2602.09485)
- [arXiv260211] SpotAgent: Grounding Visual Geo-localization in Large Vision-Language Models through Agentic Reasoning [link](https://arxiv.org/pdf/2602.09463)
- [arXiv260211] Boltzmann Reinforcement Learning for Noise resilience in Analog Ising Machines [link](https://arxiv.org/pdf/2602.09162)
- [arXiv260211] P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads [link](https://arxiv.org/pdf/2602.09443)
- [arXiv260211] ExO-PPO: an Extended Off-policy Proximal Policy Optimization Algorithm [link](https://arxiv.org/pdf/2602.09726)
- [arXiv260211] Grounding LTL Tasks in Sub-Symbolic RL Environments for Zero-Shot Generalization [link](https://arxiv.org/pdf/2602.09761)
- [arXiv260211] Flexible Entropy Control in RLVR with Gradient-Preserving Perspective [link](https://arxiv.org/pdf/2602.09782)
- [arXiv260211] A Controlled Study of Double DQN and Dueling DQN Under Cross-Environment Transfer [link](https://arxiv.org/pdf/2602.09810)
- [arXiv260211] Code2World: A GUI World Model via Renderable Code Generation [link](https://arxiv.org/pdf/2602.09856)
- [arXiv260211] ESTAR: Early-Stopping Token-Aware Reasoning For Efficient Inference [link](https://arxiv.org/pdf/2602.10004)
- [arXiv260211] Answer First, Reason Later: Aligning Search Relevance via Mode-Balanced Reinforcement Learning [link](https://arxiv.org/pdf/2602.10006)
- [arXiv260211] A Collaborative Safety Shield for Safe and Efficient CAV Lane Changes in Congested On-Ramp Merging [link](https://arxiv.org/pdf/2602.10007)
- [arXiv260211] ADORA: Training Reasoning Models with Dynamic Advantage Estimation on Reinforcement Learning [link](https://arxiv.org/pdf/2602.10019)
- [arXiv260211] Fake-HR1: Rethinking reasoning of vision language model for synthetic image detection [link](https://arxiv.org/pdf/2602.10042)
- [arXiv260211] Optimistic World Models: Efficient Exploration in Model-Based Deep Reinforcement Learning [link](https://arxiv.org/pdf/2602.10044)
- [arXiv260211] Long Chain-of-Thought Compression via Fine-Grained Group Policy Optimization [link](https://arxiv.org/pdf/2602.10048)
- [arXiv260211] Features as Rewards: Scalable Supervision for Open-Ended Tasks via Interpretability [link](https://arxiv.org/pdf/2602.10067)
- [arXiv260211] Anagent For Enhancing Scientific Table & Figure Analysis [link](https://arxiv.org/pdf/2602.10081)
- [arXiv260211] CODE-SHARP: Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs [link](https://arxiv.org/pdf/2602.10085)
- [arXiv260211] Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning [link](https://arxiv.org/pdf/2602.10090)

**cs.AI/cs.LG contains "accelerate" total: 12**
- [arXiv260211] SpinCastML an Open Decision-Making Application for Inverse Design of Electrospinning Manufacturing: A Machine Learning, Optimal Sampling and Inverse Monte Carlo Approach [link](https://arxiv.org/pdf/2602.09120)
- [arXiv260211] FlyAOC: Evaluating Agentic Ontology Curation of Drosophila Scientific Knowledge Bases [link](https://arxiv.org/pdf/2602.09163)
- [arXiv260211] Genocide by Algorithm in Gaza: Artificial Intelligence, Countervailing Responsibility, and the Corruption of Public Discourse [link](https://arxiv.org/pdf/2602.09202)
- [arXiv260211] From Adam to Adam-Like Lagrangians: Second-Order Nonlocal Dynamics [link](https://arxiv.org/pdf/2602.09101)
- [arXiv260211] Accelerating Post-Quantum Cryptography via LLM-Driven Hardware-Software Co-Design [link](https://arxiv.org/pdf/2602.09410)
- [arXiv260211] Learning to Discover Iterative Spectral Algorithms [link](https://arxiv.org/pdf/2602.09530)
- [arXiv260211] Efficient Distance Pruning for Process Suffix Comparison in Prescriptive Process Monitoring [link](https://arxiv.org/pdf/2602.09039)
- [arXiv260211] UniComp: A Unified Evaluation of Large Language Model Compression via Pruning, Quantization and Distillation [link](https://arxiv.org/pdf/2602.09130)
- [arXiv260211] Stemphonic: All-at-once Flexible Multi-stem Music Generation [link](https://arxiv.org/pdf/2602.09891)
- [arXiv260211] Drug Release Modeling using Physics-Informed Neural Networks [link](https://arxiv.org/pdf/2602.09963)
- [arXiv260211] Position: Message-passing and spectral GNNs are two sides of the same coin [link](https://arxiv.org/pdf/2602.10031)
- [arXiv260211] E2CAR: An Efficient 2D-CNN Framework for Real-Time EEG Artifact Removal on Edge Devices [link](https://arxiv.org/pdf/2602.09035)

## 2026-02-12

**cs.DC total: 10**

- **[arXiv260212] Fine-Tuning GPT-5 for GPU Kernel Generation**
  - **tags:** [mlsys], [GPU kernels], [reinforcement learning, fine-tuning, Triton, GPU kernel generation, post-training, KernelBench]
  - **authors:** Ali Tehrani, Yahya Emara, Essam Wissam, Wojciech Paluch, Waleed Atallah, Łukasz Dudziak, Mohamed S. Abdelfattah
  - **institution:** Makora
  - **link:** https://arxiv.org/pdf/2602.11000
  - **Simple LLM Summary:** The paper presents a method for fine-tuning GPT-5 using reinforcement learning to generate efficient GPU kernels, specifically for Triton code. It demonstrates that this approach significantly improves correctness and performance over a baseline model and outperforms a standard compiler, showing RL is effective for specialized domains where supervised training data is scarce.

- **[arXiv260212] Execution-Centric Characterization of FP8 Matrix Cores, Asynchronous Execution, and Structured Sparsity on AMD MI300A**
  - **tags:** [mlsys], [GPU kernels], [microbenchmarking, FP8 matrix cores, asynchronous compute engines (ACE), structured sparsity, occupancy-aware scheduling]
  - **authors:** Aaron Jarmusch, Connor Vitz, Sunita Chandrasekaran
  - **institution:** University of Delaware
  - **link:** https://arxiv.org/pdf/2602.10262
  - **Simple LLM Summary:** This paper uses targeted microbenchmarks to characterize the execution behavior of advanced accelerator features (FP8 matrix cores, asynchronous execution, and structured sparsity) on the AMD MI300A APU. It quantifies performance trade-offs and provides practical guidance for scheduling and concurrency decisions on unified nodes.

- **[arXiv260212] Flash-SD-KDE: Accelerating SD-KDE with Tensor Cores**
  - **tags:** [mlsys], [GPU kernels], [kernel density estimation, score debiasing, tensor cores, matrix multiplication, GPU acceleration]
  - **authors:** Elliot L. Epstein, Rajat Vadiraj Dwaraknath, John Winnicki
  - **institution:** Stanford University
  - **link:** https://arxiv.org/pdf/2602.10378
  - **Simple LLM Summary:** This paper introduces Flash-SD-KDE, a method that accelerates Score-Debiased Kernel Density Estimation (SD-KDE) by reordering its computation to expose matrix-multiplication structure, enabling the use of Tensor Cores on GPUs. The approach achieves speedups of up to 47x over a strong GPU baseline and makes large-scale score-debiased density estimation practical.

- **[arXiv260212] Implementability of Global Distributed Protocols modulo Network Architectures**
  - **tags:** [sys], [distributed systems verification], [global protocols, implementability, network architectures, coherence conditions, symbolic algorithms, Sprout(A)]
  - **authors:** Elaine Li, Thomas Wies
  - **institution:** New York University
  - **link:** https://arxiv.org/pdf/2602.10320
  - **Simple LLM Summary:** The paper proposes a network-parametric framework for verifying the implementability of global distributed protocols. It introduces a set of Coherence Conditions that precisely characterize implementability under minimal operational axioms for message buffers. The main conclusion is that this characterization generalizes to five common asynchronous network architectures and enables the development of a network-parametric verification tool, Sprout(A), without sacrificing performance.

- **[arXiv260212] Chamfer-Linkage for Hierarchical Agglomerative Clustering**
  - **tags:** [ai], [clustering algorithms], [hierarchical agglomerative clustering, chamfer distance, linkage function, dendrogram]
  - **authors:** Kishen N Gowda, Willem Fletcher, MohammadHossein Bateni, Laxman Dhulipala, D Ellis Hershkowitz, Rajesh Jayaram, Jakub Łącki
  - **institution:** University of Maryland, Brown University, Google Research
  - **link:** https://arxiv.org/pdf/2602.10444
  - **Simple LLM Summary:** This paper introduces Chamfer-linkage, a novel linkage function for Hierarchical Agglomerative Clustering (HAC) that uses Chamfer distance to measure inter-cluster similarity. Theoretically, it maintains O(n²) time complexity, and experimentally, it consistently outperforms classical linkages like average-linkage and Ward's method across diverse datasets, establishing it as a practical drop-in replacement.

- **[arXiv260212] Computing Least Fixed Points with Overwrite Semantics in Parallel and Distributed Systems**
  - **tags:** [sys], [parallel and distributed algorithms], [fixed points, overwrite semantics, monotone inflationary functions, bounded staleness, i-locality]
  - **authors:** Vijay K. Garg, Rohan Garg
  - **institution:** University of Texas at Austin, Purdue University
  - **link:** https://arxiv.org/pdf/2602.10486
  - **Simple LLM Summary:** This paper introduces methods for computing least fixed points of monotone inflationary functions using coordinate-wise overwriting semantics in parallel and distributed systems. It proves convergence theorems under progressively relaxed synchronization models, including interleaving, parallel with update-only-on-change, and distributed with bounded staleness and locality. The results provide the first exact convergence guarantees for overwrite-based parallel updates without requiring join operations or contraction assumptions.

- **[arXiv260212] BOute: Cost-Efficient LLM Serving with Heterogeneous LLMs and GPUs via Multi-Objective Bayesian Optimization**
  - **tags:** [mlsys], [llm inference], [multi-objective Bayesian optimization, heterogeneous query routing, heterogeneous model deployment, quality-aware scheduling]
  - **authors:** Youhe Jiang, Fangcheng Fu, Eiko Yoneki
  - **institution:** University of Cambridge, Shanghai Jiao Tong University
  - **link:** https://arxiv.org/pdf/2602.10729
  - **Simple LLM Summary:** The paper introduces BOute, a scheduling system that uses multi-objective Bayesian optimization to co-optimize query routing strategies and model deployment across heterogeneous LLMs and GPUs for cost-efficient serving. It demonstrates significant improvements in cost-efficiency, outperforming state-of-the-art systems by up to 157% under the same budget or reducing costs by an average of 38% while meeting performance targets.

- **[arXiv260212] Min-Sum Uniform Coverage Problem by Autonomous Mobile Robots**
  - **tags:** [sys], [distributed robotics], [Look-Compute-Move (LCM) model, non-rigid motion, fair asynchronous scheduler, oblivious robots, deterministic distributed algorithm]
  - **authors:** Animesh Maiti, Abhinav Chakraborty, Bibhuti Das, Subhash Bhagat, Krishnendu Mukhopadhyaya
  - **institution:** Indian Institute of Technology Jodhpur, Birla Institute of Technology, Mesra, Université du Québec en Outaouais, Indian Statistical Institute
  - **link:** https://arxiv.org/pdf/2602.11125
  - **Simple LLM Summary:** This paper presents deterministic distributed algorithms for autonomous, oblivious robots to achieve uniform coverage on a line segment and a circle while minimizing the total distance traveled. For the line segment, the algorithm always achieves the optimal min-sum cost. For the circle, the work characterizes which initial configurations are deterministically unsolvable and provides an optimal algorithm for all other cases.

- **[arXiv260212] Authenticated Workflows: A Systems Approach to Protecting Agentic AI**
  - **tags:** [mlsys], [others], [authenticated workflows, cryptographic attestations, MAPL policy language, deterministic security, boundary protection]
  - **authors:** Mohan Rajagopalan, Vinay Rao
  - **institution:** MACAW Security, Inc., ROOST.tools
  - **link:** https://arxiv.org/pdf/2602.10465
  - **Simple LLM Summary:** The paper introduces "authenticated workflows," a deterministic security layer for agentic AI that protects four boundaries (prompts, tools, data, context) by enforcing intent and integrity using cryptographic proofs and runtime policy enforcement. It proposes the MAPL policy language for scalable, dynamic constraint expression and demonstrates practicality through integration with nine AI frameworks. The approach provides deterministic security, eliminating entire attack classes by design and achieving 100% recall with zero false positives in empirical tests.

- **[arXiv260212] KORAL: Knowledge Graph Guided LLM Reasoning for SSD Operational Analysis**
  - **tags:** [mlsys], [llm inference], [knowledge graph, large language model, ontology, SSD telemetry, descriptive analysis, predictive analysis, prescriptive analysis, what-if analysis]
  - **authors:** Mayur Akewar, Sandeep Madireddy, Dongsheng Luo, Janki Bhimani
  - **institution:** Florida International University, Argonne National Laboratory
  - **link:** https://arxiv.org/pdf/2602.10246
  - **Simple LLM Summary:** This paper presents KORAL, a framework that integrates Large Language Models (LLMs) with structured Knowledge Graphs (KGs) to analyze Solid State Drive (SSD) operations. It uses a Data KG from telemetry and a Literature KG from documents to guide the LLM in providing evidence-based, explainable analysis. The evaluation shows that KORAL delivers expert-level diagnosis and actionable recommendations for improving SSD service quality.


**cs.AI/cs.LG contains "reinforcement learning" total: 34**
- [arXiv260212] Chatting with Images for Introspective Visual Thinking [link](https://arxiv.org/pdf/2602.11073)
- [arXiv260212] Learning to Evict from Key-Value Cache [link](https://arxiv.org/pdf/2602.10238)
- [arXiv260212] SimuScene: Training and Benchmarking Code Generation to Simulate Physical Scenarios [link](https://arxiv.org/pdf/2602.10840)
- [arXiv260212] Divide, Harmonize, Then Conquer It: Shooting Multi-Commodity Flow Problems with Multimodal Language Models [link](https://arxiv.org/pdf/2602.11057)
- [arXiv260212] Mitigating Reward Hacking in RLHF via Bayesian Non-negative Reward Modeling [link](https://arxiv.org/pdf/2602.10623)
- [arXiv260212] RePO: Bridging On-Policy Learning and Off-Policy Knowledge through Rephrasing Policy Optimization [link](https://arxiv.org/pdf/2602.10819)
- [arXiv260212] Neuro-symbolic Action Masking for Deep Reinforcement Learning [link](https://arxiv.org/pdf/2602.10598)
- [arXiv260212] Are More Tokens Rational? Inference-Time Scaling in Language Models as Adaptive Resource Rationality [link](https://arxiv.org/pdf/2602.10329)
- [arXiv260212] Internalizing Meta-Experience into Memory for Guided Reinforcement Learning in Large Language Models [link](https://arxiv.org/pdf/2602.10224)
- [arXiv260212] Why Does RL Generalize Better Than SFT? A Data-Centric Perspective on VLM Post-Training [link](https://arxiv.org/pdf/2602.10815)
- [arXiv260212] Online Causal Kalman Filtering for Stable and Effective Policy Optimization [link](https://arxiv.org/pdf/2602.10609)
- [arXiv260212] Prioritize the Process, Not Just the Outcome: Rewarding Latent Thought Trajectories Improves Reasoning in Looped Language Models [link](https://arxiv.org/pdf/2602.10520)
- [arXiv260212] Resource-Efficient Model-Free Reinforcement Learning for Board Games [link](https://arxiv.org/pdf/2602.10894)
- [arXiv260212] Breaking the Curse of Repulsion: Optimistic Distributionally Robust Policy Optimization for Off-Policy Generative Recommendation [link](https://arxiv.org/pdf/2602.10430)
- [arXiv260212] Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters [link](https://arxiv.org/pdf/2602.10604)
- [arXiv260212] Safety Recovery in Reasoning Models Is Only a Few Early Steering Steps Away [link](https://arxiv.org/pdf/2602.11096)
- [arXiv260212] ICA: Information-Aware Credit Assignment for Visually Grounded Long-Horizon Information-Seeking Agents [link](https://arxiv.org/pdf/2602.10863)
- [arXiv260212] DataChef: Cooking Up Optimal Data Recipes for LLM Adaptation via Reinforcement Learning [link](https://arxiv.org/pdf/2602.11089)
- [arXiv260212] Found-RL: foundation model-enhanced reinforcement learning for autonomous driving [link](https://arxiv.org/pdf/2602.10458)
- [arXiv260212] Interpretable Attention-Based Multi-Agent PPO for Latency Spike Resolution in 6G RAN Slicing [link](https://arxiv.org/pdf/2602.11076)
- [arXiv260212] LLM-Based Scientific Equation Discovery via Physics-Informed Token-Regularized Policy Optimization [link](https://arxiv.org/pdf/2602.10576)
- [arXiv260212] Multimodal Information Fusion for Chart Understanding: A Survey of MLLMs -- Evolution, Limitations, and Cognitive Enhancement [link](https://arxiv.org/pdf/2602.10138)
- [arXiv260212] What Makes Value Learning Efficient in Residual Reinforcement Learning? [link](https://arxiv.org/pdf/2602.10539)
- [arXiv260212] MetaphorStar: Image Metaphor Understanding and Reasoning with End-to-End Visual Reinforcement Learning [link](https://arxiv.org/pdf/2602.10575)
- [arXiv260212] Confounding Robust Continuous Control via Automatic Reward Shaping [link](https://arxiv.org/pdf/2602.10305)
- [arXiv260212] Near-Constant Strong Violation and Last-Iterate Convergence for Online CMDPs via Decaying Safety Margins [link](https://arxiv.org/pdf/2602.10917)
- [arXiv260212] Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation [link](https://arxiv.org/pdf/2602.10699)
- [arXiv260212] Control Reinforcement Learning: Token-Level Mechanistic Analysis via Learned SAE Feature Steering [link](https://arxiv.org/pdf/2602.10437)
- [arXiv260212] AudioRouter: Data Efficient Audio Understanding via RL based Dual Reasoning [link](https://arxiv.org/pdf/2602.10439)
- [arXiv260212] VESPO: Variational Sequence-Level Soft Policy Optimization for Stable Off-Policy LLM Training [link](https://arxiv.org/pdf/2602.10693)
- [arXiv260212] OmniVL-Guard: Towards Unified Vision-Language Forgery Detection and Grounding via Balanced RL [link](https://arxiv.org/pdf/2602.10687)
- [arXiv260212] Asymmetric Prompt Weighting for Reinforcement Learning with Verifiable Rewards [link](https://arxiv.org/pdf/2602.11128)
- [arXiv260212] Data-Efficient Hierarchical Goal-Conditioned Reinforcement Learning via Normalizing Flows [link](https://arxiv.org/pdf/2602.11142)
- [arXiv260212] Beyond SMILES: Evaluating Agentic Systems for Drug Discovery [link](https://arxiv.org/pdf/2602.10163)

**cs.AI/cs.LG contains "accelerate" total: 17**
- [arXiv260212] From Natural Language to Materials Discovery:The Materials Knowledge Navigation Agent [link](https://arxiv.org/pdf/2602.11123)
- [arXiv260212] From Buffers to Registers: Unlocking Fine-Grained FlashAttention with Hybrid-Bonded 3D NPU Co-Design [link](https://arxiv.org/pdf/2602.11016)
- [arXiv260212] Domain Knowledge Guided Bayesian Optimization For Autonomous Alignment Of Complex Scientific Instruments [link](https://arxiv.org/pdf/2602.10670)
- [arXiv260212] Flow caching for autoregressive video generation [link](https://arxiv.org/pdf/2602.10825)
- [arXiv260212] Adaptive Optimization via Momentum on Variance-Normalized Gradients [link](https://arxiv.org/pdf/2602.10204)
- [arXiv260212] Transport, Don't Generate: Deterministic Geometric Flows for Combinatorial Optimization [link](https://arxiv.org/pdf/2602.10794)
- [arXiv260212] Semi-Supervised Cross-Domain Imitation Learning [link](https://arxiv.org/pdf/2602.10793)
- [arXiv260212] $μ$pscaling small models: Principled warm starts and hyperparameter transfer [link](https://arxiv.org/pdf/2602.10545)
- [arXiv260212] Compute Only Once: UG-Separation for Efficient Large Recommendation Models [link](https://arxiv.org/pdf/2602.10455)
- [arXiv260212] When to Memorize and When to Stop: Gated Recurrent Memory for Long-Context Reasoning [link](https://arxiv.org/pdf/2602.10560)
- [arXiv260212] On the Use of a Large Language Model to Support the Conduction of a Systematic Mapping Study: A Brief Report from a Practitioner's View [link](https://arxiv.org/pdf/2602.10147)
- [arXiv260212] AI-PACE: A Framework for Integrating AI into Medical Education [link](https://arxiv.org/pdf/2602.10527)
- [arXiv260212] Confounding Robust Continuous Control via Automatic Reward Shaping [link](https://arxiv.org/pdf/2602.10305)
- [arXiv260212] Automated Model Design using Gated Neuron Selection in Telecom [link](https://arxiv.org/pdf/2602.10854)
- [arXiv260212] Asymmetric Prompt Weighting for Reinforcement Learning with Verifiable Rewards [link](https://arxiv.org/pdf/2602.11128)
- [arXiv260212] TokaMark: A Comprehensive Benchmark for MAST Tokamak Plasma Models [link](https://arxiv.org/pdf/2602.10132)
- [arXiv260212] EVA: Towards a universal model of the immune system [link](https://arxiv.org/pdf/2602.10168)

## 2026-02-13

**cs.DC total: 15**

- **[arXiv260213] Differentially Private Perturbed Push-Sum Protocol and Its Application in Non-Convex Optimization**
  - **tags:** [mlsys], [others], [differential privacy, decentralized optimization, push-sum protocol, sensitivity estimation, partial communication]
  - **authors:** Yiming Zhou, Kaiping Xue, Enhong Chen
  - **institution:** University of Science and Technology of China
  - **link:** https://arxiv.org/pdf/2602.11544
  - **Simple LLM Summary:** The paper proposes DPPS, a differentially private push-sum protocol for decentralized communication, and its application PartPSP for non-convex optimization. PartPSP uses partial communication to reduce noise injection and improve utility under the same privacy budget. The method is theoretically proven to converge and experimentally shown to outperform existing privacy-preserving decentralized optimization algorithms.

- **[arXiv260213] Real Life Is Uncertain. Consensus Should Be Too!**
  - **tags:** [sys], [distributed systems], [consensus protocols, probabilistic failure model, fault tolerance, quorum intersection]
  - **authors:** Reginald Frank, Soujanya Ponnapalli, Octavio Lomeli, Neil Giridharan, Marcos K Aguilera, Natacha Crooks
  - **institution:** UC Berkeley, Broadcom
  - **link:** https://arxiv.org/pdf/2602.11362
  - **Simple LLM Summary:** The paper argues that traditional consensus protocols, which rely on a threshold-based failure model, are an oversimplification. It proposes adopting a probabilistic failure model that leverages individual machine failure curves to design more reliable and efficient systems. The main conclusion is that this approach can bypass traditional bottlenecks like majority quorum intersection, leading to more cost-effective and sustainable distributed systems.

- **[arXiv260213] GORGO: Maximizing KV-Cache Reuse While Minimizing Network Latency in Cross-Region LLM Load Balancing**
  - **tags:** [mlsys], [llm inference], [KV-cache reuse, prefix caching, load balancing, network-aware routing, centralized proxy]
  - **authors:** Alessio Ricci Toniolo, Abinaya Dinesh, Rome Thorstenson
  - **institution:** Arcadia Research Team
  - **link:** https://arxiv.org/pdf/2602.11688
  - **Simple LLM Summary:** The paper introduces GORGO, a load balancing method for cross-region LLM inference that minimizes Time-to-First-Token by jointly optimizing KV-cache reuse and network latency. It demonstrates that GORGO reduces tail latency and improves average performance by preventing inefficient cross-region forwarding, with its centralized proxy implementation being 2.5x faster in median TTFT than previous approaches.

- **[arXiv260213] MUSE: Multi-Tenant Model Serving With Seamless Model Updates**
  - **tags:** [mlsys], [others], [multi-tenant model serving, score calibration, two-level score transformation, intent-based routing, reference distribution mapping]
  - **authors:** Cláudio Correia, Alberto E. A. Ferreira, Lucas Martins, Miguel P. Bento, Sofia Guerreiro, Ricardo Ribeiro Pereira, Ana Sofia Gomes, Jacopo Bono, Hugo Ferreira, Pedro Bizarro
  - **institution:** Feedzai
  - **link:** https://arxiv.org/pdf/2602.11776
  - **Simple LLM Summary:** The paper introduces MUSE, a multi-tenant model serving framework that decouples model scores from client decision boundaries using a two-level score transformation to map outputs to a stable reference distribution. This enables seamless model updates without requiring clients to recalibrate thresholds, reducing update lead time from weeks to minutes. The system has been deployed at scale, processing billions of events and saving millions in fraud losses by improving model resilience against shifting attacks.

- **[arXiv260213] Designing Scalable Rate Limiting Systems: Algorithms, Architecture, and Distributed Solutions**
  - **tags:** [sys], [distributed systems], [Redis Sorted Sets, Rolling Window algorithm, Lua scripting, Redis Cluster, CAP theorem]
  - **authors:** Bo Guan
  - **institution:** WynerTech Solutions
  - **link:** https://arxiv.org/pdf/2602.11741
  - **Simple LLM Summary:** This paper proposes a scalable distributed rate limiting system using Redis Sorted Sets and the Rolling Window algorithm, with atomic operations ensured by Lua scripting. The core contribution is quantifying the trade-off between accuracy and memory cost for this approach compared to Token Bucket and Fixed Window algorithms. The architecture is deployed on a Redis Cluster, accepting AP from the CAP theorem as a pragmatic trade-off for availability and scalability.

- **[arXiv260213] PAM: Processing Across Memory Hierarchy for Efficient KV-centric LLM Serving System**
  - **tags:** [mlsys], [llm inference], [processing-in-memory (PIM), KV cache, attention computation, memory hierarchy, PAMattention, KV scheduling]
  - **authors:** Lian Liu, Shixin Zhao, Yutian Zhou, Yintao He, Mengdi Wang, Yinhe Han, Ying Wang
  - **institution:** Institute of Computing Technology, Chinese Academy of Sciences; University of Chinese Academy of Sciences
  - **link:** https://arxiv.org/pdf/2602.11521
  - **Simple LLM Summary:** The paper proposes PAM, a KV-centric LLM serving system that coordinates heterogeneous PIM-enabled memory devices in a hierarchical architecture to address the bandwidth and capacity bottlenecks of KV cache operations. Its core methods include exploiting context locality for KV token distribution, the PAMattention algorithm for parallel computation, and dynamic online KV scheduling. The system significantly enhances the efficiency and scalability of LLM serving by simultaneously meeting high bandwidth and large capacity demands.

- **[arXiv260213] LAER-MoE: Load-Adaptive Expert Re-layout for Efficient Mixture-of-Experts Training**
  - **tags:** [mlsys], [llm training], [Fully Sharded Expert Parallel (FSEP), load balancing planner, expert re-layout, All-to-All communication, expert parallelism]
  - **authors:** Xinyi Liu, Yujie Wang, Fangcheng Fu, Xuefeng Xiao, Huixia Li, Jiashi Li, Bin Cui
  - **institution:** Peking University, Shanghai Jiao Tong University, Bytedance Seed
  - **link:** https://arxiv.org/pdf/2602.11686
  - **Simple LLM Summary:** This paper introduces LAER-MoE, an efficient training framework for Mixture-of-Experts models. Its core method is a novel Fully Sharded Expert Parallel (FSEP) paradigm that enables flexible expert parameter re-layout and a load balancing planner to address load imbalance during training. The system achieves up to 1.69x acceleration compared to state-of-the-art training systems.

- **[arXiv260213] LoRA-based Parameter-Efficient LLMs for Continuous Learning in Edge-based Malware Detection**
  - **tags:** [mlsys], [llm inference], [LoRA, continuous learning, parameter-efficient fine-tuning, edge computing, malware detection, transformer models, federated aggregation]
  - **authors:** Christian Rondanini, Barbara Carminati, Elena Ferrari, Niccolò Lardo, Ashish Kundu
  - **institution:** University of Insubria, Cisco Research
  - **link:** https://arxiv.org/pdf/2602.11655
  - **Simple LLM Summary:** This paper proposes a continuous learning architecture for edge-based malware detection using parameter-efficient LoRA adapters on lightweight transformer models. The method enables local fine-tuning on edge devices and global knowledge sharing by aggregating and redistributing only the LoRA modules, improving generalization to unseen attacks. The approach achieves up to 20-25% accuracy gains while adding minimal memory overhead, making it practical for resource-constrained edge hardware.

- **[arXiv260213] RL over Commodity Networks: Overcoming the Bandwidth Barrier with Lossless Sparse Deltas**
  - **tags:** [mlsys], [post-training], [sparse delta checkpoint, multi-stream transmission, bandwidth-aware scheduling, lease-based fault tolerance, rollout generation overlap]
  - **authors:** Chaoyi Ruan, Geng Luo, Xinyi Wan, Long Zhao, Qinghe Wang, Jiaan Zhu, Duling Xu, Guanbin Xu, Dehui Wei, Xiang Liu, Cheng Li, Haifeng Sun, Congcong Miao, Jialin Li
  - **institution:** National University of Singapore, Anhui University, University of Science and Technology of China, Renmin University of China
  - **link:** https://arxiv.org/pdf/2602.11456
  - **Simple LLM Summary:** The paper presents SparrowRL, a system for efficient RL post-training over commodity networks by exploiting the sparsity of model updates, representing each step as a lossless sparse delta. It pipelines delta extraction and transmission, overlaps transfer with computation, and uses intelligent scheduling to coordinate distributed workers. This approach dramatically reduces transfer payload and improves throughput, narrowing the performance gap with expensive RDMA clusters while offering better cost efficiency.

- **[arXiv260213] Future Mining: Learning for Safety and Security**
  - **tags:** [mlsys], [others], [multimodal perception, secure federated learning, reinforcement learning, DTN communication, energy-aware sensing, machine unlearning, spatial-temporal transformers]
  - **authors:** Md Sazedur Rahman, Mizanur Rahman Jewel, Sanjay Madria
  - **institution:** Missouri University of Science and Technology
  - **link:** https://arxiv.org/pdf/2602.11472
  - **Simple LLM Summary:** The paper proposes a Unified Smart Safety and Security Architecture for mining, integrating modules for multimodal perception, secure federated learning, and DTN-enabled communication. It concludes that this cohesive framework is necessary to build a resilient and trustworthy intelligent mining system capable of maintaining safety and operational continuity under harsh and adversarial conditions.

- **[arXiv260213] An Auction-Based Mechanism for Optimal Task Allocation and Resource Aware Containerization**
  - **tags:** [sys], [distributed computing], [auction-based mechanism, Docker swarm, containerization, task offloading, resource allocation]
  - **authors:** Ramakant kumar
  - **institution:** Not specified
  - **link:** https://arxiv.org/pdf/2602.11998
  - **Simple LLM Summary:** The paper proposes AUC-RAC, an auction-based mechanism for optimal task allocation in IoT environments using Docker swarm for containerization. It enables efficient offloading of computation-intensive tasks among local servers through a bidding process. Experimental results show the approach improves task allocation and computation services for IoT devices.

- **[arXiv260213] Contention Resolution, With and Without a Global Clock**
  - **tags:** [sys], [distributed computing], [contention resolution, global clock, local clock, randomized protocols, latency analysis, complexity separation]
  - **authors:** Zixi Cai, Kuowen Chen, Shengquan Du, Tsvi Kopelowitz, Seth Pettie, Ben Plosk
  - **institution:** Tsinghua University, Bar-Ilan University, University of Michigan
  - **link:** https://arxiv.org/pdf/2602.12070
  - **Simple LLM Summary:** The paper studies the Contention Resolution problem in distributed systems, comparing models with and without a global clock. It designs a new protocol with improved latency using a global clock, establishing a complexity gap between the two models, and proves that no single protocol can be optimal for both expected and high-probability latency metrics.

- **[arXiv260213] OServe: Accelerating LLM Serving via Spatial-Temporal Workload Orchestration**
  - **tags:** [mlsys], [llm inference], [workload-aware scheduling, workload-adaptive switching, spatial-temporal heterogeneity, heterogeneous model deployment, data parallelism, tensor parallelism, pipeline parallelism]
  - **authors:** Youhe Jiang, Fangcheng Fu, Taiyi Wang, Guoliang He, Eiko Yoneki
  - **institution:** University of Cambridge, Shanghai Jiao Tong University
  - **link:** https://arxiv.org/pdf/2602.12151
  - **Simple LLM Summary:** OServe is an LLM serving system that addresses spatial and temporal workload heterogeneity via a workload-aware scheduling algorithm and an adaptive switching method for model deployments. It dynamically optimizes and migrates heterogeneous model configurations based on real-time and predicted workload changes. Experiments on real-world traces show OServe improves performance by up to 2× compared to state-of-the-art systems.

- **[arXiv260213] Legitimate Overrides in Decentralized Protocols**
  - **tags:** [sys], [blockchain governance], [emergency mechanisms, stochastic cost-minimization, Scope × Authority taxonomy, heavy-tailed distribution]
  - **authors:** Oghenekaro Elem, Nimrod Talmon
  - **institution:** Parametrig, Ben-Gurion University of the Negev (BGU), Input Output Global (IOG)
  - **link:** https://arxiv.org/pdf/2602.12260
  - **Simple LLM Summary:** The paper develops a Scope × Authority taxonomy to analyze emergency override mechanisms in decentralized protocols and formalizes the trade-offs as a stochastic cost-minimization problem. It concludes that containment time depends on authority type, losses are heavy-tailed, and community sentiment affects the cost of maintaining intervention capability, leading to concrete design principles for emergency governance.

- **[arXiv260213] PrefillShare: A Shared Prefill Module for KV Reuse in Multi-LLM Disaggregated Serving**
  - **tags:** [mlsys], [llm inference], [KV cache reuse, prefill sharing, disaggregated serving, model factorization, fine-tuning, routing mechanism]
  - **authors:** Sunghyeon Woo, Hoseung Kim, Sunghwan Shim, Minjung Jo, Hyunjoon Jeong, Jeongtae Lee, Joonghoon Kim, Sungjae Lee, Baeseong Park, Se Jung Kwon, Dongsoo Lee
  - **institution:** NAVER Cloud
  - **link:** https://arxiv.org/pdf/2602.12029
  - **Simple LLM Summary:** The paper proposes PrefillShare, a method that factorizes an LLM into separate prefill and decode modules, freezes the prefill module, and fine-tunes only the decode module to allow multiple specialized models to share a single prefill computation and KV cache for a common prompt prefix. This approach, integrated with a routing mechanism in a disaggregated serving system, reduces redundancy in multi-model agent workflows. It maintains accuracy while significantly improving latency and throughput compared to standard serving.


**cs.AI/cs.LG contains "reinforcement learning" total: 41**
- [arXiv260213] Quark Medical Alignment: A Holistic Multi-Dimensional Alignment and Collaborative Optimization Paradigm [link](https://arxiv.org/pdf/2602.11661)
- [arXiv260213] SWE-MiniSandbox: Container-Free Reinforcement Learning for Building Software Engineering Agents [link](https://arxiv.org/pdf/2602.11210)
- [arXiv260213] Distributionally Robust Cooperative Multi-Agent Reinforcement Learning via Robust Value Factorization [link](https://arxiv.org/pdf/2602.11437)
- [arXiv260213] Credit Where It is Due: Cross-Modality Connectivity Drives Precise Reinforcement Learning for MLLM Reasoning [link](https://arxiv.org/pdf/2602.11455)
- [arXiv260213] Patch the Distribution Mismatch: RL Rewriting Agent for Stable Off-Policy SFT [link](https://arxiv.org/pdf/2602.11220)
- [arXiv260213] TSR: Trajectory-Search Rollouts for Multi-Turn RL of LLM Agents [link](https://arxiv.org/pdf/2602.11767)
- [arXiv260213] When and What to Ask: AskBench and Rubric-Guided RLVR for LLM Clarification [link](https://arxiv.org/pdf/2602.11199)
- [arXiv260213] TDPNavigator-Placer: Thermal- and Wirelength-Aware Chiplet Placement in 2.5D Systems Through Multi-Agent Reinforcement Learning [link](https://arxiv.org/pdf/2602.11187)
- [arXiv260213] Pushing Forward Pareto Frontiers of Proactive Agents with Behavioral Agentic Optimization [link](https://arxiv.org/pdf/2602.11351)
- [arXiv260213] TabSieve: Explicit In-Table Evidence Selection for Tabular Prediction [link](https://arxiv.org/pdf/2602.11700)
- [arXiv260213] Native Reasoning Models: Training Language Models to Reason on Unverifiable Data [link](https://arxiv.org/pdf/2602.11549)
- [arXiv260213] DICE: Diffusion Large Language Models Excel at Generating CUDA Kernels [link](https://arxiv.org/pdf/2602.11715)
- [arXiv260213] Learning to Configure Agentic AI Systems [link](https://arxiv.org/pdf/2602.11574)
- [arXiv260213] Unifying Stable Optimization and Reference Regularization in RLHF [link](https://arxiv.org/pdf/2602.11523)
- [arXiv260213] The Five Ws of Multi-Agent Communication: Who Talks to Whom, When, What, and Why -- A Survey from MARL to Emergent Language and LLMs [link](https://arxiv.org/pdf/2602.11583)
- [arXiv260213] Can We Really Learn One Representation to Optimize All Rewards? [link](https://arxiv.org/pdf/2602.11399)
- [arXiv260213] Adaptive Milestone Reward for GUI Agents [link](https://arxiv.org/pdf/2602.11524)
- [arXiv260213] Temperature as a Meta-Policy: Adaptive Temperature in LLM Reinforcement Learning [link](https://arxiv.org/pdf/2602.11779)
- [arXiv260213] RELATE: A Reinforcement Learning-Enhanced LLM Framework for Advertising Text Generation [link](https://arxiv.org/pdf/2602.11780)
- [arXiv260213] Detecting RLVR Training Data via Structural Convergence of Reasoning [link](https://arxiv.org/pdf/2602.11792)
- [arXiv260213] Temporal Difference Learning with Constrained Initial Representations [link](https://arxiv.org/pdf/2602.11800)
- [arXiv260213] From Path Signatures to Sequential Modeling: Incremental Signature Contributions for Offline RL [link](https://arxiv.org/pdf/2602.11805)
- [arXiv260213] Predicting LLM Output Length via Entropy-Guided Representations [link](https://arxiv.org/pdf/2602.11812)
- [arXiv260213] In-Context Function Learning in Large Language Models [link](https://arxiv.org/pdf/2602.11863)
- [arXiv260213] Echo: Towards Advanced Audio Comprehension via Audio-Interleaved Reasoning [link](https://arxiv.org/pdf/2602.11909)
- [arXiv260213] Extending Puzzle for Mixture-of-Experts Reasoning Models with Application to GPT-OSS Acceleration [link](https://arxiv.org/pdf/2602.11937)
- [arXiv260213] Gaia2: Benchmarking LLM Agents on Dynamic and Asynchronous Environments [link](https://arxiv.org/pdf/2602.11964)
- [arXiv260213] Accelerating Robotic Reinforcement Learning with Agent Guidance [link](https://arxiv.org/pdf/2602.11978)
- [arXiv260213] FedGRPO: Privately Optimizing Foundation Models with Group-Relative Rewards from Domain Client [link](https://arxiv.org/pdf/2602.12014)
- [arXiv260213] Improving HPC Code Generation Capability of LLMs via Online Reinforcement Learning with Real-Machine Benchmark Rewards [link](https://arxiv.org/pdf/2602.12049)
- [arXiv260213] Geometry of Uncertainty: Learning Metric Spaces for Multimodal State Estimation in RL [link](https://arxiv.org/pdf/2602.12087)
- [arXiv260213] On the Complexity of Offline Reinforcement Learning with $Q^\star$-Approximation and Partial Coverage [link](https://arxiv.org/pdf/2602.12107)
- [arXiv260213] Stop Unnecessary Reflection: Training LRMs for Efficient Reasoning with Adaptive Reflection and Length Coordinated Penalty [link](https://arxiv.org/pdf/2602.12113)
- [arXiv260213] Meta-Sel: Efficient Demonstration Selection for In-Context Learning via Supervised Meta-Learning [link](https://arxiv.org/pdf/2602.12123)
- [arXiv260213] Capability-Oriented Training Induced Alignment Risk [link](https://arxiv.org/pdf/2602.12124)
- [arXiv260213] Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation [link](https://arxiv.org/pdf/2602.12125)
- [arXiv260213] Seq2Seq2Seq: Lossless Data Compression via Discrete Latent Transformers and Reinforcement Learning [link](https://arxiv.org/pdf/2602.12146)
- [arXiv260213] DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing [link](https://arxiv.org/pdf/2602.12205)
- [arXiv260213] Towards On-Policy SFT: Distribution Discriminant Theory and its Applications in LLM Training [link](https://arxiv.org/pdf/2602.12222)
- [arXiv260213] Intrinsic-Energy Joint Embedding Predictive Architectures Induce Quasimetric Spaces [link](https://arxiv.org/pdf/2602.12245)
- [arXiv260213] CM2: Reinforcement Learning with Checklist Rewards for Multi-Turn and Multi-Step Agentic Tool Use [link](https://arxiv.org/pdf/2602.12268)

**cs.AI/cs.LG contains "accelerate" total: 12**
- [arXiv260213] PatientHub: A Unified Framework for Patient Simulation [link](https://arxiv.org/pdf/2602.11684)
- [arXiv260213] Exploring Multiple High-Scoring Subspaces in Generative Flow Networks [link](https://arxiv.org/pdf/2602.11491)
- [arXiv260213] TDPNavigator-Placer: Thermal- and Wirelength-Aware Chiplet Placement in 2.5D Systems Through Multi-Agent Reinforcement Learning [link](https://arxiv.org/pdf/2602.11187)
- [arXiv260213] The PBSAI Governance Ecosystem: A Multi-Agent AI Reference Architecture for Securing Enterprise AI Estates [link](https://arxiv.org/pdf/2602.11301)
- [arXiv260213] Divide and Learn: Multi-Objective Combinatorial Optimization at Scale [link](https://arxiv.org/pdf/2602.11346)
- [arXiv260213] ThinkRouter: Efficient Reasoning via Routing Thinking between Latent and Discrete Spaces [link](https://arxiv.org/pdf/2602.11683)
- [arXiv260213] Evaluating LLM Safety Under Repeated Inference via Accelerated Prompt Stress Testing [link](https://arxiv.org/pdf/2602.11786)
- [arXiv260213] Deep Kernel Fusion for Transformers [link](https://arxiv.org/pdf/2602.11808)
- [arXiv260213] Accelerating Robotic Reinforcement Learning with Agent Guidance [link](https://arxiv.org/pdf/2602.11978)
- [arXiv260213] Multi UAVs Preflight Planning in a Shared and Dynamic Airspace [link](https://arxiv.org/pdf/2602.12055)
- [arXiv260213] Few-Shot Design Optimization by Exploiting Auxiliary Information [link](https://arxiv.org/pdf/2602.12112)
- [arXiv260213] Categorical Flow Maps [link](https://arxiv.org/pdf/2602.12233)
