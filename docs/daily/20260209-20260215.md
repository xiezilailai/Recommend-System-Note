# 20260209-20260215

## 2026-02-09

**cs.DC total: 21**

- **[arXiv260209] Computationally Efficient Laplacian CL-colME**
  - **tags:** [mlsys], [cluster infrastructure], [decentralized learning, collaborative mean estimation, laplacian consensus, graph-based consensus, doubly stochastic matrices]
  - **authors:** Nikola Stankovic
  - **institution:** IEEE
  - **link:** https://arxiv.org/pdf/2602.06070
  - **Simple LLM Summary:** This paper proposes CL-colME, a Laplacian-based consensus variant of decentralized collaborative mean estimation that avoids the computationally expensive normalization of doubly stochastic matrices. Simulation results show that CL-colME maintains the convergence and accuracy of the prior C-colME method while improving computational efficiency.

- **[arXiv260209] Experimental Analysis of Server-Side Caching for Web Performance**
  - **tags:** [sys], [web performance optimization], [server-side caching, in-memory cache, response time optimization, time-to-live (TTL), performance evaluation]
  - **authors:** Mohammad Umar, Bharat Tripathi
  - **institution:** Allenhouse Business School
  - **link:** https://arxiv.org/pdf/2602.06074
  - **Simple LLM Summary:** This paper experimentally compares the performance of a web server with and without simple in-memory caching using a fixed time-to-live. The results demonstrate that server-side caching significantly reduces response times, making it an effective and simple technique for improving performance in small-scale or educational web applications.

- **[arXiv260209] iScheduler: Reinforcement Learning-Driven Continual Optimization for Large-Scale Resource Investment Problems**
  - **tags:** [mlsys], [cluster infrastructure], [reinforcement learning, iterative scheduling, Markov decision process, resource investment problem, L-RIPLIB benchmark]
  - **authors:** Yi-Xiang Hu, Yuke Wang, Feng Wu, Zirui Huang, Shuli Zeng, Xiang-Yang Li
  - **institution:** University of Science and Technology of China
  - **link:** https://arxiv.org/pdf/2602.06064
  - **Simple LLM Summary:** The paper presents iScheduler, a framework that uses reinforcement learning to formulate and solve large-scale resource investment problems as a Markov decision process over decomposed subproblems. It achieves competitive resource costs while reducing the time to find a feasible schedule by up to 43 times compared to commercial solvers, and supports efficient schedule reconfiguration by reusing unaffected parts.

- **[arXiv260209] HQP: Sensitivity-Aware Hybrid Quantization and Pruning for Ultra-Low-Latency Edge AI Inference**
  - **tags:** [mlsys], [post-training], [hybrid quantization, structural pruning, Fisher Information Matrix, 8-bit post-training quantization, sensitivity-aware pruning, model compression]
  - **authors:** Dinesh Gopalan, Ratul Ali
  - **institution:** AMD, Jahangirnagar University
  - **link:** https://arxiv.org/pdf/2602.06069
  - **Simple LLM Summary:** This paper introduces the HQP framework, which combines sensitivity-aware structural pruning using a Fisher Information Matrix approximation and conditional 8-bit post-training quantization to optimize models for edge inference. The method achieves up to 3.12× speedup and 55% size reduction while limiting accuracy drop to under 1.5%, outperforming single-objective compression techniques.

- **[arXiv260209] FlashSketch: Sketch-Kernel Co-Design for Fast Sparse Sketching on GPUs**
  - **tags:** [mlsys], [GPU kernels], [sparse sketching, BlockPerm-SJLT, sketch-kernel co-design, oblivious subspace embedding, CUDA]
  - **authors:** Rajat Vadiraj Dwaraknath, Sungyoon Kim, Mert Pilanci
  - **institution:** Stanford University
  - **link:** https://arxiv.org/pdf/2602.06071
  - **Simple LLM Summary:** The paper introduces FlashSketch, a method that co-designs a new sparse sketch family called BlockPerm-SJLT with an optimized CUDA kernel to improve GPU efficiency. This approach introduces a tunable parameter to trade off between GPU performance and sketching robustness. The method achieves a geomean speedup of 1.7x over prior state-of-the-art GPU sketches, advancing the Pareto frontier of sketching quality versus speed.

- **[arXiv260209] PackInfer: Compute- and I/O-Efficient Attention for Batched LLM Inference**
  - **tags:** [mlsys], [llm inference], [kernel-level attention, load-balanced execution groups, I/O-aware grouping, KV cache reorganization, FlashAttention]
  - **authors:** Rui Ning, Wei Zhang, Fan Lai
  - **institution:** Nanjing University, University of Illinois Urbana-Champaign
  - **link:** https://arxiv.org/pdf/2602.06072
  - **Simple LLM Summary:** PackInfer is a kernel-level attention framework that improves batched LLM inference by packing heterogeneous requests into load-balanced execution groups and reorganizing KV caches with I/O-aware grouping. This approach reduces computation and I/O imbalance, leading to better GPU utilization. Evaluations show it reduces latency by 13.0-20.1% and improves throughput by 20% compared to FlashAttention.

- **[arXiv260209] Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers**
  - **tags:** [mlsys], [llm training], [matrix-based optimizers, data parallelism, tensor parallelism, asynchronous compute, load balancing, Shampoo, SOAP, Muon]
  - **authors:** Liangyu Wang, Siqi Zhang, Junjie Wang, Yiming Dong, Bo Zheng, Zihan Qiu, Shengkun Tang, Di Wang, Rui Men, Dayiheng Liu
  - **institution:** KAUST, Alibaba Group, Peking University, MBZUAI
  - **link:** https://arxiv.org/pdf/2602.06079
  - **Simple LLM Summary:** The paper proposes Canzona, a framework that decouples logical optimizer assignment from physical parameter distribution to efficiently run matrix-based optimizers in distributed LLM training. It introduces strategies like α-Balanced Static Partitioning for Data Parallelism and an Asynchronous Compute pipeline for Tensor Parallelism. Evaluations show it achieves significant speedups in iteration time and reduces optimizer step latency compared to baselines.

- **[arXiv260209] MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments**
  - **tags:** [mlsys], [others], [memory-centric benchmark, mobile GUI agents, LLM-as-judge evaluation, pass@k, cross-session learning, cross-temporal retention, cross-spatial retention]
  - **authors:** Guangyi Liu, Pengxiang Zhao, Yaozhen Liang, Qinyi Luo, Shunye Tang, Yuxiang Chai, Weifeng Lin, Han Xiao, WenHao Wang, Siheng Chen, Zhengxi Lu, Gao Wu, Hao Wang, Liang Liu, Yong Liu
  - **institution:** Zhejiang University, Nankai University, The Chinese University of Hong Kong, Shanghai Jiao Tong University, vivo AI Lab
  - **link:** https://arxiv.org/pdf/2602.06075
  - **Simple LLM Summary:** This paper introduces MemGUI-Bench, a comprehensive benchmark designed to evaluate the memory capabilities of mobile GUI agents, featuring an automated evaluation pipeline with staged LLM-as-judge and pass@k metrics. The study reveals significant memory deficits across all 11 evaluated state-of-the-art agents and identifies distinct failure modes, concluding with actionable design implications for improving agent memory.

- **[arXiv260209] Quantifying Energy-Efficient Edge Intelligence: Inference-time Scaling Laws for Heterogeneous Computing**
  - **tags:** [mlsys], [llm inference], [scaling laws, heterogeneous orchestration, hardware-aware routing, progressive sample multiplexing, cost models, Intelligence Per Watt, Energy-Coverage Efficiency, Price-Power-Performance]
  - **authors:** Satyam Kumar, Saurabh Jha
  - **institution:** Not explicitly provided in the given text. The author names are listed without affiliations or email domains.
  - **link:** https://arxiv.org/pdf/2602.06057
  - **Simple LLM Summary:** This paper introduces QEIL, a framework that uses inference-time scaling laws and heterogeneous hardware orchestration (across CPU, GPU, NPU) to optimize LLM inference on edge devices. It demonstrates that this approach achieves superlinear efficiency gains, significantly improves coverage and energy efficiency, and reduces latency without accuracy loss, establishing it as an optimal strategy for energy-constrained systems.

- **[arXiv260209] Mapping Gemma3 onto an Edge Dataflow Architecture**
  - **tags:** [mlsys], [llm inference], [dequantization engine, tiled matrix multiplication, FlowQKV, FusedDQP, FlowKV, Q4NX quantization, dataflow architecture]
  - **authors:** Shouyu Du, Miaoxiang Yu, Zhiheng Ni, Jillian Cai, Qing Yang, Tao Wei, Zhenyu Xu
  - **institution:** Clemson University, University of Rhode Island
  - **link:** https://arxiv.org/pdf/2602.06063
  - **Simple LLM Summary:** This paper presents the first end-to-end deployment of the Gemma3 family of models on an AMD Ryzen AI NPU using hardware-aware techniques like optimized dequantization, fused kernels, and novel attention mechanisms (FlowQKV, FlowKV). The proposed methods, combined with a 4-bit quantization format, achieve significant speedups and power efficiency improvements over iGPU and CPU baselines. The work demonstrates that modern NPUs can enable practical, low-power LLM and VLM inference at the edge and provides a blueprint for mapping transformers onto tiled dataflow accelerators.

- **[arXiv260209] LAAFD: LLM-based Agents for Accelerated FPGA Design**
  - **tags:** [mlsys], [llm inference], [high-level synthesis, agentic workflow, pipelining, vectorization, dataflow partitioning, co-simulation]
  - **authors:** Maxim Moraru, Kamalavasan Kamalakkannan, Jered Dominguez-Trujillo, Patrick Diehl, Atanu Barai, Julien Loiseau, Zachary Kent Baker, Howard Pritchard, Galen M Shipman
  - **institution:** Los Alamos National Laboratory
  - **link:** https://arxiv.org/pdf/2602.06085
  - **Simple LLM Summary:** This paper introduces LAAFD, an agentic workflow that uses large language models to automatically translate general-purpose C++ code into optimized FPGA kernels for Vitis HLS. The system automates hardware optimizations like pipelining and uses feedback from HLS reports and co-simulation to iteratively improve performance. The results show that LAAFD achieves near hand-tuned performance, significantly lowering the expertise barrier for FPGA acceleration.

- **[arXiv260209] AdFL: In-Browser Federated Learning for Online Advertisement**
  - **tags:** [mlsys], [others], [federated learning, differential privacy, in-browser training, ad viewability prediction]
  - **authors:** Ahmad Alemari, Pritam Sen, Cristian Borcea
  - **institution:** New Jersey Institute of Technology, Jazan University
  - **link:** https://arxiv.org/pdf/2602.06336
  - **Simple LLM Summary:** This paper presents AdFL, a federated learning framework that operates within web browsers to learn user ad preferences without sharing raw data, balancing targeted advertising with privacy. The system aggregates local updates into a global model and was tested with a proof-of-concept model for ad viewability prediction. Experiments show the framework is feasible, achieves high prediction accuracy (up to 92.59% AUC), and maintains adequate performance when differential privacy is applied to protect local model parameters.

- **[arXiv260209] BouquetFL: Emulating diverse participant hardware in Federated Learning**
  - **tags:** [mlsys], [cluster infrastructure], [federated learning, hardware emulation, resource restriction, GPU constraints, CPU throttling, memory constraints]
  - **authors:** Arno Geimer
  - **institution:** Unknown (Author email/affiliation not provided in excerpt)
  - **link:** https://arxiv.org/pdf/2602.06498
  - **Simple LLM Summary:** The paper presents BouquetFL, a framework that simulates heterogeneous client hardware (CPU, RAM, GPU) on a single machine through programmatic resource restriction for Federated Learning experiments. It concludes that this approach addresses a methodological gap by enabling controlled, reproducible study of system heterogeneity without requiring multiple physical devices, bringing FL research closer to practical deployment conditions.

- **[arXiv260209] Degradation of Feature Space in Continual Learning**
  - **tags:** [ai], [continual learning], [contrastive learning, isotropic regularization, feature space anisotropy, catastrophic forgetting, stability-plasticity dilemma]
  - **authors:** Chiara Lanza, Roberto Pereira, Marco Miozzo, Eduard Angelats, Paolo Dini
  - **institution:** CTTC (Centre Tecnològic de Telecomunicacions de Catalunya)
  - **link:** https://arxiv.org/pdf/2602.06586
  - **Simple LLM Summary:** This paper investigates whether enforcing isotropy in the feature space can improve representation quality in continual learning. Through experiments using contrastive continual learning techniques on CIFAR datasets, the study finds that isotropic regularization fails to improve and can even degrade model accuracy. The results suggest that isotropy, beneficial in centralized training, may not be a suitable inductive bias for non-stationary continual learning scenarios.

- **[arXiv260209] FCDP: Fully Cached Data Parallel for Communication-Avoiding Large-Scale Training**
  - **tags:** [mlsys], [llm training], [ZeRO-3, host memory caching, parameter-efficient fine-tuning (PEFT), all-gather, communication optimization]
  - **authors:** Gyeongseo Park, Eungyeong Lee, Song-woo Sok, Myung-Hoon Cha, Kwangwon Koh, Baik-Song An, Hongyeon Kim, Ki-Dong Kang
  - **institution:** Electronics and Telecommunications Research Institute (ETRI)
  - **link:** https://arxiv.org/pdf/2602.06499
  - **Simple LLM Summary:** The paper proposes FCDP, a method that caches forward-pass parameters in host memory and reuses them during the backward pass via fast intra-node all-gather to eliminate redundant inter-node communication. This approach preserves ZeRO-3's minimal GPU memory footprint while significantly boosting throughput on bandwidth-limited commodity clusters. The results show FCDP achieves up to 100x higher throughput than ZeRO-3 while maintaining the same maximum batch size.

- **[arXiv260209] Reinforcement Learning-Based Dynamic Management of Structured Parallel Farm Skeletons on Serverless Platforms**
  - **tags:** [mlsys], [cluster infrastructure], [reinforcement learning, autoscaling, serverless, OpenFaaS, farm skeleton, QoS, Gymnasium]
  - **authors:** Lanpei Li, Massimo Coppola, Malio Li, Valerio Besozzi, Jack Bell, Vincenzo Lomonaco
  - **institution:** National Research Council of Italy (CNR), University of Pisa, LUISS University
  - **link:** https://arxiv.org/pdf/2602.06555
  - **Simple LLM Summary:** This paper proposes a reinforcement learning-based framework for dynamically managing parallel task farm skeletons on serverless platforms like OpenFaaS. The method uses RL agents to learn autoscaling policies for worker pools, aiming to meet QoS targets. The results show that this AI-driven approach outperforms reactive, model-based baselines by better handling platform-specific limitations, improving QoS while maintaining efficient resource usage.

- **[arXiv260209] Same Engine, Multiple Gears: Parallelizing Fixpoint Iteration at Different Granularities (Extended Version)**
  - **tags:** [sys], [static analysis], [fixpoint iteration, parallelization, top-down solver, task granularity, immediate approach, independent approach]
  - **authors:** Ali Rasim Kocal, Michael Schwarz, Simmo Saan, Helmut Seidl
  - **institution:** Technical University of Munich, National University of Singapore, University of Tartu
  - **link:** https://arxiv.org/pdf/2602.06680
  - **Simple LLM Summary:** The paper proposes a parallelization method for a generic fixpoint engine used in static analysis, which is parametric in task granularity and implements two competing philosophies: an immediate approach with a shared thread-safe state and an independent approach with per-task states. The method is integrated into the Goblint static analysis framework and evaluated on large real-world programs. The results demonstrate that this flexible parallelization can significantly reduce analysis times.

- **[arXiv260209] Wonderboom -- Efficient, and Censorship-Resilient Signature Aggregation for Million Scale Consensus**
  - **tags:** [sys], [blockchain consensus], [signature aggregation, censorship-resilience, Byzantine Fault Tolerance, validator set, quorum attestation]
  - **authors:** Zeta Avarikioti, Ray Neiheiser, Krzysztof Pietrzak, Michelle X. Yeo
  - **institution:** TU Wien, Institute of Science and Technology Austria, Nanyang Technological University, Aarhus University, Common Prefix
  - **link:** https://arxiv.org/pdf/2602.06655
  - **Simple LLM Summary:** This paper introduces Wonderboom, a new signature aggregation protocol designed to speed up finality in large-scale blockchain consensus like Ethereum. It claims to aggregate millions of validator signatures 32 times faster than the current state-of-the-art while offering improved security against censorship and stake-shifting attacks. The authors also implement a simulation tool to demonstrate that Wonderboom can handle over 2 million signatures within a single Ethereum slot.

- **[arXiv260209] DualMap: Enabling Both Cache Affinity and Load Balancing for Distributed LLM Serving**
  - **tags:** [mlsys], [llm inference], [cache-affinity scheduling, load balancing, dual-mapping, SLO-aware routing, hotspot-aware rebalancing, dual-hash-ring scaling]
  - **authors:** Ying Yuan, Pengfei Zuo, Bo Wang, Zhangyu Chen, Zhipeng Tan, Zhou Yu
  - **institution:** Huazhong University of Science and Technology, Huawei
  - **link:** https://arxiv.org/pdf/2602.06502
  - **Simple LLM Summary:** The paper proposes DualMap, a dual-mapping scheduling strategy for distributed LLM serving that uses two hash functions to map each request to two candidate instances, intelligently selecting the better one to achieve both cache affinity and load balancing. It incorporates SLO-aware routing, hotspot-aware rebalancing, and lightweight scaling techniques to handle dynamic workloads. Experiments show DualMap improves effective request capacity by up to 2.25x under the same TTFT SLO constraints compared to state-of-the-art methods.

- **[arXiv260209] Implementing Grassroots Logic Programs with Multiagent Transition Systems and AI**
  - **tags:** [sys], [concurrent logic programming], [multiagent transition systems, operational semantics, deterministic semantics, grassroots platforms, peer-to-peer]
  - **authors:** Ehud Shapiro
  - **institution:** London School of Economics, Weizmann Institute of Science
  - **link:** https://arxiv.org/pdf/2602.06934
  - **Simple LLM Summary:** This paper presents deterministic operational semantics (dGLP and madGLP) for implementing Grassroots Logic Programs, a concurrent logic programming language for peer-to-peer systems. The authors prove the correctness of these semantics and describe their use as formal specifications for AI-assisted implementation in Dart for workstation and smartphone platforms. The main conclusion is the development of a mathematically verified, implementation-ready semantic framework for building distributed, grassroots applications.

- **[arXiv260209] Distributed Knowledge in Simplicial Models**
  - **tags:** [sys], [distributed computing], [simplicial complexes, epistemic logic, distributed knowledge, majority consensus, Kripke models]
  - **authors:** Éric Goubault, Jérémy Ledent, Sergio Rajsbaum
  - **institution:** LIX, CNRS, École Polytechnique, Institut Polytechnique de Paris; Université Paris Cité, CNRS, IRIF; Instituto de Matemáticas, UNAM
  - **link:** https://arxiv.org/pdf/2602.06945
  - **Simple LLM Summary:** This paper introduces simplicial complexes as models for multi-agent epistemic logic, shifting from world-based Kripke models to agent-view-based models. It connects distributed computing, logic, and topology, showing how distributed knowledge relates to solving the majority consensus task in different communication models and identifying logical obstructions when it is unsolvable.


**cs.AI/cs.LG contains "reinforcement learning" total: 30**
- [arXiv260209] Transformer-Based Reinforcement Learning for Autonomous Orbital Collision Avoidance in Partially Observable Environments [link](https://arxiv.org/pdf/2602.06088)
- [arXiv260209] Jackpot: Optimal Budgeted Rejection Sampling for Extreme Actor-Policy Mismatch Reinforcement Learning [link](https://arxiv.org/pdf/2602.06107)
- [arXiv260209] Self-Improving World Modelling with Latent Actions [link](https://arxiv.org/pdf/2602.06130)
- [arXiv260209] Flow Matching for Offline Reinforcement Learning with Discrete Actions [link](https://arxiv.org/pdf/2602.06138)
- [arXiv260209] Learning Rate Scaling across LoRA Ranks and Transfer to Full Finetuning [link](https://arxiv.org/pdf/2602.06204)
- [arXiv260209] Online Adaptive Reinforcement Learning with Echo State Networks for Non-Stationary Dynamics [link](https://arxiv.org/pdf/2602.06326)
- [arXiv260209] Training Data Selection with Gradient Orthogonality for Efficient Domain Adaptation [link](https://arxiv.org/pdf/2602.06359)
- [arXiv260209] Unlocking Noisy Real-World Corpora for Foundation Model Pre-Training via Quality-Aware Tokenization [link](https://arxiv.org/pdf/2602.06394)
- [arXiv260209] TrailBlazer: History-Guided Reinforcement Learning for Black-Box LLM Jailbreaking [link](https://arxiv.org/pdf/2602.06440)
- [arXiv260209] Prism: Spectral Parameter Sharing for Multi-Agent Reinforcement Learning [link](https://arxiv.org/pdf/2602.06476)
- [arXiv260209] AgentCPM-Explore: Realizing Long-Horizon Deep Exploration for Edge-Scale Agents [link](https://arxiv.org/pdf/2602.06485)
- [arXiv260209] Adaptive Uncertainty-Aware Tree Search for Robust Reasoning [link](https://arxiv.org/pdf/2602.06493)
- [arXiv260209] Progress Constraints for Reinforcement Learning in Behavior Trees [link](https://arxiv.org/pdf/2602.06525)
- [arXiv260209] Dynamics-Aligned Shared Hypernetworks for Zero-Shot Actuator Inversion [link](https://arxiv.org/pdf/2602.06550)
- [arXiv260209] SeeUPO: Sequence-Level Agentic-RL with Convergence Guarantees [link](https://arxiv.org/pdf/2602.06554)
- [arXiv260209] SPARC: Separating Perception And Reasoning Circuits for Test-time Scaling of VLMs [link](https://arxiv.org/pdf/2602.06566)
- [arXiv260209] Sample-Efficient Policy Space Response Oracles with Joint Experience Best Response [link](https://arxiv.org/pdf/2602.06599)
- [arXiv260209] The hidden risks of temporal resampling in clinical reinforcement learning [link](https://arxiv.org/pdf/2602.06603)
- [arXiv260209] Humanoid Manipulation Interface: Humanoid Whole-Body Manipulation from Robot-Free Demonstrations [link](https://arxiv.org/pdf/2602.06643)
- [arXiv260209] compar:IA: The French Government's LLM arena to collect French-language human prompts and preference data [link](https://arxiv.org/pdf/2602.06669)
- [arXiv260209] F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare [link](https://arxiv.org/pdf/2602.06717)
- [arXiv260209] Semantically Labelled Automata for Multi-Task Reinforcement Learning with LTL Instructions [link](https://arxiv.org/pdf/2602.06746)
- [arXiv260209] Soft Forward-Backward Representations for Zero-shot Reinforcement Learning with General Utilities [link](https://arxiv.org/pdf/2602.06769)
- [arXiv260209] Generating Data-Driven Reasoning Rubrics for Domain-Adaptive Reward Modeling [link](https://arxiv.org/pdf/2602.06795)
- [arXiv260209] AEGPO: Adaptive Entropy-Guided Policy Optimization for Diffusion Models [link](https://arxiv.org/pdf/2602.06825)
- [arXiv260209] A first realization of reinforcement learning-based closed-loop EEG-TMS [link](https://arxiv.org/pdf/2602.06907)
- [arXiv260209] Continuous-time reinforcement learning: ellipticity enables model-free value function approximation [link](https://arxiv.org/pdf/2602.06930)
- [arXiv260209] Cochain Perspectives on Temporal-Difference Signals for Learning Beyond Markov Dynamics [link](https://arxiv.org/pdf/2602.06939)
- [arXiv260209] Optimal Derivative Feedback Control for an Active Magnetic Levitation System: An Experimental Study on Data-Driven Approaches [link](https://arxiv.org/pdf/2602.06944)
- [arXiv260209] InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning [link](https://arxiv.org/pdf/2602.06960)

**cs.AI/cs.LG contains "accelerate" total: 16**
- [arXiv260209] Analyzing Diffusion and Autoregressive Vision Language Models in Multimodal Embedding Space [link](https://arxiv.org/pdf/2602.06056)
- [arXiv260209] Compressing LLMs with MoP: Mixture of Pruners [link](https://arxiv.org/pdf/2602.06127)
- [arXiv260209] Stop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding [link](https://arxiv.org/pdf/2602.06161)
- [arXiv260209] To 2:4 Sparsity and Beyond: Neuron-level Activation Function to Accelerate LLM Pre-Training [link](https://arxiv.org/pdf/2602.06183)
- [arXiv260209] Accelerating Vision Transformers on Brain Processing Unit [link](https://arxiv.org/pdf/2602.06300)
- [arXiv260209] Beyond Code Contributions: How Network Position, Temporal Bursts, and Code Review Activities Shape Contributor Influence in Large-Scale Open Source Ecosystems [link](https://arxiv.org/pdf/2602.06426)
- [arXiv260209] Principle-Evolvable Scientific Discovery via Uncertainty Minimization [link](https://arxiv.org/pdf/2602.06448)
- [arXiv260209] SaDiT: Efficient Protein Backbone Design via Latent Structural Tokenization and Diffusion Transformers [link](https://arxiv.org/pdf/2602.06706)
- [arXiv260209] GhostCite: A Large-Scale Analysis of Citation Validity in the Age of Large Language Models [link](https://arxiv.org/pdf/2602.06718)
- [arXiv260209] AEGPO: Adaptive Entropy-Guided Policy Optimization for Diffusion Models [link](https://arxiv.org/pdf/2602.06825)
- [arXiv260209] Are Deep Learning Based Hybrid PDE Solvers Reliable? Why Training Paradigms and Update Strategies Matter [link](https://arxiv.org/pdf/2602.06842)
- [arXiv260209] Rethinking Multi-Condition DiTs: Eliminating Redundant Attention via Position-Alignment and Keyword-Scoping [link](https://arxiv.org/pdf/2602.06850)
- [arXiv260209] AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents [link](https://arxiv.org/pdf/2602.06855)
- [arXiv260209] When RL Meets Adaptive Speculative Training: A Unified Training-Serving System [link](https://arxiv.org/pdf/2602.06932)
- [arXiv260209] DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos [link](https://arxiv.org/pdf/2602.06949)
- [arXiv260209] InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning [link](https://arxiv.org/pdf/2602.06960)
